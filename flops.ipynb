{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efcb5c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LENGTHS = [2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, \n",
    "           2048, 4096, 8192, 16384, 32768, 65536, 131072, 262144, 524288, 1048576]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "424920c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 10143\n",
      "============================================================\n",
      "ECHO STATE TRANSFORMER V2 - FLOPs BREAKDOWN\n",
      "============================================================\n",
      "Input Embedding: 491,520 FLOPs\n",
      "Output Projection: 491,520 FLOPs\n",
      "\n",
      "Per Layer FLOPs: 67,719,168\n",
      "Total Layers FLOPs: 67,719,168\n",
      "\n",
      "--- Layer Breakdown ---\n",
      "Memory Operations: 14,438,400 FLOPs (21.3%)\n",
      "Attention Mechanisms: 42,860,544 FLOPs (63.3%)\n",
      "Feed Forward: 10,420,224 FLOPs (15.4%)\n",
      "\n",
      "--- Detailed Memory Breakdown ---\n",
      "  adaptive_lr_mm: 196,608 FLOPs\n",
      "  adaptive_lr_softmax: 36,864 FLOPs\n",
      "  feed_sparse_mm: 1,376,256 FLOPs\n",
      "  echo_sparse_mm: 3,096,576 FLOPs\n",
      "  echo_bias_add: 442,368 FLOPs\n",
      "  state_update_mul1: 442,368 FLOPs\n",
      "  feed_echo_add: 442,368 FLOPs\n",
      "  tanh: 442,368 FLOPs\n",
      "  state_update_mul2: 442,368 FLOPs\n",
      "  state_update_final_add: 442,368 FLOPs\n",
      "  output_mm: 7,077,888 FLOPs\n",
      "============================================================\n",
      "TOTAL FLOPs: 68,702,208\n",
      "TOTAL GFLOPs: 0.069\n",
      "============================================================\n",
      "\n",
      "=== TEST AVEC DIFFÉRENTES LONGUEURS DE SÉQUENCE ===\n",
      "Sequence Length:    2, Total FLOPs:      447,280, GFLOPs:    0.000\n",
      "Sequence Length:    4, Total FLOPs:      894,560, GFLOPs:    0.001\n",
      "Sequence Length:    8, Total FLOPs:    1,789,120, GFLOPs:    0.002\n",
      "Sequence Length:   16, Total FLOPs:    3,578,240, GFLOPs:    0.004\n",
      "Sequence Length:   32, Total FLOPs:    7,156,480, GFLOPs:    0.007\n",
      "Sequence Length:   64, Total FLOPs:   14,312,960, GFLOPs:    0.014\n",
      "Sequence Length:  128, Total FLOPs:   28,625,920, GFLOPs:    0.029\n",
      "Sequence Length:  256, Total FLOPs:   57,251,840, GFLOPs:    0.057\n",
      "Sequence Length:  512, Total FLOPs:  114,503,680, GFLOPs:    0.115\n",
      "Sequence Length: 1024, Total FLOPs:  229,007,360, GFLOPs:    0.229\n",
      "Sequence Length: 2048, Total FLOPs:  458,014,720, GFLOPs:    0.458\n",
      "Sequence Length: 4096, Total FLOPs:  916,029,440, GFLOPs:    0.916\n",
      "Sequence Length: 8192, Total FLOPs: 1,832,058,880, GFLOPs:    1.832\n",
      "Sequence Length: 16384, Total FLOPs: 3,664,117,760, GFLOPs:    3.664\n",
      "Sequence Length: 32768, Total FLOPs: 7,328,235,520, GFLOPs:    7.328\n",
      "Sequence Length: 65536, Total FLOPs: 14,656,471,040, GFLOPs:   14.656\n",
      "Sequence Length: 131072, Total FLOPs: 29,312,942,080, GFLOPs:   29.313\n",
      "Sequence Length: 262144, Total FLOPs: 58,625,884,160, GFLOPs:   58.626\n",
      "Sequence Length: 524288, Total FLOPs: 117,251,768,320, GFLOPs:  117.252\n",
      "Sequence Length: 1048576, Total FLOPs: 234,503,536,640, GFLOPs:  234.504\n"
     ]
    }
   ],
   "source": [
    "def count_flops_est(model, batch_size=1, sequence_length=1):\n",
    "    \"\"\"\n",
    "    Compte les FLOPs (Floating Point Operations) pour le modèle Echo State Transformer (nouvelle implémentation).\n",
    "    \n",
    "    Parameters:\n",
    "    - model: Instance du modèle EST\n",
    "    - batch_size: Taille du batch\n",
    "    - sequence_length: Longueur de la séquence\n",
    "    \n",
    "    Returns:\n",
    "    - dict: Dictionnaire détaillé des FLOPs par composant\n",
    "    - int: Total des FLOPs\n",
    "    \"\"\"\n",
    "    \n",
    "    # Paramètres du modèle\n",
    "    B = batch_size\n",
    "    T = sequence_length\n",
    "    L = model.num_layers  # Nombre de couches\n",
    "    M = model.memory_units  # Nombre d'unités mémoire\n",
    "    R = model.memory_dim  # Dimension mémoire\n",
    "    D = model.attention_dim  # Dimension attention\n",
    "    I = model.enc_in  # Dimension d'entrée\n",
    "    \n",
    "    # Paramètres de sortie selon la tâche\n",
    "    if hasattr(model.projection, 'out_features'):\n",
    "        O = model.projection.out_features\n",
    "    else:\n",
    "        O = D  # Par défaut\n",
    "    \n",
    "    flops_breakdown = {}\n",
    "    \n",
    "    # ==================== EMBEDDING LAYER ====================\n",
    "    \n",
    "    # Input embedding (DataEmbedding)\n",
    "    # Approximation: principalement la projection linéaire value_embedding\n",
    "    flops_breakdown['input_embedding'] = B * T * I * D\n",
    "    \n",
    "    # ==================== EST LAYER LEVEL (répété L fois pour T timesteps) ====================\n",
    "    \n",
    "    layer_flops = {}\n",
    "    \n",
    "    # ==================== MEMORY FORWARD ====================\n",
    "    \n",
    "    memory_flops = {}\n",
    "    \n",
    "    # Adaptive Leak Rate computation\n",
    "    # X @ adaptive_lr: [B, M, 1, D] @ [M, D, 1] -> [B, M, 1, 1]\n",
    "    memory_flops['adaptive_lr_mm'] = B * M * D * 1 * T\n",
    "    \n",
    "    # Softmax sur adaptive_lr (division par temperature + softmax)\n",
    "    memory_flops['adaptive_lr_softmax'] = B * M * 1 * 1 * 3 * T  # exp + sum + div\n",
    "    \n",
    "    # Feed computation (sparse matrix multiplication)\n",
    "    # Estimation basée sur la connectivité fixe\n",
    "    input_connectivity = model.est_layers[0].memory.input_connectivity\n",
    "    feed_connections = int(input_connectivity * R)  # Nombre de connexions par colonne\n",
    "    # Sparse MM: [B, M, 1, D] avec [M, D, R] -> [B, M, 1, R]\n",
    "    memory_flops['feed_sparse_mm'] = B * M * D * feed_connections * T\n",
    "    \n",
    "    # Echo computation (sparse matrix multiplication + bias)\n",
    "    res_connectivity = model.est_layers[0].memory.res_connectivity\n",
    "    echo_connections = int(res_connectivity * R)  # Nombre de connexions par colonne\n",
    "    # Sparse MM: [B, M, 1, R] avec [M, R, R] -> [B, M, 1, R]\n",
    "    memory_flops['echo_sparse_mm'] = B * M * R * echo_connections * T\n",
    "    # Addition du biais: [B, M, 1, R] + [M, 1, R]\n",
    "    memory_flops['echo_bias_add'] = B * M * 1 * R * T\n",
    "    \n",
    "    # State update computation\n",
    "    # (1 - lr) * state: [B, M, 1, 1] * [B, M, 1, R]\n",
    "    memory_flops['state_update_mul1'] = B * M * 1 * R * T\n",
    "    # lr * tanh(feed + echo): addition + tanh + multiplication\n",
    "    memory_flops['feed_echo_add'] = B * M * 1 * R * T  # feed + echo\n",
    "    memory_flops['tanh'] = B * M * 1 * R * T  # tanh\n",
    "    memory_flops['state_update_mul2'] = B * M * 1 * R * T  # lr * tanh(...)\n",
    "    # Final addition: ((1-lr)*state) + lr*tanh(...)\n",
    "    memory_flops['state_update_final_add'] = B * M * 1 * R * T\n",
    "    \n",
    "    # Output computation: new_state @ Wout\n",
    "    # [B, M, 1, R] @ [M, R, D] -> [B, M, 1, D]\n",
    "    memory_flops['output_mm'] = B * M * R * D * T\n",
    "    \n",
    "    # ==================== ATTENTION MECHANISMS ====================\n",
    "    \n",
    "    attention_flops = {}\n",
    "    \n",
    "    # Attention on previous states\n",
    "    # Q = emb @ Wq: [B, 1, 1, D] @ [M, D, D] -> [B, M, 1, D]\n",
    "    attention_flops['Q_computation'] = B * M * D * D * T\n",
    "    \n",
    "    # K = Sout @ Wk: [B, M, M, D] @ [M, D, D] -> [B, M, M, D]\n",
    "    attention_flops['K_computation'] = B * M * M * D * D * T\n",
    "    \n",
    "    # V = Sout @ Wv: [B, M, M, D] @ [M, D, D] -> [B, M, M, D]\n",
    "    attention_flops['V_computation'] = B * M * M * D * D * T\n",
    "    \n",
    "    # Scaled dot product attention: Q @ K^T\n",
    "    # [B, M, 1, D] @ [B, M, D, M] -> [B, M, 1, M]\n",
    "    attention_flops['QKT_mm'] = B * M * 1 * D * M * T\n",
    "    \n",
    "    # Scale + Softmax\n",
    "    attention_flops['scale_attention'] = B * M * 1 * M * T\n",
    "    attention_flops['attention_softmax'] = B * M * 1 * M * 3 * T  # exp + sum + div\n",
    "    \n",
    "    # Attention weights @ V: [B, M, 1, M] @ [B, M, M, D] -> [B, M, 1, D]\n",
    "    attention_flops['attention_V_mm'] = B * M * 1 * M * D * T\n",
    "    \n",
    "    # Dropout (pas de FLOPs)\n",
    "    \n",
    "    # Residual connection + norm1\n",
    "    attention_flops['residual_add1'] = B * M * D * T\n",
    "    # RMS Norm: sqrt(mean(x^2)) et division (approximation: 3 ops par élément)\n",
    "    attention_flops['norm1'] = B * M * D * 3 * T\n",
    "    \n",
    "    # Self-attention on current state\n",
    "    # SQ, SK, SV computations: [B, M, D] @ [D, D] -> [B, M, D] (x3)\n",
    "    attention_flops['SQ_computation'] = B * M * D * D * T\n",
    "    attention_flops['SK_computation'] = B * M * D * D * T\n",
    "    attention_flops['SV_computation'] = B * M * D * D * T\n",
    "    \n",
    "    # Self-attention: SQ @ SK^T: [B, M, D] @ [B, D, M] -> [B, M, M]\n",
    "    attention_flops['self_QKT_mm'] = B * M * D * M * T\n",
    "    \n",
    "    # Scale + Softmax\n",
    "    attention_flops['self_scale_attention'] = B * M * M * T\n",
    "    attention_flops['self_attention_softmax'] = B * M * M * 3 * T\n",
    "    \n",
    "    # Self-attention weights @ SV: [B, M, M] @ [B, M, D] -> [B, M, D]\n",
    "    attention_flops['self_attention_V_mm'] = B * M * M * D * T\n",
    "    \n",
    "    # Dropout (pas de FLOPs)\n",
    "    \n",
    "    # Residual connection + norm2\n",
    "    attention_flops['residual_add2'] = B * M * D * T\n",
    "    attention_flops['norm2'] = B * M * D * 3 * T\n",
    "    \n",
    "    # ==================== FEED FORWARD ====================\n",
    "    \n",
    "    ff_flops = {}\n",
    "    \n",
    "    # Reduction: [B, M*D] @ [M*D, D] -> [B, D]\n",
    "    ff_flops['reduction_mm'] = B * (M * D) * D * T\n",
    "    \n",
    "    # Feed forward in: [B, D] @ [D, 4*D] -> [B, 4*D]\n",
    "    ff_flops['ff_in_mm'] = B * D * (4 * D) * T\n",
    "    \n",
    "    # GELU activation (approximation: 4 ops par élément)\n",
    "    ff_flops['gelu'] = B * (4 * D) * 4 * T\n",
    "    \n",
    "    # Feed forward out: [B, 4*D] @ [4*D, D] -> [B, D]\n",
    "    ff_flops['ff_out_mm'] = B * (4 * D) * D * T\n",
    "    \n",
    "    # Dropout (pas de FLOPs)\n",
    "    \n",
    "    # Residual connection + norm3\n",
    "    ff_flops['residual_add3'] = B * D * T\n",
    "    ff_flops['norm3'] = B * D * 3 * T\n",
    "    \n",
    "    # ==================== ASSEMBLY LAYER FLOPS ====================\n",
    "    \n",
    "    # Somme des FLOPs pour une couche EST\n",
    "    layer_total = (sum(memory_flops.values()) + \n",
    "                  sum(attention_flops.values()) + \n",
    "                  sum(ff_flops.values()))\n",
    "    \n",
    "    layer_flops['memory'] = memory_flops\n",
    "    layer_flops['attention'] = attention_flops\n",
    "    layer_flops['feed_forward'] = ff_flops\n",
    "    layer_flops['total_per_layer'] = layer_total\n",
    "    \n",
    "    # ==================== OUTPUT PROJECTION ====================\n",
    "    \n",
    "    # Classification: flatten + projection\n",
    "    if model.task_name == 'classification':\n",
    "        # Flatten: [B, T, D] -> [B, T*D] (pas de FLOPs)\n",
    "        # Projection: [B, T*D] @ [T*D, O] -> [B, O]\n",
    "        flops_breakdown['output_projection'] = B * (T * D) * O\n",
    "        # GELU + Dropout (pas de FLOPs pour dropout)\n",
    "        flops_breakdown['output_activation'] = B * (T * D) * 2  # GELU approximation\n",
    "    else:\n",
    "        # Direct projection: [B, T, D] @ [D, O] -> [B, T, O]\n",
    "        flops_breakdown['output_projection'] = B * T * D * O\n",
    "        flops_breakdown['output_activation'] = 0\n",
    "    \n",
    "    # ==================== NORMALIZATION (si applicable) ====================\n",
    "    \n",
    "    normalization_flops = 0\n",
    "    if model.task_name in ['long_term_forecast', 'short_term_forecast']:\n",
    "        # Mean computation: B * T * D operations\n",
    "        normalization_flops += B * T * D\n",
    "        # Std computation: B * T * D operations (var + sqrt)\n",
    "        normalization_flops += B * T * D * 2\n",
    "        # Normalization: B * T * D operations (subtract + divide)\n",
    "        normalization_flops += B * T * D * 2\n",
    "        # Denormalization: B * T * D operations (multiply + add)\n",
    "        normalization_flops += B * T * D * 2\n",
    "    \n",
    "    flops_breakdown['normalization'] = normalization_flops\n",
    "    \n",
    "    # ==================== TOTAL CALCULATION ====================\n",
    "    \n",
    "    flops_breakdown['layers'] = layer_flops\n",
    "    flops_breakdown['total_layers'] = layer_total * L  # Multiplier par le nombre de couches\n",
    "    \n",
    "    total_flops = (flops_breakdown['input_embedding'] + \n",
    "                  flops_breakdown['total_layers'] + \n",
    "                  flops_breakdown['output_projection'] +\n",
    "                  flops_breakdown['output_activation'] +\n",
    "                  flops_breakdown['normalization'])\n",
    "    \n",
    "    flops_breakdown['total'] = total_flops\n",
    "    \n",
    "    return flops_breakdown, total_flops\n",
    "\n",
    "def print_flops_breakdown_est(flops_breakdown, total_flops):\n",
    "    \"\"\"\n",
    "    Affiche un résumé détaillé des FLOPs pour EST v2.\n",
    "    \n",
    "    Parameters:\n",
    "    - flops_breakdown: Dictionnaire des FLOPs par composant\n",
    "    - total_flops: Total des FLOPs\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(f\"ECHO STATE TRANSFORMER V2 - FLOPs BREAKDOWN\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"Input Embedding: {flops_breakdown['input_embedding']:,} FLOPs\")\n",
    "    print(f\"Output Projection: {flops_breakdown['output_projection']:,} FLOPs\")\n",
    "    if flops_breakdown['output_activation'] > 0:\n",
    "        print(f\"Output Activation: {flops_breakdown['output_activation']:,} FLOPs\")\n",
    "    if flops_breakdown['normalization'] > 0:\n",
    "        print(f\"Normalization: {flops_breakdown['normalization']:,} FLOPs\")\n",
    "    \n",
    "    print(f\"\\nPer Layer FLOPs: {flops_breakdown['layers']['total_per_layer']:,}\")\n",
    "    print(f\"Total Layers FLOPs: {flops_breakdown['total_layers']:,}\")\n",
    "    \n",
    "    print(f\"\\n--- Layer Breakdown ---\")\n",
    "    memory_total = sum(flops_breakdown['layers']['memory'].values())\n",
    "    attention_total = sum(flops_breakdown['layers']['attention'].values())\n",
    "    ff_total = sum(flops_breakdown['layers']['feed_forward'].values())\n",
    "    \n",
    "    layer_total = flops_breakdown['layers']['total_per_layer']\n",
    "    print(f\"Memory Operations: {memory_total:,} FLOPs ({memory_total/layer_total*100:.1f}%)\")\n",
    "    print(f\"Attention Mechanisms: {attention_total:,} FLOPs ({attention_total/layer_total*100:.1f}%)\")\n",
    "    print(f\"Feed Forward: {ff_total:,} FLOPs ({ff_total/layer_total*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\n--- Detailed Memory Breakdown ---\")\n",
    "    for component, flops in flops_breakdown['layers']['memory'].items():\n",
    "        print(f\"  {component}: {flops:,} FLOPs\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(f\"TOTAL FLOPs: {total_flops:,}\")\n",
    "    print(f\"TOTAL GFLOPs: {total_flops / 1e9:.3f}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "\n",
    "\n",
    "# Test de la nouvelle fonction count_flops_est_v2 avec le modèle EST\n",
    "from models.EST import Model as EST\n",
    "\n",
    "# Configuration du modèle EST (adaptée à votre nouvelle implémentation)\n",
    "configs_est = type('Config', (), {\n",
    "    'task_name': '',\n",
    "    'pred_len': 10, \n",
    "    'seq_len': 10,\n",
    "    'num_layers': 1,  # Moins de couches\n",
    "    'd_model': 16,   # Dimension plus petite\n",
    "    'dropout': 0.0,\n",
    "    'memory_units': 4,  # Moins d'unités\n",
    "    'memory_dim': 36,   # Dimension plus petite\n",
    "    'memory_connectivity': 0.2,\n",
    "    'enc_in': 10,\n",
    "    'c_out': 10,\n",
    "    'num_class': 10,\n",
    "    'embed': 'timeF',\n",
    "    'freq': 'h'\n",
    "})()\n",
    "\n",
    "# Créer le modèle EST\n",
    "model_est = EST(configs_est)\n",
    "print(\"Model parameters:\", sum(p.numel() for p in model_est.parameters()))\n",
    "\n",
    "# Test avec un exemple\n",
    "batch_size = 32\n",
    "sequence_length = 96\n",
    "flops_breakdown, total_flops = count_flops_est(model_est, batch_size=batch_size, sequence_length=sequence_length)\n",
    "\n",
    "# Afficher les résultats détaillés\n",
    "print_flops_breakdown_est(flops_breakdown, total_flops)\n",
    "\n",
    "# Test avec différentes longueurs de séquence\n",
    "print(f\"\\n=== TEST AVEC DIFFÉRENTES LONGUEURS DE SÉQUENCE ===\")\n",
    "flops_est = []\n",
    "\n",
    "for seq_len in SEQ_LENGTHS:\n",
    "    flops_breakdown, total_flops = count_flops_est(model_est, batch_size=10, sequence_length=seq_len)\n",
    "    print(f\"Sequence Length: {seq_len:4d}, Total FLOPs: {total_flops:12,}, GFLOPs: {total_flops/1e9:8.3f}\")\n",
    "    flops_est.append(total_flops)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3166e47f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/naowak/Thesis/code/Time-Series-Library/tsl_venv/lib/python3.11/site-packages/local_attention/rotary.py:33: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled = False)\n",
      "/Users/naowak/Thesis/code/Time-Series-Library/tsl_venv/lib/python3.11/site-packages/local_attention/rotary.py:55: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled = False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 102272\n",
      "=== MODÈLE TRANSFORMER VANILLA CRÉÉ ===\n",
      "Task: \n",
      "============================================================\n",
      "TRANSFORMER VANILLA - FLOPs BREAKDOWN\n",
      "============================================================\n",
      "Input Embedding: 5,898,240 FLOPs\n",
      "Positional/Temporal Embedding: 786,432 FLOPs\n",
      "Encoder Norm: 589,824 FLOPs\n",
      "\n",
      "=== ENCODER ===\n",
      "Per Encoder Layer FLOPs: 201,326,592\n",
      "Total Encoder Layers FLOPs: 402,653,184\n",
      "\n",
      "--- Encoder Layer Breakdown ---\n",
      "Multi-Head Attention: 98,304,000 FLOPs (48.8%)\n",
      "Feed Forward Network (Conv): 103,022,592 FLOPs (51.2%)\n",
      "\n",
      "--- Detailed Encoder Attention Breakdown ---\n",
      "  Q_projection: 12,582,912 FLOPs\n",
      "  K_projection: 12,582,912 FLOPs\n",
      "  V_projection: 12,582,912 FLOPs\n",
      "  attention_scores: 18,874,368 FLOPs\n",
      "  scale_attention: 2,359,296 FLOPs\n",
      "  attention_softmax: 7,077,888 FLOPs\n",
      "  attention_values: 18,874,368 FLOPs\n",
      "  output_projection: 12,582,912 FLOPs\n",
      "  residual_add: 196,608 FLOPs\n",
      "  layer_norm1: 589,824 FLOPs\n",
      "\n",
      "--- Detailed Encoder Feed Forward Breakdown ---\n",
      "  conv1: 50,331,648 FLOPs\n",
      "  activation: 1,572,864 FLOPs\n",
      "  conv2: 50,331,648 FLOPs\n",
      "  residual_add: 196,608 FLOPs\n",
      "  layer_norm2: 589,824 FLOPs\n",
      "============================================================\n",
      "TOTAL FLOPs: 409,927,680\n",
      "TOTAL GFLOPs: 0.410\n",
      "============================================================\n",
      "\n",
      "=== TEST AVEC DIFFÉRENTES LONGUEURS DE SÉQUENCE ===\n",
      "Sequence Length:    2, Total FLOPs:    2,067,200, GFLOPs:    0.002\n",
      "Sequence Length:    4, Total FLOPs:    4,160,000, GFLOPs:    0.004\n",
      "Sequence Length:    8, Total FLOPs:    8,422,400, GFLOPs:    0.008\n",
      "Sequence Length:   16, Total FLOPs:   17,254,400, GFLOPs:    0.017\n",
      "Sequence Length:   32, Total FLOPs:   36,147,200, GFLOPs:    0.036\n",
      "Sequence Length:   64, Total FLOPs:   78,848,000, GFLOPs:    0.079\n",
      "Sequence Length:  128, Total FLOPs:  183,910,400, GFLOPs:    0.184\n",
      "Sequence Length:  256, Total FLOPs:  472,678,400, GFLOPs:    0.473\n",
      "Sequence Length:  512, Total FLOPs: 1,364,787,200, GFLOPs:    1.365\n",
      "Sequence Length: 1024, Total FLOPs: 4,407,296,000, GFLOPs:    4.407\n",
      "Sequence Length: 2048, Total FLOPs: 15,525,478,400, GFLOPs:   15.525\n",
      "Sequence Length: 4096, Total FLOPs: 57,894,502,400, GFLOPs:   57.895\n",
      "Sequence Length: 8192, Total FLOPs: 223,163,187,200, GFLOPs:  223.163\n",
      "Sequence Length: 16384, Total FLOPs: 875,823,104,000, GFLOPs:  875.823\n",
      "Sequence Length: 32768, Total FLOPs: 3,469,633,126,400, GFLOPs: 3469.633\n",
      "Sequence Length: 65536, Total FLOPs: 13,811,213,926,400, GFLOPs: 13811.214\n",
      "Sequence Length: 131072, Total FLOPs: 55,110,218,547,200, GFLOPs: 55110.219\n",
      "Sequence Length: 262144, Total FLOPs: 220,171,599,872,000, GFLOPs: 220171.600\n",
      "Sequence Length: 524288, Total FLOPs: 880,147,850,854,400, GFLOPs: 880147.851\n",
      "Sequence Length: 1048576, Total FLOPs: 3,519,514,306,150,400, GFLOPs: 3519514.306\n"
     ]
    }
   ],
   "source": [
    "def count_flops_transformer_vanilla(model, batch_size=1, sequence_length=1, configs={}):\n",
    "    \"\"\"\n",
    "    Compte les FLOPs (Floating Point Operations) pour le modèle Transformer Vanilla.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: Instance du modèle Transformer\n",
    "    - batch_size: Taille du batch\n",
    "    - sequence_length: Longueur de la séquence\n",
    "    \n",
    "    Returns:\n",
    "    - dict: Dictionnaire détaillé des FLOPs par composant\n",
    "    - int: Total des FLOPs\n",
    "    \"\"\"\n",
    "    \n",
    "    # Paramètres du modèle\n",
    "    B = batch_size\n",
    "    S = sequence_length  # Sequence length\n",
    "\n",
    "    # Récupérer les paramètres depuis le modèle\n",
    "    # Encoder parameters\n",
    "    encoder_layers = configs.get('e_layers')\n",
    "    d_model = configs.get('d_model')\n",
    "    n_heads = configs.get('n_heads')  # Valeur par défaut si non spécifié\n",
    "    \n",
    "    # Pour d_ff, on regarde les conv layers (conv1 et conv2)\n",
    "    if encoder_layers > 0:\n",
    "        d_ff = configs.get('d_ff')\n",
    "    else:\n",
    "        d_ff = d_model * 4\n",
    "\n",
    "    I = configs.get('enc_in')  # Input features\n",
    "    \n",
    "    # Output dimension selon la tâche\n",
    "    if model.task_name == 'classification':\n",
    "        O = model.projection.out_features\n",
    "    elif hasattr(model, 'projection'):\n",
    "        O = model.projection.out_features\n",
    "    elif model.task_name in ['long_term_forecast', 'short_term_forecast'] and hasattr(model, 'decoder'):\n",
    "        O = model.decoder.projection.out_features\n",
    "    else:\n",
    "        O = d_model\n",
    "    \n",
    "    # Dimension par tête d'attention\n",
    "    head_dim = d_model // n_heads\n",
    "    \n",
    "    flops_breakdown = {}\n",
    "    \n",
    "    # ==================== INPUT EMBEDDING ====================\n",
    "    \n",
    "    # Input embedding: Conv1d tokenization\n",
    "    # Conv1d: [B, I, S] -> [B, d_model, S] avec kernel_size=3, padding=1\n",
    "    conv_kernel_size = model.enc_embedding.value_embedding.tokenConv.kernel_size[0]\n",
    "    flops_breakdown['input_embedding'] = B * S * I * d_model * conv_kernel_size\n",
    "    \n",
    "    # Positional embedding (pas de FLOPs, juste addition)\n",
    "    # Temporal embedding: Linear layer [4 -> d_model]\n",
    "    temporal_features = model.enc_embedding.temporal_embedding.embed.in_features\n",
    "    flops_breakdown['positional_temporal_embedding'] = B * S * temporal_features * d_model\n",
    "    \n",
    "    # ==================== ENCODER LAYERS ====================\n",
    "    \n",
    "    encoder_layer_flops = {}\n",
    "    \n",
    "    # ==================== MULTI-HEAD ATTENTION ====================\n",
    "    \n",
    "    attention_flops = {}\n",
    "    \n",
    "    # Query, Key, Value projections\n",
    "    # Input: [B, S, d_model] -> Output: [B, S, d_model] (pour chacune des 3 projections)\n",
    "    attention_flops['Q_projection'] = B * S * d_model * d_model\n",
    "    attention_flops['K_projection'] = B * S * d_model * d_model\n",
    "    attention_flops['V_projection'] = B * S * d_model * d_model\n",
    "    \n",
    "    # Reshape pour multi-head: [B, S, d_model] -> [B, n_heads, S, head_dim]\n",
    "    # Pas de FLOPs, juste un reshape\n",
    "    \n",
    "    # Attention scores: Q @ K^T\n",
    "    # Q: [B, n_heads, S, head_dim], K^T: [B, n_heads, head_dim, S] -> [B, n_heads, S, S]\n",
    "    attention_flops['attention_scores'] = B * n_heads * S * head_dim * S\n",
    "    \n",
    "    # Scale by sqrt(head_dim)\n",
    "    attention_flops['scale_attention'] = B * n_heads * S * S\n",
    "    \n",
    "    # Softmax sur la dimension des clés (S)\n",
    "    # Approximation: exp + sum + divide = 3 opérations par élément\n",
    "    attention_flops['attention_softmax'] = B * n_heads * S * S * 3\n",
    "    \n",
    "    # Dropout (pas de FLOPs)\n",
    "    \n",
    "    # Attention weights @ V: [B, n_heads, S, S] @ [B, n_heads, S, head_dim] -> [B, n_heads, S, head_dim]\n",
    "    attention_flops['attention_values'] = B * n_heads * S * S * head_dim\n",
    "    \n",
    "    # Concatenate heads: [B, n_heads, S, head_dim] -> [B, S, d_model]\n",
    "    # Pas de FLOPs, juste un reshape\n",
    "    \n",
    "    # Output projection: [B, S, d_model] @ [d_model, d_model] -> [B, S, d_model]\n",
    "    attention_flops['output_projection'] = B * S * d_model * d_model\n",
    "    \n",
    "    # Dropout (pas de FLOPs)\n",
    "    \n",
    "    # Residual connection + LayerNorm1\n",
    "    attention_flops['residual_add'] = B * S * d_model\n",
    "    attention_flops['layer_norm1'] = B * S * d_model * 3  # mean, var, normalize\n",
    "    \n",
    "    # ==================== FEED FORWARD NETWORK (Conv-based) ====================\n",
    "    \n",
    "    feedforward_flops = {}\n",
    "    \n",
    "    # Conv1: [B, d_model, S] -> [B, d_ff, S] avec kernel_size=1\n",
    "    feedforward_flops['conv1'] = B * S * d_model * d_ff\n",
    "    \n",
    "    # Activation (GELU)\n",
    "    # Approximation: 2 opérations par élément pour GELU\n",
    "    feedforward_flops['activation'] = B * S * d_ff * 2\n",
    "    \n",
    "    # Dropout (pas de FLOPs)\n",
    "    \n",
    "    # Conv2: [B, d_ff, S] -> [B, d_model, S] avec kernel_size=1\n",
    "    feedforward_flops['conv2'] = B * S * d_ff * d_model\n",
    "    \n",
    "    # Dropout (pas de FLOPs)\n",
    "    \n",
    "    # Residual connection + LayerNorm2\n",
    "    feedforward_flops['residual_add'] = B * S * d_model\n",
    "    feedforward_flops['layer_norm2'] = B * S * d_model * 3\n",
    "    \n",
    "    # ==================== ASSEMBLY ENCODER LAYER FLOPS ====================\n",
    "    \n",
    "    # Somme des FLOPs pour une couche d'encodeur\n",
    "    encoder_layer_total = sum(attention_flops.values()) + sum(feedforward_flops.values())\n",
    "    \n",
    "    encoder_layer_flops['attention'] = attention_flops\n",
    "    encoder_layer_flops['feedforward'] = feedforward_flops\n",
    "    encoder_layer_flops['total_per_layer'] = encoder_layer_total\n",
    "    \n",
    "    # ==================== LAYER NORM FINALE ENCODER ====================\n",
    "    \n",
    "    encoder_norm_flops = 0\n",
    "    if hasattr(model.encoder, 'norm') and model.encoder.norm is not None:\n",
    "        encoder_norm_flops = B * S * d_model * 3\n",
    "    \n",
    "    # ==================== DECODER (si applicable) ====================\n",
    "    \n",
    "    decoder_flops = 0\n",
    "    decoder_layer_flops = {}\n",
    "    \n",
    "    if model.task_name in ['long_term_forecast', 'short_term_forecast'] and hasattr(model, 'decoder'):\n",
    "        # Paramètres decoder\n",
    "        decoder_layers = len(model.decoder.layers) if hasattr(model.decoder, 'layers') else 0\n",
    "        \n",
    "        if decoder_layers > 0:\n",
    "            # Dec embedding (similaire à enc_embedding)\n",
    "            dec_embedding_flops = B * S * I * d_model * conv_kernel_size + B * S * temporal_features * d_model\n",
    "            \n",
    "            # Pour chaque couche decoder (structure similaire à l'encoder)\n",
    "            decoder_attention_flops = {}\n",
    "            \n",
    "            # Self-attention (avec masque causal)\n",
    "            decoder_attention_flops['self_Q_proj'] = B * S * d_model * d_model\n",
    "            decoder_attention_flops['self_K_proj'] = B * S * d_model * d_model\n",
    "            decoder_attention_flops['self_V_proj'] = B * S * d_model * d_model\n",
    "            decoder_attention_flops['self_attention_scores'] = B * n_heads * S * head_dim * S\n",
    "            decoder_attention_flops['self_scale_attention'] = B * n_heads * S * S\n",
    "            decoder_attention_flops['self_attention_softmax'] = B * n_heads * S * S * 3\n",
    "            decoder_attention_flops['self_attention_values'] = B * n_heads * S * S * head_dim\n",
    "            decoder_attention_flops['self_output_proj'] = B * S * d_model * d_model\n",
    "            decoder_attention_flops['self_residual'] = B * S * d_model\n",
    "            decoder_attention_flops['self_norm1'] = B * S * d_model * 3\n",
    "            \n",
    "            # Cross-attention (decoder vers encoder)\n",
    "            decoder_attention_flops['cross_Q_proj'] = B * S * d_model * d_model\n",
    "            decoder_attention_flops['cross_K_proj'] = B * S * d_model * d_model\n",
    "            decoder_attention_flops['cross_V_proj'] = B * S * d_model * d_model\n",
    "            decoder_attention_flops['cross_attention_scores'] = B * n_heads * S * head_dim * S\n",
    "            decoder_attention_flops['cross_scale_attention'] = B * n_heads * S * S\n",
    "            decoder_attention_flops['cross_attention_softmax'] = B * n_heads * S * S * 3\n",
    "            decoder_attention_flops['cross_attention_values'] = B * n_heads * S * S * head_dim\n",
    "            decoder_attention_flops['cross_output_proj'] = B * S * d_model * d_model\n",
    "            decoder_attention_flops['cross_residual'] = B * S * d_model\n",
    "            decoder_attention_flops['cross_norm2'] = B * S * d_model * 3\n",
    "            \n",
    "            # Feed forward decoder (Conv-based)\n",
    "            decoder_ff_flops = {}\n",
    "            decoder_ff_flops['conv1'] = B * S * d_model * d_ff\n",
    "            decoder_ff_flops['activation'] = B * S * d_ff * 2\n",
    "            decoder_ff_flops['conv2'] = B * S * d_ff * d_model\n",
    "            decoder_ff_flops['residual_add'] = B * S * d_model\n",
    "            decoder_ff_flops['layer_norm3'] = B * S * d_model * 3\n",
    "            \n",
    "            # Total par couche decoder\n",
    "            decoder_layer_total = sum(decoder_attention_flops.values()) + sum(decoder_ff_flops.values())\n",
    "            \n",
    "            decoder_layer_flops['attention'] = decoder_attention_flops\n",
    "            decoder_layer_flops['feedforward'] = decoder_ff_flops\n",
    "            decoder_layer_flops['total_per_layer'] = decoder_layer_total\n",
    "            \n",
    "            # Total decoder\n",
    "            decoder_flops = (dec_embedding_flops + \n",
    "                            decoder_layer_total * decoder_layers + \n",
    "                            B * S * d_model * 3)  # Norm finale\n",
    "            \n",
    "            # Projection finale\n",
    "            decoder_projection_flops = B * S * d_model * O\n",
    "            \n",
    "            flops_breakdown['dec_embedding'] = dec_embedding_flops\n",
    "            flops_breakdown['decoder_layers'] = decoder_layer_total * decoder_layers\n",
    "            flops_breakdown['decoder_projection'] = decoder_projection_flops\n",
    "    \n",
    "    # ==================== OUTPUT PROJECTION ====================\n",
    "    \n",
    "    output_flops = 0\n",
    "    \n",
    "    if model.task_name == 'classification':\n",
    "        # Flatten + GELU + Dropout + Projection\n",
    "        # output = output.reshape(B, -1): pas de FLOPs\n",
    "        # GELU: [B, S * d_model]\n",
    "        gelu_flops = B * S * d_model * 2\n",
    "        # Projection: [B, S * d_model] @ [S * d_model, num_classes] -> [B, num_classes]\n",
    "        projection_flops = B * (S * d_model) * O\n",
    "        \n",
    "        output_flops = gelu_flops + projection_flops\n",
    "        \n",
    "        flops_breakdown['output_activation'] = gelu_flops\n",
    "        flops_breakdown['output_projection'] = projection_flops\n",
    "        \n",
    "    elif model.task_name in ['imputation', 'anomaly_detection']:\n",
    "        # Direct projection: [B, S, d_model] @ [d_model, O] -> [B, S, O]\n",
    "        output_flops = B * S * d_model * O\n",
    "        flops_breakdown['output_projection'] = output_flops\n",
    "    \n",
    "    # ==================== TOTAL CALCULATION ====================\n",
    "    \n",
    "    flops_breakdown['input_embedding'] = flops_breakdown['input_embedding']\n",
    "    flops_breakdown['positional_temporal_embedding'] = flops_breakdown['positional_temporal_embedding']\n",
    "    flops_breakdown['encoder_layers'] = encoder_layer_flops\n",
    "    flops_breakdown['total_encoder_layers'] = encoder_layer_total * encoder_layers\n",
    "    flops_breakdown['encoder_norm'] = encoder_norm_flops\n",
    "    \n",
    "    if decoder_flops > 0:\n",
    "        flops_breakdown['decoder_layers_detail'] = decoder_layer_flops\n",
    "        flops_breakdown['total_decoder'] = decoder_flops\n",
    "    \n",
    "    total_flops = (flops_breakdown['input_embedding'] + \n",
    "                  flops_breakdown['positional_temporal_embedding'] +\n",
    "                  flops_breakdown['total_encoder_layers'] + \n",
    "                  flops_breakdown['encoder_norm'] +\n",
    "                  decoder_flops +\n",
    "                  output_flops)\n",
    "    \n",
    "    flops_breakdown['total'] = total_flops\n",
    "    \n",
    "    return flops_breakdown, total_flops\n",
    "\n",
    "def print_flops_breakdown_transformer_vanilla(flops_breakdown, total_flops):\n",
    "    \"\"\"\n",
    "    Affiche un résumé détaillé des FLOPs pour le Transformer Vanilla.\n",
    "    \n",
    "    Parameters:\n",
    "    - flops_breakdown: Dictionnaire des FLOPs par composant\n",
    "    - total_flops: Total des FLOPs\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(f\"TRANSFORMER VANILLA - FLOPs BREAKDOWN\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"Input Embedding: {flops_breakdown['input_embedding']:,} FLOPs\")\n",
    "    print(f\"Positional/Temporal Embedding: {flops_breakdown['positional_temporal_embedding']:,} FLOPs\")\n",
    "    print(f\"Encoder Norm: {flops_breakdown['encoder_norm']:,} FLOPs\")\n",
    "    \n",
    "    if 'output_projection' in flops_breakdown:\n",
    "        print(f\"Output Projection: {flops_breakdown['output_projection']:,} FLOPs\")\n",
    "    if 'output_activation' in flops_breakdown:\n",
    "        print(f\"Output Activation: {flops_breakdown['output_activation']:,} FLOPs\")\n",
    "    \n",
    "    print(f\"\\n=== ENCODER ===\")\n",
    "    print(f\"Per Encoder Layer FLOPs: {flops_breakdown['encoder_layers']['total_per_layer']:,}\")\n",
    "    print(f\"Total Encoder Layers FLOPs: {flops_breakdown['total_encoder_layers']:,}\")\n",
    "    \n",
    "    print(f\"\\n--- Encoder Layer Breakdown ---\")\n",
    "    encoder_attention_total = sum(flops_breakdown['encoder_layers']['attention'].values())\n",
    "    encoder_ff_total = sum(flops_breakdown['encoder_layers']['feedforward'].values())\n",
    "    encoder_layer_total = flops_breakdown['encoder_layers']['total_per_layer']\n",
    "    \n",
    "    print(f\"Multi-Head Attention: {encoder_attention_total:,} FLOPs ({encoder_attention_total/encoder_layer_total*100:.1f}%)\")\n",
    "    print(f\"Feed Forward Network (Conv): {encoder_ff_total:,} FLOPs ({encoder_ff_total/encoder_layer_total*100:.1f}%)\")\n",
    "    \n",
    "    # Decoder si présent\n",
    "    if 'total_decoder' in flops_breakdown:\n",
    "        print(f\"\\n=== DECODER ===\")\n",
    "        print(f\"Dec Embedding: {flops_breakdown['dec_embedding']:,} FLOPs\")\n",
    "        print(f\"Total Decoder Layers: {flops_breakdown['decoder_layers']:,} FLOPs\")\n",
    "        print(f\"Decoder Projection: {flops_breakdown['decoder_projection']:,} FLOPs\")\n",
    "        print(f\"Total Decoder: {flops_breakdown['total_decoder']:,} FLOPs\")\n",
    "        \n",
    "        if 'decoder_layers_detail' in flops_breakdown:\n",
    "            decoder_attention_total = sum(flops_breakdown['decoder_layers_detail']['attention'].values())\n",
    "            decoder_ff_total = sum(flops_breakdown['decoder_layers_detail']['feedforward'].values())\n",
    "            decoder_layer_total = flops_breakdown['decoder_layers_detail']['total_per_layer']\n",
    "            \n",
    "            print(f\"\\n--- Decoder Layer Breakdown ---\")\n",
    "            print(f\"Attention Mechanisms: {decoder_attention_total:,} FLOPs ({decoder_attention_total/decoder_layer_total*100:.1f}%)\")\n",
    "            print(f\"Feed Forward Network: {decoder_ff_total:,} FLOPs ({decoder_ff_total/decoder_layer_total*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\n--- Detailed Encoder Attention Breakdown ---\")\n",
    "    for component, flops in flops_breakdown['encoder_layers']['attention'].items():\n",
    "        print(f\"  {component}: {flops:,} FLOPs\")\n",
    "    \n",
    "    print(f\"\\n--- Detailed Encoder Feed Forward Breakdown ---\")\n",
    "    for component, flops in flops_breakdown['encoder_layers']['feedforward'].items():\n",
    "        print(f\"  {component}: {flops:,} FLOPs\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(f\"TOTAL FLOPs: {total_flops:,}\")\n",
    "    print(f\"TOTAL GFLOPs: {total_flops / 1e9:.3f}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "# Test du modèle Transformer Vanilla\n",
    "from models.Transformer import Model as TransformerVanilla\n",
    "\n",
    "# Configuration pour le modèle Transformer\n",
    "conf_transformer = {\n",
    "    'task_name': '',\n",
    "    'pred_len': 1,\n",
    "    'seq_len': 10,\n",
    "    'enc_in': 10,        # Features d'entrée\n",
    "    'dec_in': 10,        # Features decoder (si applicable)\n",
    "    'c_out': 1,          # Features de sortie\n",
    "    'num_class': 10,     # Nombre de classes pour classification\n",
    "    'd_model': 64,       # Dimension du modèle\n",
    "    'n_heads': 8,        # Nombre de têtes d'attention\n",
    "    'd_ff': 256,         # Dimension feedforward\n",
    "    'e_layers': 2,       # Nombre de couches encoder\n",
    "    'd_layers': 1,       # Nombre de couches decoder\n",
    "    'factor': 5,         # Facteur pour attention\n",
    "    'dropout': 0.1,\n",
    "    'activation': 'gelu',\n",
    "    'embed': 'timeF',\n",
    "    'freq': 'h'\n",
    "}\n",
    "configs_transformer = type('Config', (), conf_transformer)()\n",
    "\n",
    "# Créer le modèle\n",
    "model_transformer = TransformerVanilla(configs_transformer)\n",
    "print(\"Model parameters:\", sum(p.numel() for p in model_transformer.parameters()))\n",
    "\n",
    "print(\"=== MODÈLE TRANSFORMER VANILLA CRÉÉ ===\")\n",
    "print(f\"Task: {model_transformer.task_name}\")\n",
    "if hasattr(model_transformer, 'decoder'):\n",
    "    print(f\"Decoder layers: {len(model_transformer.decoder.attn_layers)}\")\n",
    "\n",
    "# Test des FLOPs\n",
    "batch_size = 32\n",
    "sequence_length = 96\n",
    "\n",
    "flops_breakdown, total_flops = count_flops_transformer_vanilla(\n",
    "    model_transformer, batch_size=batch_size, sequence_length=sequence_length, configs=conf_transformer\n",
    ")\n",
    "\n",
    "# Afficher les résultats\n",
    "print_flops_breakdown_transformer_vanilla(flops_breakdown, total_flops)\n",
    "\n",
    "# Test avec différentes longueurs de séquence\n",
    "print(f\"\\n=== TEST AVEC DIFFÉRENTES LONGUEURS DE SÉQUENCE ===\")\n",
    "flops_transformer_vanilla = []\n",
    "\n",
    "for seq_len in SEQ_LENGTHS:\n",
    "    flops_breakdown, total_flops = count_flops_transformer_vanilla(\n",
    "        model_transformer, batch_size=10, sequence_length=seq_len, configs=conf_transformer\n",
    "    )\n",
    "    print(f\"Sequence Length: {seq_len:4d}, Total FLOPs: {total_flops:12,}, GFLOPs: {total_flops/1e9:8.3f}\")\n",
    "    flops_transformer_vanilla.append(total_flops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cb3c92e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([447280,\n",
       "  894560,\n",
       "  1789120,\n",
       "  3578240,\n",
       "  7156480,\n",
       "  14312960,\n",
       "  28625920,\n",
       "  57251840,\n",
       "  114503680,\n",
       "  229007360,\n",
       "  458014720,\n",
       "  916029440,\n",
       "  1832058880,\n",
       "  3664117760,\n",
       "  7328235520,\n",
       "  14656471040,\n",
       "  29312942080,\n",
       "  58625884160,\n",
       "  117251768320,\n",
       "  234503536640],\n",
       " [2067200,\n",
       "  4160000,\n",
       "  8422400,\n",
       "  17254400,\n",
       "  36147200,\n",
       "  78848000,\n",
       "  183910400,\n",
       "  472678400,\n",
       "  1364787200,\n",
       "  4407296000,\n",
       "  15525478400,\n",
       "  57894502400,\n",
       "  223163187200,\n",
       "  875823104000,\n",
       "  3469633126400,\n",
       "  13811213926400,\n",
       "  55110218547200,\n",
       "  220171599872000,\n",
       "  880147850854400,\n",
       "  3519514306150400])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flops_est, flops_transformer_vanilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3642c674",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "sns.lineplot(x=SEQ_LENGTHS, y=flops_est, label='EST')\n",
    "sns.lineplot(x=SEQ_LENGTHS, y=flops_transformer_vanilla, label='Transformer')\n",
    "# sns.lineplot(x=SEQ_LENGTHS, y=flops_lstm, label='LSTM')\n",
    "plt.xlabel('Sequence Length')\n",
    "plt.ylabel('FLOPS')\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467bfa24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4781d4df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094b145d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5675fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b089eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tsl_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
