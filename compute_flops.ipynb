{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efcb5c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LENGTHS = [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, \n",
    "           2048, 4096, 8192, 16384, 32768, 65536, 131072, 262144, 524288, 1048576,\n",
    "              2097152, 4194304, 8388608, 16777216]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "424920c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 1055439\n",
      "============================================================\n",
      "ECHO STATE TRANSFORMER V2 - FLOPs BREAKDOWN\n",
      "============================================================\n",
      "Input Embedding: 5,898,240 FLOPs\n",
      "Output Projection: 5,898,240 FLOPs\n",
      "\n",
      "Per Layer FLOPs: 7,046,860,800\n",
      "Total Layers FLOPs: 7,046,860,800\n",
      "\n",
      "--- Layer Breakdown ---\n",
      "Memory Operations: 183,275,520 FLOPs (2.6%)\n",
      "Attention Mechanisms: 5,492,834,304 FLOPs (77.9%)\n",
      "Feed Forward: 1,370,750,976 FLOPs (19.5%)\n",
      "\n",
      "--- Detailed Memory Breakdown ---\n",
      "  adaptive_lr_mm: 2,359,296 FLOPs\n",
      "  adaptive_lr_softmax: 36,864 FLOPs\n",
      "  feed_sparse_mm: 18,874,368 FLOPs\n",
      "  echo_sparse_mm: 6,291,456 FLOPs\n",
      "  echo_bias_add: 786,432 FLOPs\n",
      "  state_update_mul1: 786,432 FLOPs\n",
      "  feed_echo_add: 786,432 FLOPs\n",
      "  tanh: 786,432 FLOPs\n",
      "  state_update_mul2: 786,432 FLOPs\n",
      "  state_update_final_add: 786,432 FLOPs\n",
      "  output_mm: 150,994,944 FLOPs\n",
      "============================================================\n",
      "TOTAL FLOPs: 7,058,657,280\n",
      "TOTAL GFLOPs: 7.059\n",
      "============================================================\n",
      "\n",
      "=== TEST AVEC DIFFÉRENTES LONGUEURS DE SÉQUENCE ===\n",
      "Sequence Length:    1, Total FLOPs:   22,977,400, GFLOPs:    0.023\n",
      "Sequence Length:    2, Total FLOPs:   45,954,800, GFLOPs:    0.046\n",
      "Sequence Length:    4, Total FLOPs:   91,909,600, GFLOPs:    0.092\n",
      "Sequence Length:    8, Total FLOPs:  183,819,200, GFLOPs:    0.184\n",
      "Sequence Length:   16, Total FLOPs:  367,638,400, GFLOPs:    0.368\n",
      "Sequence Length:   32, Total FLOPs:  735,276,800, GFLOPs:    0.735\n",
      "Sequence Length:   64, Total FLOPs: 1,470,553,600, GFLOPs:    1.471\n",
      "Sequence Length:  128, Total FLOPs: 2,941,107,200, GFLOPs:    2.941\n",
      "Sequence Length:  256, Total FLOPs: 5,882,214,400, GFLOPs:    5.882\n",
      "Sequence Length:  512, Total FLOPs: 11,764,428,800, GFLOPs:   11.764\n",
      "Sequence Length: 1024, Total FLOPs: 23,528,857,600, GFLOPs:   23.529\n",
      "Sequence Length: 2048, Total FLOPs: 47,057,715,200, GFLOPs:   47.058\n",
      "Sequence Length: 4096, Total FLOPs: 94,115,430,400, GFLOPs:   94.115\n",
      "Sequence Length: 8192, Total FLOPs: 188,230,860,800, GFLOPs:  188.231\n",
      "Sequence Length: 16384, Total FLOPs: 376,461,721,600, GFLOPs:  376.462\n",
      "Sequence Length: 32768, Total FLOPs: 752,923,443,200, GFLOPs:  752.923\n",
      "Sequence Length: 65536, Total FLOPs: 1,505,846,886,400, GFLOPs: 1505.847\n",
      "Sequence Length: 131072, Total FLOPs: 3,011,693,772,800, GFLOPs: 3011.694\n",
      "Sequence Length: 262144, Total FLOPs: 6,023,387,545,600, GFLOPs: 6023.388\n",
      "Sequence Length: 524288, Total FLOPs: 12,046,775,091,200, GFLOPs: 12046.775\n",
      "Sequence Length: 1048576, Total FLOPs: 24,093,550,182,400, GFLOPs: 24093.550\n",
      "Sequence Length: 2097152, Total FLOPs: 48,187,100,364,800, GFLOPs: 48187.100\n",
      "Sequence Length: 4194304, Total FLOPs: 96,374,200,729,600, GFLOPs: 96374.201\n",
      "Sequence Length: 8388608, Total FLOPs: 192,748,401,459,200, GFLOPs: 192748.401\n",
      "Sequence Length: 16777216, Total FLOPs: 385,496,802,918,400, GFLOPs: 385496.803\n"
     ]
    }
   ],
   "source": [
    "def count_flops_est(model, batch_size=1, sequence_length=1):\n",
    "    \"\"\"\n",
    "    Compte les FLOPs (Floating Point Operations) pour le modèle Echo State Transformer (nouvelle implémentation).\n",
    "    \n",
    "    Parameters:\n",
    "    - model: Instance du modèle EST\n",
    "    - batch_size: Taille du batch\n",
    "    - sequence_length: Longueur de la séquence\n",
    "    \n",
    "    Returns:\n",
    "    - dict: Dictionnaire détaillé des FLOPs par composant\n",
    "    - int: Total des FLOPs\n",
    "    \"\"\"\n",
    "    \n",
    "    # Paramètres du modèle\n",
    "    B = batch_size\n",
    "    T = sequence_length\n",
    "    L = model.num_layers  # Nombre de couches\n",
    "    M = model.memory_units  # Nombre d'unités mémoire\n",
    "    R = model.memory_dim  # Dimension mémoire\n",
    "    D = model.attention_dim  # Dimension attention\n",
    "    I = model.enc_in  # Dimension d'entrée\n",
    "    \n",
    "    # Paramètres de sortie selon la tâche\n",
    "    if hasattr(model.projection, 'out_features'):\n",
    "        O = model.projection.out_features\n",
    "    else:\n",
    "        O = D  # Par défaut\n",
    "    \n",
    "    flops_breakdown = {}\n",
    "    \n",
    "    # ==================== EMBEDDING LAYER ====================\n",
    "    \n",
    "    # Input embedding (DataEmbedding)\n",
    "    # Approximation: principalement la projection linéaire value_embedding\n",
    "    flops_breakdown['input_embedding'] = B * T * I * D\n",
    "    \n",
    "    # ==================== EST LAYER LEVEL (répété L fois pour T timesteps) ====================\n",
    "    \n",
    "    layer_flops = {}\n",
    "    \n",
    "    # ==================== MEMORY FORWARD ====================\n",
    "    \n",
    "    memory_flops = {}\n",
    "    \n",
    "    # Adaptive Leak Rate computation\n",
    "    # X @ adaptive_lr: [B, M, 1, D] @ [M, D, 1] -> [B, M, 1, 1]\n",
    "    memory_flops['adaptive_lr_mm'] = B * M * D * 1 * T\n",
    "    \n",
    "    # Softmax sur adaptive_lr (division par temperature + softmax)\n",
    "    memory_flops['adaptive_lr_softmax'] = B * M * 1 * 1 * 3 * T  # exp + sum + div\n",
    "    \n",
    "    # Feed computation (sparse matrix multiplication)\n",
    "    # Estimation basée sur la connectivité fixe\n",
    "    input_connectivity = model.est_layers[0].memory.input_connectivity\n",
    "    feed_connections = int(input_connectivity * R)  # Nombre de connexions par colonne\n",
    "    # Sparse MM: [B, M, 1, D] avec [M, D, R] -> [B, M, 1, R]\n",
    "    memory_flops['feed_sparse_mm'] = B * M * D * feed_connections * T\n",
    "    \n",
    "    # Echo computation (sparse matrix multiplication + bias)\n",
    "    res_connectivity = model.est_layers[0].memory.res_connectivity\n",
    "    echo_connections = int(res_connectivity * R)  # Nombre de connexions par colonne\n",
    "    # Sparse MM: [B, M, 1, R] avec [M, R, R] -> [B, M, 1, R]\n",
    "    memory_flops['echo_sparse_mm'] = B * M * R * echo_connections * T\n",
    "    # Addition du biais: [B, M, 1, R] + [M, 1, R]\n",
    "    memory_flops['echo_bias_add'] = B * M * 1 * R * T\n",
    "    \n",
    "    # State update computation\n",
    "    # (1 - lr) * state: [B, M, 1, 1] * [B, M, 1, R]\n",
    "    memory_flops['state_update_mul1'] = B * M * 1 * R * T\n",
    "    # lr * tanh(feed + echo): addition + tanh + multiplication\n",
    "    memory_flops['feed_echo_add'] = B * M * 1 * R * T  # feed + echo\n",
    "    memory_flops['tanh'] = B * M * 1 * R * T  # tanh\n",
    "    memory_flops['state_update_mul2'] = B * M * 1 * R * T  # lr * tanh(...)\n",
    "    # Final addition: ((1-lr)*state) + lr*tanh(...)\n",
    "    memory_flops['state_update_final_add'] = B * M * 1 * R * T\n",
    "    \n",
    "    # Output computation: new_state @ Wout\n",
    "    # [B, M, 1, R] @ [M, R, D] -> [B, M, 1, D]\n",
    "    memory_flops['output_mm'] = B * M * R * D * T\n",
    "    \n",
    "    # ==================== ATTENTION MECHANISMS ====================\n",
    "    \n",
    "    attention_flops = {}\n",
    "    \n",
    "    # Attention on previous states\n",
    "    # Q = emb @ Wq: [B, 1, 1, D] @ [M, D, D] -> [B, M, 1, D]\n",
    "    attention_flops['Q_computation'] = B * M * D * D * T\n",
    "    \n",
    "    # K = Sout @ Wk: [B, M, M, D] @ [M, D, D] -> [B, M, M, D]\n",
    "    attention_flops['K_computation'] = B * M * M * D * D * T\n",
    "    \n",
    "    # V = Sout @ Wv: [B, M, M, D] @ [M, D, D] -> [B, M, M, D]\n",
    "    attention_flops['V_computation'] = B * M * M * D * D * T\n",
    "    \n",
    "    # Scaled dot product attention: Q @ K^T\n",
    "    # [B, M, 1, D] @ [B, M, D, M] -> [B, M, 1, M]\n",
    "    attention_flops['QKT_mm'] = B * M * 1 * D * M * T\n",
    "    \n",
    "    # Scale + Softmax\n",
    "    attention_flops['scale_attention'] = B * M * 1 * M * T\n",
    "    attention_flops['attention_softmax'] = B * M * 1 * M * 3 * T  # exp + sum + div\n",
    "    \n",
    "    # Attention weights @ V: [B, M, 1, M] @ [B, M, M, D] -> [B, M, 1, D]\n",
    "    attention_flops['attention_V_mm'] = B * M * 1 * M * D * T\n",
    "    \n",
    "    # Dropout (pas de FLOPs)\n",
    "    \n",
    "    # Residual connection + norm1\n",
    "    attention_flops['residual_add1'] = B * M * D * T\n",
    "    # RMS Norm: sqrt(mean(x^2)) et division (approximation: 3 ops par élément)\n",
    "    attention_flops['norm1'] = B * M * D * 3 * T\n",
    "    \n",
    "    # Self-attention on current state\n",
    "    # SQ, SK, SV computations: [B, M, D] @ [D, D] -> [B, M, D] (x3)\n",
    "    attention_flops['SQ_computation'] = B * M * D * D * T\n",
    "    attention_flops['SK_computation'] = B * M * D * D * T\n",
    "    attention_flops['SV_computation'] = B * M * D * D * T\n",
    "    \n",
    "    # Self-attention: SQ @ SK^T: [B, M, D] @ [B, D, M] -> [B, M, M]\n",
    "    attention_flops['self_QKT_mm'] = B * M * D * M * T\n",
    "    \n",
    "    # Scale + Softmax\n",
    "    attention_flops['self_scale_attention'] = B * M * M * T\n",
    "    attention_flops['self_attention_softmax'] = B * M * M * 3 * T\n",
    "    \n",
    "    # Self-attention weights @ SV: [B, M, M] @ [B, M, D] -> [B, M, D]\n",
    "    attention_flops['self_attention_V_mm'] = B * M * M * D * T\n",
    "    \n",
    "    # Dropout (pas de FLOPs)\n",
    "    \n",
    "    # Residual connection + norm2\n",
    "    attention_flops['residual_add2'] = B * M * D * T\n",
    "    attention_flops['norm2'] = B * M * D * 3 * T\n",
    "    \n",
    "    # ==================== FEED FORWARD ====================\n",
    "    \n",
    "    ff_flops = {}\n",
    "    \n",
    "    # Reduction: [B, M*D] @ [M*D, D] -> [B, D]\n",
    "    ff_flops['reduction_mm'] = B * (M * D) * D * T\n",
    "    \n",
    "    # Feed forward in: [B, D] @ [D, 4*D] -> [B, 4*D]\n",
    "    ff_flops['ff_in_mm'] = B * D * (4 * D) * T\n",
    "    \n",
    "    # GELU activation (approximation: 4 ops par élément)\n",
    "    ff_flops['gelu'] = B * (4 * D) * 4 * T\n",
    "    \n",
    "    # Feed forward out: [B, 4*D] @ [4*D, D] -> [B, D]\n",
    "    ff_flops['ff_out_mm'] = B * (4 * D) * D * T\n",
    "    \n",
    "    # Dropout (pas de FLOPs)\n",
    "    \n",
    "    # Residual connection + norm3\n",
    "    ff_flops['residual_add3'] = B * D * T\n",
    "    ff_flops['norm3'] = B * D * 3 * T\n",
    "    \n",
    "    # ==================== ASSEMBLY LAYER FLOPS ====================\n",
    "    \n",
    "    # Somme des FLOPs pour une couche EST\n",
    "    layer_total = (sum(memory_flops.values()) + \n",
    "                  sum(attention_flops.values()) + \n",
    "                  sum(ff_flops.values()))\n",
    "    \n",
    "    layer_flops['memory'] = memory_flops\n",
    "    layer_flops['attention'] = attention_flops\n",
    "    layer_flops['feed_forward'] = ff_flops\n",
    "    layer_flops['total_per_layer'] = layer_total\n",
    "    \n",
    "    # ==================== OUTPUT PROJECTION ====================\n",
    "    \n",
    "    # Classification: flatten + projection\n",
    "    if model.task_name == 'classification':\n",
    "        # Flatten: [B, T, D] -> [B, T*D] (pas de FLOPs)\n",
    "        # Projection: [B, T*D] @ [T*D, O] -> [B, O]\n",
    "        flops_breakdown['output_projection'] = B * (T * D) * O\n",
    "        # GELU + Dropout (pas de FLOPs pour dropout)\n",
    "        flops_breakdown['output_activation'] = B * (T * D) * 2  # GELU approximation\n",
    "    else:\n",
    "        # Direct projection: [B, T, D] @ [D, O] -> [B, T, O]\n",
    "        flops_breakdown['output_projection'] = B * T * D * O\n",
    "        flops_breakdown['output_activation'] = 0\n",
    "    \n",
    "    # ==================== NORMALIZATION (si applicable) ====================\n",
    "    \n",
    "    normalization_flops = 0\n",
    "    if model.task_name in ['long_term_forecast', 'short_term_forecast']:\n",
    "        # Mean computation: B * T * D operations\n",
    "        normalization_flops += B * T * D\n",
    "        # Std computation: B * T * D operations (var + sqrt)\n",
    "        normalization_flops += B * T * D * 2\n",
    "        # Normalization: B * T * D operations (subtract + divide)\n",
    "        normalization_flops += B * T * D * 2\n",
    "        # Denormalization: B * T * D operations (multiply + add)\n",
    "        normalization_flops += B * T * D * 2\n",
    "    \n",
    "    flops_breakdown['normalization'] = normalization_flops\n",
    "    \n",
    "    # ==================== TOTAL CALCULATION ====================\n",
    "    \n",
    "    flops_breakdown['layers'] = layer_flops\n",
    "    flops_breakdown['total_layers'] = layer_total * L  # Multiplier par le nombre de couches\n",
    "    \n",
    "    total_flops = (flops_breakdown['input_embedding'] + \n",
    "                  flops_breakdown['total_layers'] + \n",
    "                  flops_breakdown['output_projection'] +\n",
    "                  flops_breakdown['output_activation'] +\n",
    "                  flops_breakdown['normalization'])\n",
    "    \n",
    "    flops_breakdown['total'] = total_flops\n",
    "    \n",
    "    return flops_breakdown, total_flops\n",
    "\n",
    "def print_flops_breakdown_est(flops_breakdown, total_flops):\n",
    "    \"\"\"\n",
    "    Affiche un résumé détaillé des FLOPs pour EST v2.\n",
    "    \n",
    "    Parameters:\n",
    "    - flops_breakdown: Dictionnaire des FLOPs par composant\n",
    "    - total_flops: Total des FLOPs\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(f\"ECHO STATE TRANSFORMER V2 - FLOPs BREAKDOWN\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"Input Embedding: {flops_breakdown['input_embedding']:,} FLOPs\")\n",
    "    print(f\"Output Projection: {flops_breakdown['output_projection']:,} FLOPs\")\n",
    "    if flops_breakdown['output_activation'] > 0:\n",
    "        print(f\"Output Activation: {flops_breakdown['output_activation']:,} FLOPs\")\n",
    "    if flops_breakdown['normalization'] > 0:\n",
    "        print(f\"Normalization: {flops_breakdown['normalization']:,} FLOPs\")\n",
    "    \n",
    "    print(f\"\\nPer Layer FLOPs: {flops_breakdown['layers']['total_per_layer']:,}\")\n",
    "    print(f\"Total Layers FLOPs: {flops_breakdown['total_layers']:,}\")\n",
    "    \n",
    "    print(f\"\\n--- Layer Breakdown ---\")\n",
    "    memory_total = sum(flops_breakdown['layers']['memory'].values())\n",
    "    attention_total = sum(flops_breakdown['layers']['attention'].values())\n",
    "    ff_total = sum(flops_breakdown['layers']['feed_forward'].values())\n",
    "    \n",
    "    layer_total = flops_breakdown['layers']['total_per_layer']\n",
    "    print(f\"Memory Operations: {memory_total:,} FLOPs ({memory_total/layer_total*100:.1f}%)\")\n",
    "    print(f\"Attention Mechanisms: {attention_total:,} FLOPs ({attention_total/layer_total*100:.1f}%)\")\n",
    "    print(f\"Feed Forward: {ff_total:,} FLOPs ({ff_total/layer_total*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\n--- Detailed Memory Breakdown ---\")\n",
    "    for component, flops in flops_breakdown['layers']['memory'].items():\n",
    "        print(f\"  {component}: {flops:,} FLOPs\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(f\"TOTAL FLOPs: {total_flops:,}\")\n",
    "    print(f\"TOTAL GFLOPs: {total_flops / 1e9:.3f}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "\n",
    "\n",
    "# Test de la nouvelle fonction count_flops_est_v2 avec le modèle EST\n",
    "from models.EST import Model as EST\n",
    "\n",
    "# Configuration du modèle EST (adaptée à votre nouvelle implémentation)\n",
    "configs_est = type('Config', (), {\n",
    "    'task_name': 'anomaly_detection',  \n",
    "    'pred_len': 10, \n",
    "    'seq_len': 10,\n",
    "    'num_layers': 1,  # Moins de couches\n",
    "    'd_model': 192,   # Dimension plus petite\n",
    "    'dropout': 0.0,\n",
    "    'memory_units': 4,  # Moins d'unités\n",
    "    'memory_dim': 64,   # Dimension plus petite\n",
    "    'memory_connectivity': 0.125,\n",
    "    'enc_in': 10,\n",
    "    'c_out': 10,\n",
    "    'num_class': 10,\n",
    "    'embed': 'timeF',\n",
    "    'freq': 'h'\n",
    "})()\n",
    "\n",
    "# Créer le modèle EST\n",
    "model_est = EST(configs_est)\n",
    "print(\"Model parameters:\", sum(p.numel() for p in model_est.parameters()))\n",
    "\n",
    "# Test avec un exemple\n",
    "batch_size = 32\n",
    "sequence_length = 96\n",
    "flops_breakdown, total_flops = count_flops_est(model_est, batch_size=batch_size, sequence_length=sequence_length)\n",
    "\n",
    "# Afficher les résultats détaillés\n",
    "print_flops_breakdown_est(flops_breakdown, total_flops)\n",
    "\n",
    "# Test avec différentes longueurs de séquence\n",
    "print(f\"\\n=== TEST AVEC DIFFÉRENTES LONGUEURS DE SÉQUENCE ===\")\n",
    "flops_est = []\n",
    "\n",
    "for seq_len in SEQ_LENGTHS:\n",
    "    flops_breakdown, total_flops = count_flops_est(model_est, batch_size=10, sequence_length=seq_len)\n",
    "    print(f\"Sequence Length: {seq_len:4d}, Total FLOPs: {total_flops:12,}, GFLOPs: {total_flops/1e9:8.3f}\")\n",
    "    flops_est.append(total_flops)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3166e47f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/naowak/Thesis/code/Time-Series-Library/tsl_venv/lib/python3.11/site-packages/local_attention/rotary.py:33: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled = False)\n",
      "/Users/naowak/Thesis/code/Time-Series-Library/tsl_venv/lib/python3.11/site-packages/local_attention/rotary.py:55: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled = False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 1063681\n",
      "=== MODÈLE TRANSFORMER VANILLA CRÉÉ ===\n",
      "Task: anomaly_detection\n",
      "============================================================\n",
      "TRANSFORMER VANILLA - FLOPs BREAKDOWN\n",
      "============================================================\n",
      "Input Embedding: 23,592,960 FLOPs\n",
      "Positional/Temporal Embedding: 3,145,728 FLOPs\n",
      "Encoder Norm: 2,359,296 FLOPs\n",
      "Output Projection: 786,432 FLOPs\n",
      "\n",
      "=== ENCODER ===\n",
      "Per Encoder Layer FLOPs: 1,780,482,048\n",
      "Total Encoder Layers FLOPs: 3,560,964,096\n",
      "\n",
      "--- Encoder Layer Breakdown ---\n",
      "Multi-Head Attention: 968,884,224 FLOPs (54.4%)\n",
      "Feed Forward Network (Conv): 811,597,824 FLOPs (45.6%)\n",
      "\n",
      "--- Detailed Encoder Attention Breakdown ---\n",
      "  Q_projection: 201,326,592 FLOPs\n",
      "  K_projection: 201,326,592 FLOPs\n",
      "  V_projection: 201,326,592 FLOPs\n",
      "  attention_scores: 75,497,472 FLOPs\n",
      "  scale_attention: 2,359,296 FLOPs\n",
      "  attention_softmax: 7,077,888 FLOPs\n",
      "  attention_values: 75,497,472 FLOPs\n",
      "  output_projection: 201,326,592 FLOPs\n",
      "  residual_add: 786,432 FLOPs\n",
      "  layer_norm1: 2,359,296 FLOPs\n",
      "\n",
      "--- Detailed Encoder Feed Forward Breakdown ---\n",
      "  conv1: 402,653,184 FLOPs\n",
      "  activation: 3,145,728 FLOPs\n",
      "  conv2: 402,653,184 FLOPs\n",
      "  residual_add: 786,432 FLOPs\n",
      "  layer_norm2: 2,359,296 FLOPs\n",
      "============================================================\n",
      "TOTAL FLOPs: 3,590,848,512\n",
      "TOTAL GFLOPs: 3.591\n",
      "============================================================\n",
      "\n",
      "=== TEST AVEC DIFFÉRENTES LONGUEURS DE SÉQUENCE ===\n",
      "Sequence Length:    1, Total FLOPs:   10,655,360, GFLOPs:    0.011\n",
      "Sequence Length:    2, Total FLOPs:   21,332,480, GFLOPs:    0.021\n",
      "Sequence Length:    4, Total FLOPs:   42,752,000, GFLOPs:    0.043\n",
      "Sequence Length:    8, Total FLOPs:   85,852,160, GFLOPs:    0.086\n",
      "Sequence Length:   16, Total FLOPs:  173,096,960, GFLOPs:    0.173\n",
      "Sequence Length:   32, Total FLOPs:  351,764,480, GFLOPs:    0.352\n",
      "Sequence Length:   64, Total FLOPs:  725,811,200, GFLOPs:    0.726\n",
      "Sequence Length:  128, Total FLOPs: 1,540,751,360, GFLOPs:    1.541\n",
      "Sequence Length:  256, Total FLOPs: 3,438,018,560, GFLOPs:    3.438\n",
      "Sequence Length:  512, Total FLOPs: 8,302,100,480, GFLOPs:    8.302\n",
      "Sequence Length: 1024, Total FLOPs: 22,308,454,400, GFLOPs:   22.308\n",
      "Sequence Length: 2048, Total FLOPs: 67,433,922,560, GFLOPs:   67.434\n",
      "Sequence Length: 4096, Total FLOPs: 226,135,900,160, GFLOPs:  226.136\n",
      "Sequence Length: 8192, Total FLOPs: 817,344,020,480, GFLOPs:  817.344\n",
      "Sequence Length: 16384, Total FLOPs: 3,094,976,921,600, GFLOPs: 3094.977\n",
      "Sequence Length: 32768, Total FLOPs: 12,031,109,365,760, GFLOPs: 12031.109\n",
      "Sequence Length: 65536, Total FLOPs: 47,426,840,821,760, GFLOPs: 47426.841\n",
      "Sequence Length: 131072, Total FLOPs: 188,312,170,004,480, GFLOPs: 188312.170\n",
      "Sequence Length: 262144, Total FLOPs: 750,458,293,452,800, GFLOPs: 750458.293\n",
      "Sequence Length: 524288, Total FLOPs: 2,996,252,400,680,960, GFLOPs: 2996252.401\n",
      "Sequence Length: 1048576, Total FLOPs: 11,973,848,056,463,360, GFLOPs: 11973848.056\n",
      "Sequence Length: 2097152, Total FLOPs: 47,873,069,133,332,480, GFLOPs: 47873069.133\n",
      "Sequence Length: 4194304, Total FLOPs: 191,447,630,348,288,000, GFLOPs: 191447630.348\n",
      "Sequence Length: 8388608, Total FLOPs: 765,701,229,023,068,160, GFLOPs: 765701229.023\n",
      "Sequence Length: 16777216, Total FLOPs: 3,062,626,331,352,104,960, GFLOPs: 3062626331.352\n"
     ]
    }
   ],
   "source": [
    "def count_flops_transformer_vanilla(model, batch_size=1, sequence_length=1, configs={}):\n",
    "    \"\"\"\n",
    "    Compte les FLOPs (Floating Point Operations) pour le modèle Transformer Vanilla.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: Instance du modèle Transformer\n",
    "    - batch_size: Taille du batch\n",
    "    - sequence_length: Longueur de la séquence\n",
    "    \n",
    "    Returns:\n",
    "    - dict: Dictionnaire détaillé des FLOPs par composant\n",
    "    - int: Total des FLOPs\n",
    "    \"\"\"\n",
    "    \n",
    "    # Paramètres du modèle\n",
    "    B = batch_size\n",
    "    S = sequence_length  # Sequence length\n",
    "\n",
    "    # Récupérer les paramètres depuis le modèle\n",
    "    # Encoder parameters\n",
    "    encoder_layers = configs.get('e_layers')\n",
    "    d_model = configs.get('d_model')\n",
    "    n_heads = configs.get('n_heads')  # Valeur par défaut si non spécifié\n",
    "    \n",
    "    # Pour d_ff, on regarde les conv layers (conv1 et conv2)\n",
    "    if encoder_layers > 0:\n",
    "        d_ff = configs.get('d_ff')\n",
    "    else:\n",
    "        d_ff = d_model * 4\n",
    "\n",
    "    I = configs.get('enc_in')  # Input features\n",
    "    \n",
    "    # Output dimension selon la tâche\n",
    "    if model.task_name == 'classification':\n",
    "        O = model.projection.out_features\n",
    "    elif hasattr(model, 'projection'):\n",
    "        O = model.projection.out_features\n",
    "    elif model.task_name in ['long_term_forecast', 'short_term_forecast'] and hasattr(model, 'decoder'):\n",
    "        O = model.decoder.projection.out_features\n",
    "    else:\n",
    "        O = d_model\n",
    "    \n",
    "    # Dimension par tête d'attention\n",
    "    head_dim = d_model // n_heads\n",
    "    \n",
    "    flops_breakdown = {}\n",
    "    \n",
    "    # ==================== INPUT EMBEDDING ====================\n",
    "    \n",
    "    # Input embedding: Conv1d tokenization\n",
    "    # Conv1d: [B, I, S] -> [B, d_model, S] avec kernel_size=3, padding=1\n",
    "    conv_kernel_size = model.enc_embedding.value_embedding.tokenConv.kernel_size[0]\n",
    "    flops_breakdown['input_embedding'] = B * S * I * d_model * conv_kernel_size\n",
    "    \n",
    "    # Positional embedding (pas de FLOPs, juste addition)\n",
    "    # Temporal embedding: Linear layer [4 -> d_model]\n",
    "    temporal_features = model.enc_embedding.temporal_embedding.embed.in_features\n",
    "    flops_breakdown['positional_temporal_embedding'] = B * S * temporal_features * d_model\n",
    "    \n",
    "    # ==================== ENCODER LAYERS ====================\n",
    "    \n",
    "    encoder_layer_flops = {}\n",
    "    \n",
    "    # ==================== MULTI-HEAD ATTENTION ====================\n",
    "    \n",
    "    attention_flops = {}\n",
    "    \n",
    "    # Query, Key, Value projections\n",
    "    # Input: [B, S, d_model] -> Output: [B, S, d_model] (pour chacune des 3 projections)\n",
    "    attention_flops['Q_projection'] = B * S * d_model * d_model\n",
    "    attention_flops['K_projection'] = B * S * d_model * d_model\n",
    "    attention_flops['V_projection'] = B * S * d_model * d_model\n",
    "    \n",
    "    # Reshape pour multi-head: [B, S, d_model] -> [B, n_heads, S, head_dim]\n",
    "    # Pas de FLOPs, juste un reshape\n",
    "    \n",
    "    # Attention scores: Q @ K^T\n",
    "    # Q: [B, n_heads, S, head_dim], K^T: [B, n_heads, head_dim, S] -> [B, n_heads, S, S]\n",
    "    attention_flops['attention_scores'] = B * n_heads * S * head_dim * S\n",
    "    \n",
    "    # Scale by sqrt(head_dim)\n",
    "    attention_flops['scale_attention'] = B * n_heads * S * S\n",
    "    \n",
    "    # Softmax sur la dimension des clés (S)\n",
    "    # Approximation: exp + sum + divide = 3 opérations par élément\n",
    "    attention_flops['attention_softmax'] = B * n_heads * S * S * 3\n",
    "    \n",
    "    # Dropout (pas de FLOPs)\n",
    "    \n",
    "    # Attention weights @ V: [B, n_heads, S, S] @ [B, n_heads, S, head_dim] -> [B, n_heads, S, head_dim]\n",
    "    attention_flops['attention_values'] = B * n_heads * S * S * head_dim\n",
    "    \n",
    "    # Concatenate heads: [B, n_heads, S, head_dim] -> [B, S, d_model]\n",
    "    # Pas de FLOPs, juste un reshape\n",
    "    \n",
    "    # Output projection: [B, S, d_model] @ [d_model, d_model] -> [B, S, d_model]\n",
    "    attention_flops['output_projection'] = B * S * d_model * d_model\n",
    "    \n",
    "    # Dropout (pas de FLOPs)\n",
    "    \n",
    "    # Residual connection + LayerNorm1\n",
    "    attention_flops['residual_add'] = B * S * d_model\n",
    "    attention_flops['layer_norm1'] = B * S * d_model * 3  # mean, var, normalize\n",
    "    \n",
    "    # ==================== FEED FORWARD NETWORK (Conv-based) ====================\n",
    "    \n",
    "    feedforward_flops = {}\n",
    "    \n",
    "    # Conv1: [B, d_model, S] -> [B, d_ff, S] avec kernel_size=1\n",
    "    feedforward_flops['conv1'] = B * S * d_model * d_ff\n",
    "    \n",
    "    # Activation (GELU)\n",
    "    # Approximation: 2 opérations par élément pour GELU\n",
    "    feedforward_flops['activation'] = B * S * d_ff * 2\n",
    "    \n",
    "    # Dropout (pas de FLOPs)\n",
    "    \n",
    "    # Conv2: [B, d_ff, S] -> [B, d_model, S] avec kernel_size=1\n",
    "    feedforward_flops['conv2'] = B * S * d_ff * d_model\n",
    "    \n",
    "    # Dropout (pas de FLOPs)\n",
    "    \n",
    "    # Residual connection + LayerNorm2\n",
    "    feedforward_flops['residual_add'] = B * S * d_model\n",
    "    feedforward_flops['layer_norm2'] = B * S * d_model * 3\n",
    "    \n",
    "    # ==================== ASSEMBLY ENCODER LAYER FLOPS ====================\n",
    "    \n",
    "    # Somme des FLOPs pour une couche d'encodeur\n",
    "    encoder_layer_total = sum(attention_flops.values()) + sum(feedforward_flops.values())\n",
    "    \n",
    "    encoder_layer_flops['attention'] = attention_flops\n",
    "    encoder_layer_flops['feedforward'] = feedforward_flops\n",
    "    encoder_layer_flops['total_per_layer'] = encoder_layer_total\n",
    "    \n",
    "    # ==================== LAYER NORM FINALE ENCODER ====================\n",
    "    \n",
    "    encoder_norm_flops = 0\n",
    "    if hasattr(model.encoder, 'norm') and model.encoder.norm is not None:\n",
    "        encoder_norm_flops = B * S * d_model * 3\n",
    "    \n",
    "    # ==================== DECODER (si applicable) ====================\n",
    "    \n",
    "    decoder_flops = 0\n",
    "    decoder_layer_flops = {}\n",
    "    \n",
    "    if model.task_name in ['long_term_forecast', 'short_term_forecast'] and hasattr(model, 'decoder'):\n",
    "        # Paramètres decoder\n",
    "        decoder_layers = len(model.decoder.layers) if hasattr(model.decoder, 'layers') else 0\n",
    "        \n",
    "        if decoder_layers > 0:\n",
    "            # Dec embedding (similaire à enc_embedding)\n",
    "            dec_embedding_flops = B * S * I * d_model * conv_kernel_size + B * S * temporal_features * d_model\n",
    "            \n",
    "            # Pour chaque couche decoder (structure similaire à l'encoder)\n",
    "            decoder_attention_flops = {}\n",
    "            \n",
    "            # Self-attention (avec masque causal)\n",
    "            decoder_attention_flops['self_Q_proj'] = B * S * d_model * d_model\n",
    "            decoder_attention_flops['self_K_proj'] = B * S * d_model * d_model\n",
    "            decoder_attention_flops['self_V_proj'] = B * S * d_model * d_model\n",
    "            decoder_attention_flops['self_attention_scores'] = B * n_heads * S * head_dim * S\n",
    "            decoder_attention_flops['self_scale_attention'] = B * n_heads * S * S\n",
    "            decoder_attention_flops['self_attention_softmax'] = B * n_heads * S * S * 3\n",
    "            decoder_attention_flops['self_attention_values'] = B * n_heads * S * S * head_dim\n",
    "            decoder_attention_flops['self_output_proj'] = B * S * d_model * d_model\n",
    "            decoder_attention_flops['self_residual'] = B * S * d_model\n",
    "            decoder_attention_flops['self_norm1'] = B * S * d_model * 3\n",
    "            \n",
    "            # Cross-attention (decoder vers encoder)\n",
    "            decoder_attention_flops['cross_Q_proj'] = B * S * d_model * d_model\n",
    "            decoder_attention_flops['cross_K_proj'] = B * S * d_model * d_model\n",
    "            decoder_attention_flops['cross_V_proj'] = B * S * d_model * d_model\n",
    "            decoder_attention_flops['cross_attention_scores'] = B * n_heads * S * head_dim * S\n",
    "            decoder_attention_flops['cross_scale_attention'] = B * n_heads * S * S\n",
    "            decoder_attention_flops['cross_attention_softmax'] = B * n_heads * S * S * 3\n",
    "            decoder_attention_flops['cross_attention_values'] = B * n_heads * S * S * head_dim\n",
    "            decoder_attention_flops['cross_output_proj'] = B * S * d_model * d_model\n",
    "            decoder_attention_flops['cross_residual'] = B * S * d_model\n",
    "            decoder_attention_flops['cross_norm2'] = B * S * d_model * 3\n",
    "            \n",
    "            # Feed forward decoder (Conv-based)\n",
    "            decoder_ff_flops = {}\n",
    "            decoder_ff_flops['conv1'] = B * S * d_model * d_ff\n",
    "            decoder_ff_flops['activation'] = B * S * d_ff * 2\n",
    "            decoder_ff_flops['conv2'] = B * S * d_ff * d_model\n",
    "            decoder_ff_flops['residual_add'] = B * S * d_model\n",
    "            decoder_ff_flops['layer_norm3'] = B * S * d_model * 3\n",
    "            \n",
    "            # Total par couche decoder\n",
    "            decoder_layer_total = sum(decoder_attention_flops.values()) + sum(decoder_ff_flops.values())\n",
    "            \n",
    "            decoder_layer_flops['attention'] = decoder_attention_flops\n",
    "            decoder_layer_flops['feedforward'] = decoder_ff_flops\n",
    "            decoder_layer_flops['total_per_layer'] = decoder_layer_total\n",
    "            \n",
    "            # Total decoder\n",
    "            decoder_flops = (dec_embedding_flops + \n",
    "                            decoder_layer_total * decoder_layers + \n",
    "                            B * S * d_model * 3)  # Norm finale\n",
    "            \n",
    "            # Projection finale\n",
    "            decoder_projection_flops = B * S * d_model * O\n",
    "            \n",
    "            flops_breakdown['dec_embedding'] = dec_embedding_flops\n",
    "            flops_breakdown['decoder_layers'] = decoder_layer_total * decoder_layers\n",
    "            flops_breakdown['decoder_projection'] = decoder_projection_flops\n",
    "    \n",
    "    # ==================== OUTPUT PROJECTION ====================\n",
    "    \n",
    "    output_flops = 0\n",
    "    \n",
    "    if model.task_name == 'classification':\n",
    "        # Flatten + GELU + Dropout + Projection\n",
    "        # output = output.reshape(B, -1): pas de FLOPs\n",
    "        # GELU: [B, S * d_model]\n",
    "        gelu_flops = B * S * d_model * 2\n",
    "        # Projection: [B, S * d_model] @ [S * d_model, num_classes] -> [B, num_classes]\n",
    "        projection_flops = B * (S * d_model) * O\n",
    "        \n",
    "        output_flops = gelu_flops + projection_flops\n",
    "        \n",
    "        flops_breakdown['output_activation'] = gelu_flops\n",
    "        flops_breakdown['output_projection'] = projection_flops\n",
    "        \n",
    "    elif model.task_name in ['imputation', 'anomaly_detection']:\n",
    "        # Direct projection: [B, S, d_model] @ [d_model, O] -> [B, S, O]\n",
    "        output_flops = B * S * d_model * O\n",
    "        flops_breakdown['output_projection'] = output_flops\n",
    "    \n",
    "    # ==================== TOTAL CALCULATION ====================\n",
    "    \n",
    "    flops_breakdown['input_embedding'] = flops_breakdown['input_embedding']\n",
    "    flops_breakdown['positional_temporal_embedding'] = flops_breakdown['positional_temporal_embedding']\n",
    "    flops_breakdown['encoder_layers'] = encoder_layer_flops\n",
    "    flops_breakdown['total_encoder_layers'] = encoder_layer_total * encoder_layers\n",
    "    flops_breakdown['encoder_norm'] = encoder_norm_flops\n",
    "    \n",
    "    if decoder_flops > 0:\n",
    "        flops_breakdown['decoder_layers_detail'] = decoder_layer_flops\n",
    "        flops_breakdown['total_decoder'] = decoder_flops\n",
    "    \n",
    "    total_flops = (flops_breakdown['input_embedding'] + \n",
    "                  flops_breakdown['positional_temporal_embedding'] +\n",
    "                  flops_breakdown['total_encoder_layers'] + \n",
    "                  flops_breakdown['encoder_norm'] +\n",
    "                  decoder_flops +\n",
    "                  output_flops)\n",
    "    \n",
    "    flops_breakdown['total'] = total_flops\n",
    "    \n",
    "    return flops_breakdown, total_flops\n",
    "\n",
    "def print_flops_breakdown_transformer_vanilla(flops_breakdown, total_flops):\n",
    "    \"\"\"\n",
    "    Affiche un résumé détaillé des FLOPs pour le Transformer Vanilla.\n",
    "    \n",
    "    Parameters:\n",
    "    - flops_breakdown: Dictionnaire des FLOPs par composant\n",
    "    - total_flops: Total des FLOPs\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(f\"TRANSFORMER VANILLA - FLOPs BREAKDOWN\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"Input Embedding: {flops_breakdown['input_embedding']:,} FLOPs\")\n",
    "    print(f\"Positional/Temporal Embedding: {flops_breakdown['positional_temporal_embedding']:,} FLOPs\")\n",
    "    print(f\"Encoder Norm: {flops_breakdown['encoder_norm']:,} FLOPs\")\n",
    "    \n",
    "    if 'output_projection' in flops_breakdown:\n",
    "        print(f\"Output Projection: {flops_breakdown['output_projection']:,} FLOPs\")\n",
    "    if 'output_activation' in flops_breakdown:\n",
    "        print(f\"Output Activation: {flops_breakdown['output_activation']:,} FLOPs\")\n",
    "    \n",
    "    print(f\"\\n=== ENCODER ===\")\n",
    "    print(f\"Per Encoder Layer FLOPs: {flops_breakdown['encoder_layers']['total_per_layer']:,}\")\n",
    "    print(f\"Total Encoder Layers FLOPs: {flops_breakdown['total_encoder_layers']:,}\")\n",
    "    \n",
    "    print(f\"\\n--- Encoder Layer Breakdown ---\")\n",
    "    encoder_attention_total = sum(flops_breakdown['encoder_layers']['attention'].values())\n",
    "    encoder_ff_total = sum(flops_breakdown['encoder_layers']['feedforward'].values())\n",
    "    encoder_layer_total = flops_breakdown['encoder_layers']['total_per_layer']\n",
    "    \n",
    "    print(f\"Multi-Head Attention: {encoder_attention_total:,} FLOPs ({encoder_attention_total/encoder_layer_total*100:.1f}%)\")\n",
    "    print(f\"Feed Forward Network (Conv): {encoder_ff_total:,} FLOPs ({encoder_ff_total/encoder_layer_total*100:.1f}%)\")\n",
    "    \n",
    "    # Decoder si présent\n",
    "    if 'total_decoder' in flops_breakdown:\n",
    "        print(f\"\\n=== DECODER ===\")\n",
    "        print(f\"Dec Embedding: {flops_breakdown['dec_embedding']:,} FLOPs\")\n",
    "        print(f\"Total Decoder Layers: {flops_breakdown['decoder_layers']:,} FLOPs\")\n",
    "        print(f\"Decoder Projection: {flops_breakdown['decoder_projection']:,} FLOPs\")\n",
    "        print(f\"Total Decoder: {flops_breakdown['total_decoder']:,} FLOPs\")\n",
    "        \n",
    "        if 'decoder_layers_detail' in flops_breakdown:\n",
    "            decoder_attention_total = sum(flops_breakdown['decoder_layers_detail']['attention'].values())\n",
    "            decoder_ff_total = sum(flops_breakdown['decoder_layers_detail']['feedforward'].values())\n",
    "            decoder_layer_total = flops_breakdown['decoder_layers_detail']['total_per_layer']\n",
    "            \n",
    "            print(f\"\\n--- Decoder Layer Breakdown ---\")\n",
    "            print(f\"Attention Mechanisms: {decoder_attention_total:,} FLOPs ({decoder_attention_total/decoder_layer_total*100:.1f}%)\")\n",
    "            print(f\"Feed Forward Network: {decoder_ff_total:,} FLOPs ({decoder_ff_total/decoder_layer_total*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\n--- Detailed Encoder Attention Breakdown ---\")\n",
    "    for component, flops in flops_breakdown['encoder_layers']['attention'].items():\n",
    "        print(f\"  {component}: {flops:,} FLOPs\")\n",
    "    \n",
    "    print(f\"\\n--- Detailed Encoder Feed Forward Breakdown ---\")\n",
    "    for component, flops in flops_breakdown['encoder_layers']['feedforward'].items():\n",
    "        print(f\"  {component}: {flops:,} FLOPs\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(f\"TOTAL FLOPs: {total_flops:,}\")\n",
    "    print(f\"TOTAL GFLOPs: {total_flops / 1e9:.3f}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "# Test du modèle Transformer Vanilla\n",
    "from models.Transformer import Model as TransformerVanilla\n",
    "\n",
    "# Configuration pour le modèle Transformer\n",
    "conf_transformer = {\n",
    "    'task_name': 'anomaly_detection', \n",
    "    'pred_len': 1,\n",
    "    'seq_len': 10,\n",
    "    'enc_in': 10,        # Features d'entrée\n",
    "    'dec_in': 10,        # Features decoder (si applicable)\n",
    "    'c_out': 1,          # Features de sortie\n",
    "    'num_class': 10,     # Nombre de classes pour classification\n",
    "    'd_model': 256,       # Dimension du modèle\n",
    "    'n_heads': 8,        # Nombre de têtes d'attention\n",
    "    'd_ff': 512,         # Dimension feedforward\n",
    "    'e_layers': 2,       # Nombre de couches encoder\n",
    "    'd_layers': 1,       # Nombre de couches decoder\n",
    "    'factor': 5,         # Facteur pour attention\n",
    "    'dropout': 0.1,\n",
    "    'activation': 'gelu',\n",
    "    'embed': 'timeF',\n",
    "    'freq': 'h'\n",
    "}\n",
    "configs_transformer = type('Config', (), conf_transformer)()\n",
    "\n",
    "# Créer le modèle\n",
    "model_transformer = TransformerVanilla(configs_transformer)\n",
    "print(\"Model parameters:\", sum(p.numel() for p in model_transformer.parameters()))\n",
    "\n",
    "print(\"=== MODÈLE TRANSFORMER VANILLA CRÉÉ ===\")\n",
    "print(f\"Task: {model_transformer.task_name}\")\n",
    "if hasattr(model_transformer, 'decoder'):\n",
    "    print(f\"Decoder layers: {len(model_transformer.decoder.attn_layers)}\")\n",
    "\n",
    "# Test des FLOPs\n",
    "batch_size = 32\n",
    "sequence_length = 96\n",
    "\n",
    "flops_breakdown, total_flops = count_flops_transformer_vanilla(\n",
    "    model_transformer, batch_size=batch_size, sequence_length=sequence_length, configs=conf_transformer\n",
    ")\n",
    "\n",
    "# Afficher les résultats\n",
    "print_flops_breakdown_transformer_vanilla(flops_breakdown, total_flops)\n",
    "\n",
    "# Test avec différentes longueurs de séquence\n",
    "print(f\"\\n=== TEST AVEC DIFFÉRENTES LONGUEURS DE SÉQUENCE ===\")\n",
    "flops_transformer_vanilla = []\n",
    "\n",
    "for seq_len in SEQ_LENGTHS:\n",
    "    flops_breakdown, total_flops = count_flops_transformer_vanilla(\n",
    "        model_transformer, batch_size=10, sequence_length=seq_len, configs=conf_transformer\n",
    "    )\n",
    "    print(f\"Sequence Length: {seq_len:4d}, Total FLOPs: {total_flops:12,}, GFLOPs: {total_flops/1e9:8.3f}\")\n",
    "    flops_transformer_vanilla.append(total_flops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1637993a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 1068096\n",
      "=== MODÈLE MAMBA CRÉÉ ===\n",
      "Task: anomaly_detection\n",
      "d_inner: 384\n",
      "dt_rank: 12\n",
      "Layers: 2\n",
      "============================================================\n",
      "MAMBA - FLOPs BREAKDOWN\n",
      "============================================================\n",
      "Input Embedding: 5,898,240 FLOPs\n",
      "Positional/Temporal Embedding: 1,179,648 FLOPs\n",
      "Final Norm: 1,769,472 FLOPs\n",
      "Output Layer: 5,898,240 FLOPs\n",
      "\n",
      "=== MAMBA LAYERS ===\n",
      "Per Mamba Layer FLOPs: 3,448,111,104\n",
      "Total Mamba Layers FLOPs: 6,896,222,208\n",
      "\n",
      "--- Mamba Layer Breakdown ---\n",
      "RMS Normalization: 1,769,472 FLOPs (0.1%)\n",
      "Input Projection: 452,984,832 FLOPs (13.1%)\n",
      "Convolution 1D: 11,796,480 FLOPs (0.3%)\n",
      "State Space Model: 2,750,939,136 FLOPs (79.8%)\n",
      "Output Processing: 230,031,360 FLOPs (6.7%)\n",
      "Residual Connection: 589,824 FLOPs (0.0%)\n",
      "\n",
      "--- Detailed SSM Breakdown ---\n",
      "  x_proj: 618,135,552 FLOPs\n",
      "  dt_proj: 14,155,776 FLOPs\n",
      "  softplus_delta: 2,359,296 FLOPs\n",
      "  deltaA_einsum: 301,989,888 FLOPs\n",
      "  deltaA_exp: 301,989,888 FLOPs\n",
      "  deltaB_u_einsum: 603,979,776 FLOPs\n",
      "  selective_scan_update: 603,979,776 FLOPs\n",
      "  selective_scan_output: 301,989,888 FLOPs\n",
      "  residual_D: 2,359,296 FLOPs\n",
      "\n",
      "--- Detailed Conv Breakdown ---\n",
      "  conv1d: 9,437,184 FLOPs\n",
      "  silu_activation: 2,359,296 FLOPs\n",
      "============================================================\n",
      "TOTAL FLOPs: 6,910,967,808\n",
      "TOTAL GFLOPs: 6.911\n",
      "============================================================\n",
      "\n",
      "=== TEST AVEC DIFFÉRENTES LONGUEURS DE SÉQUENCE ===\n",
      "Sequence Length:    1, Total FLOPs:   22,496,640, GFLOPs:    0.022\n",
      "Sequence Length:    2, Total FLOPs:   44,993,280, GFLOPs:    0.045\n",
      "Sequence Length:    4, Total FLOPs:   89,986,560, GFLOPs:    0.090\n",
      "Sequence Length:    8, Total FLOPs:  179,973,120, GFLOPs:    0.180\n",
      "Sequence Length:   16, Total FLOPs:  359,946,240, GFLOPs:    0.360\n",
      "Sequence Length:   32, Total FLOPs:  719,892,480, GFLOPs:    0.720\n",
      "Sequence Length:   64, Total FLOPs: 1,439,784,960, GFLOPs:    1.440\n",
      "Sequence Length:  128, Total FLOPs: 2,879,569,920, GFLOPs:    2.880\n",
      "Sequence Length:  256, Total FLOPs: 5,759,139,840, GFLOPs:    5.759\n",
      "Sequence Length:  512, Total FLOPs: 11,518,279,680, GFLOPs:   11.518\n",
      "Sequence Length: 1024, Total FLOPs: 23,036,559,360, GFLOPs:   23.037\n",
      "Sequence Length: 2048, Total FLOPs: 46,073,118,720, GFLOPs:   46.073\n",
      "Sequence Length: 4096, Total FLOPs: 92,146,237,440, GFLOPs:   92.146\n",
      "Sequence Length: 8192, Total FLOPs: 184,292,474,880, GFLOPs:  184.292\n",
      "Sequence Length: 16384, Total FLOPs: 368,584,949,760, GFLOPs:  368.585\n",
      "Sequence Length: 32768, Total FLOPs: 737,169,899,520, GFLOPs:  737.170\n",
      "Sequence Length: 65536, Total FLOPs: 1,474,339,799,040, GFLOPs: 1474.340\n",
      "Sequence Length: 131072, Total FLOPs: 2,948,679,598,080, GFLOPs: 2948.680\n",
      "Sequence Length: 262144, Total FLOPs: 5,897,359,196,160, GFLOPs: 5897.359\n",
      "Sequence Length: 524288, Total FLOPs: 11,794,718,392,320, GFLOPs: 11794.718\n",
      "Sequence Length: 1048576, Total FLOPs: 23,589,436,784,640, GFLOPs: 23589.437\n",
      "Sequence Length: 2097152, Total FLOPs: 47,178,873,569,280, GFLOPs: 47178.874\n",
      "Sequence Length: 4194304, Total FLOPs: 94,357,747,138,560, GFLOPs: 94357.747\n",
      "Sequence Length: 8388608, Total FLOPs: 188,715,494,277,120, GFLOPs: 188715.494\n",
      "Sequence Length: 16777216, Total FLOPs: 377,430,988,554,240, GFLOPs: 377430.989\n"
     ]
    }
   ],
   "source": [
    "def count_flops_mamba(model, batch_size=1, sequence_length=1, configs={}):\n",
    "    \"\"\"\n",
    "    Compte les FLOPs (Floating Point Operations) pour le modèle Mamba.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: Instance du modèle Mamba\n",
    "    - batch_size: Taille du batch\n",
    "    - sequence_length: Longueur de la séquence\n",
    "    - configs: Configuration du modèle (dict)\n",
    "    \n",
    "    Returns:\n",
    "    - dict: Dictionnaire détaillé des FLOPs par composant\n",
    "    - int: Total des FLOPs\n",
    "    \"\"\"\n",
    "    \n",
    "    # Paramètres du modèle\n",
    "    B = batch_size\n",
    "    L = sequence_length  # Sequence length\n",
    "    \n",
    "    # Récupérer les paramètres depuis la configuration\n",
    "    d_model = configs.get('d_model')  # Dimension du modèle\n",
    "    expand = configs.get('expand', 2)  # Facteur d'expansion\n",
    "    d_inner = d_model * expand  # Dimension interne\n",
    "    dt_rank = math.ceil(d_model / 16)  # Rang pour delta (même calcul que dans le modèle)\n",
    "    n_layers = configs.get('e_layers')  # Nombre de couches\n",
    "    \n",
    "    # Input/Output dimensions\n",
    "    I = configs.get('enc_in')  # Input features\n",
    "    O = configs.get('c_out')  # Output features\n",
    "    \n",
    "    # Paramètres SSM\n",
    "    n = configs.get('d_ff')  # d_ff (dimension état)\n",
    "    d_conv = configs.get('d_conv')  # Taille kernel conv\n",
    "    \n",
    "    flops_breakdown = {}\n",
    "    \n",
    "    # ==================== INPUT EMBEDDING ====================\n",
    "    \n",
    "    # Input embedding: X @ W -> [B, L, I] @ [I, d_model] -> [B, L, d_model]\n",
    "    flops_breakdown['input_embedding'] = B * L * I * d_model\n",
    "    \n",
    "    # Positional/Temporal embedding (approximation)\n",
    "    flops_breakdown['positional_temporal_embedding'] = B * L * d_model * 2\n",
    "    \n",
    "    # ==================== MAMBA LAYERS ====================\n",
    "    \n",
    "    mamba_layer_flops = {}\n",
    "    \n",
    "    # ==================== RMS NORM (avant chaque couche) ====================\n",
    "    \n",
    "    norm_flops = {}\n",
    "    \n",
    "    # RMS Norm: sqrt(mean(x^2)) + scale\n",
    "    # Approximation: 3 opérations par élément (square, mean, sqrt+scale)\n",
    "    norm_flops['rms_norm'] = B * L * d_model * 3\n",
    "    \n",
    "    # ==================== MAMBA BLOCK ====================\n",
    "    \n",
    "    mamba_block_flops = {}\n",
    "    \n",
    "    # Input projection: [B, L, d_model] @ [d_model, 2*d_inner] -> [B, L, 2*d_inner]\n",
    "    mamba_block_flops['in_proj'] = B * L * d_model * (2 * d_inner)\n",
    "    \n",
    "    # Split en x et res (pas de FLOPs)\n",
    "    \n",
    "    # ==================== CONVOLUTION 1D ====================\n",
    "    \n",
    "    conv_flops = {}\n",
    "    \n",
    "    # Conv1d: [B, d_inner, L] avec kernel_size=d_conv, groups=d_inner\n",
    "    # Chaque groupe fait une convolution indépendante\n",
    "    conv_flops['conv1d'] = B * L * d_inner * d_conv\n",
    "    \n",
    "    # SiLU activation: approximation 2 opérations par élément\n",
    "    conv_flops['silu_activation'] = B * L * d_inner * 2\n",
    "    \n",
    "    # ==================== STATE SPACE MODEL (SSM) ====================\n",
    "    \n",
    "    ssm_flops = {}\n",
    "    \n",
    "    # x_proj: [B, L, d_inner] @ [d_inner, dt_rank + 2*n] -> [B, L, dt_rank + 2*n]\n",
    "    ssm_flops['x_proj'] = B * L * d_inner * (dt_rank + 2 * n)\n",
    "    \n",
    "    # Split en delta, B, C (pas de FLOPs)\n",
    "    \n",
    "    # dt_proj: [B, L, dt_rank] @ [dt_rank, d_inner] -> [B, L, d_inner]\n",
    "    ssm_flops['dt_proj'] = B * L * dt_rank * d_inner\n",
    "    \n",
    "    # Softplus sur delta: approximation 2 opérations par élément\n",
    "    ssm_flops['softplus_delta'] = B * L * d_inner * 2\n",
    "    \n",
    "    # Selective scan computation\n",
    "    # deltaA = exp(delta ⊗ A): [B, L, d_inner, n]\n",
    "    # Einsum: delta [B, L, d_inner] × A [d_inner, n] -> [B, L, d_inner, n]\n",
    "    ssm_flops['deltaA_einsum'] = B * L * d_inner * n\n",
    "    ssm_flops['deltaA_exp'] = B * L * d_inner * n\n",
    "    \n",
    "    # deltaB_u = delta ⊗ B ⊗ u: [B, L, d_inner, n]\n",
    "    # Triple einsum: delta [B, L, d_inner] × B [B, L, n] × u [B, L, d_inner]\n",
    "    ssm_flops['deltaB_u_einsum'] = B * L * d_inner * n * 2  # 2 multiplications\n",
    "    \n",
    "    # Selective scan séquentiel (pour L timesteps)\n",
    "    # Pour chaque timestep i:\n",
    "    #   x = deltaA[:, i] * x + deltaB_u[:, i]: [B, d_inner, n]\n",
    "    #   y = x ⊗ C[:, i]: [B, d_inner] \n",
    "    ssm_flops['selective_scan_update'] = L * B * d_inner * n * 2  # mult + add pour x\n",
    "    ssm_flops['selective_scan_output'] = L * B * d_inner * n      # einsum pour y\n",
    "    \n",
    "    # Residual connection avec D: y + u * D\n",
    "    ssm_flops['residual_D'] = B * L * d_inner * 2  # mult + add\n",
    "    \n",
    "    # ==================== OUTPUT PROCESSING ====================\n",
    "    \n",
    "    output_flops = {}\n",
    "    \n",
    "    # SiLU sur res: approximation 2 opérations par élément\n",
    "    output_flops['silu_res'] = B * L * d_inner * 2\n",
    "    \n",
    "    # Element-wise multiplication: y * silu(res)\n",
    "    output_flops['elementwise_mult'] = B * L * d_inner\n",
    "    \n",
    "    # Output projection: [B, L, d_inner] @ [d_inner, d_model] -> [B, L, d_model]\n",
    "    output_flops['out_proj'] = B * L * d_inner * d_model\n",
    "    \n",
    "    # ==================== RESIDUAL CONNECTION ====================\n",
    "    \n",
    "    residual_flops = {}\n",
    "    \n",
    "    # Residual add: mixer_output + input\n",
    "    residual_flops['residual_add'] = B * L * d_model\n",
    "    \n",
    "    # ==================== ASSEMBLY MAMBA LAYER FLOPS ====================\n",
    "    \n",
    "    # Somme des FLOPs pour une couche Mamba\n",
    "    mamba_layer_total = (sum(norm_flops.values()) + \n",
    "                        sum(mamba_block_flops.values()) + \n",
    "                        sum(conv_flops.values()) + \n",
    "                        sum(ssm_flops.values()) + \n",
    "                        sum(output_flops.values()) + \n",
    "                        sum(residual_flops.values()))\n",
    "    \n",
    "    mamba_layer_flops['norm'] = norm_flops\n",
    "    mamba_layer_flops['mamba_block'] = mamba_block_flops\n",
    "    mamba_layer_flops['conv'] = conv_flops\n",
    "    mamba_layer_flops['ssm'] = ssm_flops\n",
    "    mamba_layer_flops['output'] = output_flops\n",
    "    mamba_layer_flops['residual'] = residual_flops\n",
    "    mamba_layer_flops['total_per_layer'] = mamba_layer_total\n",
    "    \n",
    "    # ==================== FINAL NORMALIZATION ====================\n",
    "    \n",
    "    final_norm_flops = B * L * d_model * 3\n",
    "    \n",
    "    # ==================== OUTPUT LAYER ====================\n",
    "    \n",
    "    # Output linear layer: [B, L, d_model] @ [d_model, O] -> [B, L, O]\n",
    "    output_layer_flops = B * L * d_model * O\n",
    "    \n",
    "    # ==================== NORMALIZATION (pour forecasting) ====================\n",
    "    \n",
    "    normalization_flops = 0\n",
    "    if configs.get('task_name') in ['long_term_forecast', 'short_term_forecast']:\n",
    "        # Mean computation: B * L * I operations\n",
    "        normalization_flops += B * L * I\n",
    "        # Std computation: var + sqrt\n",
    "        normalization_flops += B * L * I * 2\n",
    "        # Normalization: subtract + divide\n",
    "        normalization_flops += B * L * I * 2\n",
    "        # Denormalization: multiply + add\n",
    "        normalization_flops += B * L * I * 2\n",
    "    \n",
    "    # ==================== TOTAL CALCULATION ====================\n",
    "    \n",
    "    flops_breakdown['input_embedding'] = flops_breakdown['input_embedding']\n",
    "    flops_breakdown['positional_temporal_embedding'] = flops_breakdown['positional_temporal_embedding']\n",
    "    flops_breakdown['mamba_layers'] = mamba_layer_flops\n",
    "    flops_breakdown['total_mamba_layers'] = mamba_layer_total * n_layers\n",
    "    flops_breakdown['final_norm'] = final_norm_flops\n",
    "    flops_breakdown['output_layer'] = output_layer_flops\n",
    "    flops_breakdown['normalization'] = normalization_flops\n",
    "    \n",
    "    total_flops = (flops_breakdown['input_embedding'] + \n",
    "                  flops_breakdown['positional_temporal_embedding'] +\n",
    "                  flops_breakdown['total_mamba_layers'] + \n",
    "                  flops_breakdown['final_norm'] +\n",
    "                  flops_breakdown['output_layer'] +\n",
    "                  flops_breakdown['normalization'])\n",
    "    \n",
    "    flops_breakdown['total'] = total_flops\n",
    "    \n",
    "    return flops_breakdown, total_flops\n",
    "\n",
    "def print_flops_breakdown_mamba(flops_breakdown, total_flops):\n",
    "    \"\"\"\n",
    "    Affiche un résumé détaillé des FLOPs pour le modèle Mamba.\n",
    "    Parameters:\n",
    "    - flops_breakdown: Dictionnaire des FLOPs par composant\n",
    "    - total_flops: Total des FLOPs\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(f\"MAMBA - FLOPs BREAKDOWN\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"Input Embedding: {flops_breakdown['input_embedding']:,} FLOPs\")\n",
    "    print(f\"Positional/Temporal Embedding: {flops_breakdown['positional_temporal_embedding']:,} FLOPs\")\n",
    "    print(f\"Final Norm: {flops_breakdown['final_norm']:,} FLOPs\")\n",
    "    print(f\"Output Layer: {flops_breakdown['output_layer']:,} FLOPs\")\n",
    "    if flops_breakdown['normalization'] > 0:\n",
    "        print(f\"Normalization: {flops_breakdown['normalization']:,} FLOPs\")\n",
    "    \n",
    "    print(f\"\\n=== MAMBA LAYERS ===\")\n",
    "    print(f\"Per Mamba Layer FLOPs: {flops_breakdown['mamba_layers']['total_per_layer']:,}\")\n",
    "    print(f\"Total Mamba Layers FLOPs: {flops_breakdown['total_mamba_layers']:,}\")\n",
    "    \n",
    "    print(f\"\\n--- Mamba Layer Breakdown ---\")\n",
    "    norm_total = sum(flops_breakdown['mamba_layers']['norm'].values())\n",
    "    mamba_block_total = sum(flops_breakdown['mamba_layers']['mamba_block'].values())\n",
    "    conv_total = sum(flops_breakdown['mamba_layers']['conv'].values())\n",
    "    ssm_total = sum(flops_breakdown['mamba_layers']['ssm'].values())\n",
    "    output_total = sum(flops_breakdown['mamba_layers']['output'].values())\n",
    "    residual_total = sum(flops_breakdown['mamba_layers']['residual'].values())\n",
    "    \n",
    "    layer_total = flops_breakdown['mamba_layers']['total_per_layer']\n",
    "    print(f\"RMS Normalization: {norm_total:,} FLOPs ({norm_total/layer_total*100:.1f}%)\")\n",
    "    print(f\"Input Projection: {mamba_block_total:,} FLOPs ({mamba_block_total/layer_total*100:.1f}%)\")\n",
    "    print(f\"Convolution 1D: {conv_total:,} FLOPs ({conv_total/layer_total*100:.1f}%)\")\n",
    "    print(f\"State Space Model: {ssm_total:,} FLOPs ({ssm_total/layer_total*100:.1f}%)\")\n",
    "    print(f\"Output Processing: {output_total:,} FLOPs ({output_total/layer_total*100:.1f}%)\")\n",
    "    print(f\"Residual Connection: {residual_total:,} FLOPs ({residual_total/layer_total*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\n--- Detailed SSM Breakdown ---\")\n",
    "    for component, flops in flops_breakdown['mamba_layers']['ssm'].items():\n",
    "        print(f\"  {component}: {flops:,} FLOPs\")\n",
    "    \n",
    "    print(f\"\\n--- Detailed Conv Breakdown ---\")\n",
    "    for component, flops in flops_breakdown['mamba_layers']['conv'].items():\n",
    "        print(f\"  {component}: {flops:,} FLOPs\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(f\"TOTAL FLOPs: {total_flops:,}\")\n",
    "    print(f\"TOTAL GFLOPs: {total_flops / 1e9:.3f}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "# Test du modèle Mamba\n",
    "from models.MambaSimple import Model as MambaSimple\n",
    "import math\n",
    "\n",
    "# Configuration pour le modèle Mamba\n",
    "conf_mamba = {\n",
    "    'task_name': 'anomaly_detection',\n",
    "    'pred_len': 96,\n",
    "    'seq_len': 96,\n",
    "    'enc_in': 10,        # Features d'entrée\n",
    "    'c_out': 10,         # Features de sortie\n",
    "    'd_model': 192,       # Dimension du modèle\n",
    "    'expand': 2,         # Facteur d'expansion (d_inner = d_model * expand)\n",
    "    'd_conv': 8,         # Taille du kernel de convolution\n",
    "    'd_ff': 256,          # Dimension état SSM\n",
    "    'e_layers': 2,       # Nombre de couches\n",
    "    'dropout': 0.1,\n",
    "    'embed': 'timeF',\n",
    "    'freq': 'h'\n",
    "}\n",
    "configs_mamba = type('Config', (), conf_mamba)()\n",
    "\n",
    "# Créer le modèle Mamba\n",
    "model_mamba = MambaSimple(configs_mamba)\n",
    "print(\"Model parameters:\", sum(p.numel() for p in model_mamba.parameters()))\n",
    "\n",
    "print(\"=== MODÈLE MAMBA CRÉÉ ===\")\n",
    "print(f\"Task: {model_mamba.task_name}\")\n",
    "print(f\"d_inner: {model_mamba.d_inner}\")\n",
    "print(f\"dt_rank: {model_mamba.dt_rank}\")\n",
    "print(f\"Layers: {len(model_mamba.layers)}\")\n",
    "\n",
    "# Test des FLOPs\n",
    "batch_size = 32\n",
    "sequence_length = 96\n",
    "\n",
    "flops_breakdown, total_flops = count_flops_mamba(\n",
    "    model_mamba, batch_size=batch_size, sequence_length=sequence_length, configs=conf_mamba\n",
    ")\n",
    "\n",
    "# Afficher les résultats\n",
    "print_flops_breakdown_mamba(flops_breakdown, total_flops)\n",
    "\n",
    "# Test avec différentes longueurs de séquence\n",
    "print(f\"\\n=== TEST AVEC DIFFÉRENTES LONGUEURS DE SÉQUENCE ===\")\n",
    "flops_mamba = []\n",
    "\n",
    "for seq_len in SEQ_LENGTHS:\n",
    "    flops_breakdown, total_flops = count_flops_mamba(\n",
    "        model_mamba, batch_size=10, sequence_length=seq_len, configs=conf_mamba\n",
    "    )\n",
    "    print(f\"Sequence Length: {seq_len:4d}, Total FLOPs: {total_flops:12,}, GFLOPs: {total_flops/1e9:8.3f}\")\n",
    "    flops_mamba.append(total_flops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "465537e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 1061114\n",
      "============================================================\n",
      "REFORMER - FLOPs BREAKDOWN\n",
      "============================================================\n",
      "Input Embedding: 8,601,600 FLOPs\n",
      "Temporal Embedding: 3,440,640 FLOPs\n",
      "Encoder Norm: 2,580,480 FLOPs\n",
      "Output Projection: 8,601,600 FLOPs\n",
      "\n",
      "=== ENCODER ===\n",
      "Per Encoder Layer FLOPs: 1,620,570,192.0768354\n",
      "Total Encoder Layers FLOPs: 3,241,140,384.153671\n",
      "\n",
      "--- Encoder Layer Breakdown ---\n",
      "LSH Attention: 730,034,256.0768354 FLOPs (45.0%)\n",
      "Reversible Layers: 890,535,936 FLOPs (55.0%)\n",
      "\n",
      "--- Detailed LSH Attention Breakdown ---\n",
      "  Q_projection: 240,844,800 FLOPs\n",
      "  K_projection: 240,844,800 FLOPs\n",
      "  V_projection: 240,844,800 FLOPs\n",
      "  bucket_sorting: 323,664.0768354463 FLOPs\n",
      "  bucket_attention_scores: 3,440,640 FLOPs\n",
      "  bucket_attention_softmax: 294,912 FLOPs\n",
      "  bucket_attention_values: 3,440,640 FLOPs\n",
      "\n",
      "--- Detailed Reversible Layers Breakdown ---\n",
      "  F_linear1: 220,200,960 FLOPs\n",
      "  F_activation: 3,145,728 FLOPs\n",
      "  F_linear2: 220,200,960 FLOPs\n",
      "  F_residual_add: 430,080 FLOPs\n",
      "  G_linear1: 220,200,960 FLOPs\n",
      "  G_activation: 3,145,728 FLOPs\n",
      "  G_linear2: 220,200,960 FLOPs\n",
      "  G_residual_add: 430,080 FLOPs\n",
      "  layer_norm2: 2,580,480 FLOPs\n",
      "============================================================\n",
      "TOTAL FLOPs: 3,264,364,704.153671\n",
      "TOTAL GFLOPs: 3.264\n",
      "============================================================\n",
      "\n",
      "=== TEST AVEC DIFFÉRENTES LONGUEURS DE SÉQUENCE ===\n",
      "Sequence Length:    1, Total FLOPs: 10,624,080.0, GFLOPs:    0.011\n",
      "Sequence Length:    2, Total FLOPs: 21,248,800.0, GFLOPs:    0.021\n",
      "Sequence Length:    4, Total FLOPs: 42,498,880.0, GFLOPs:    0.042\n",
      "Sequence Length:    8, Total FLOPs: 85,000,320.0, GFLOPs:    0.085\n",
      "Sequence Length:   16, Total FLOPs: 170,005,760.0, GFLOPs:    0.170\n",
      "Sequence Length:   32, Total FLOPs: 340,021,760.0, GFLOPs:    0.340\n",
      "Sequence Length:   64, Total FLOPs: 680,064,000.0, GFLOPs:    0.680\n",
      "Sequence Length:  128, Total FLOPs: 1,360,168,960.0, GFLOPs:    1.360\n",
      "Sequence Length:  256, Total FLOPs: 2,720,419,840.0, GFLOPs:    2.720\n",
      "Sequence Length:  512, Total FLOPs: 5,441,003,520.0, GFLOPs:    5.441\n",
      "Sequence Length: 1024, Total FLOPs: 10,882,334,720.0, GFLOPs:   10.882\n",
      "Sequence Length: 2048, Total FLOPs: 21,765,324,800.0, GFLOPs:   21.765\n",
      "Sequence Length: 4096, Total FLOPs: 43,531,960,320.0, GFLOPs:   43.532\n",
      "Sequence Length: 8192, Total FLOPs: 87,066,542,080.0, GFLOPs:   87.067\n",
      "Sequence Length: 16384, Total FLOPs: 174,138,327,040.0, GFLOPs:  174.138\n",
      "Sequence Length: 32768, Total FLOPs: 348,287,139,840.0, GFLOPs:  348.287\n",
      "Sequence Length: 65536, Total FLOPs: 696,595,251,200.0, GFLOPs:  696.595\n",
      "Sequence Length: 131072, Total FLOPs: 1,393,232,445,440.0, GFLOPs: 1393.232\n",
      "Sequence Length: 262144, Total FLOPs: 2,786,548,776,960.0, GFLOPs: 2786.549\n",
      "Sequence Length: 524288, Total FLOPs: 5,573,265,326,080.0, GFLOPs: 5573.265\n",
      "Sequence Length: 1048576, Total FLOPs: 11,146,866,196,480.0, GFLOPs: 11146.866\n",
      "Sequence Length: 2097152, Total FLOPs: 22,294,403,481,600.0, GFLOPs: 22294.403\n",
      "Sequence Length: 4194304, Total FLOPs: 44,590,149,140,480.0, GFLOPs: 44590.149\n",
      "Sequence Length: 8388608, Total FLOPs: 89,182,982,635,520.0, GFLOPs: 89182.983\n",
      "Sequence Length: 16777216, Total FLOPs: 178,371,333,980,160.0, GFLOPs: 178371.334\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def count_flops_reformer_fixed(model, batch_size=1, sequence_length=1, configs={}):\n",
    "    B = batch_size\n",
    "    L = sequence_length\n",
    "\n",
    "    d_model = configs['d_model']\n",
    "    n_heads = configs['n_heads']\n",
    "    d_ff = configs['d_ff']\n",
    "    e_layers = configs['e_layers']\n",
    "    bucket_size = configs.get('bucket_size', 64)   # typiquement plus grand que 4 pour expérimenter scaling\n",
    "    n_hashes = configs.get('n_hashes', 2)\n",
    "    I = configs.get('enc_in', 1)\n",
    "    O = configs.get('c_out', 1)\n",
    "    head_dim = d_model // n_heads\n",
    "    # constants: we count multiply+add as 2 FLOPs per MAC when appropriate\n",
    "    flops = {}\n",
    "    # Input embedding (linear): B * L * I * d_model * 2 (mul+add)\n",
    "    flops['input_embedding'] = 2 * B * L * I * d_model\n",
    "    flops['temporal_embedding'] = 2 * B * L * 4 * d_model\n",
    "\n",
    "    # ----- Per encoder layer -----\n",
    "    per_layer = {}\n",
    "    # QKV projections: 3 × (B * L * d_model * d_model * 2)\n",
    "    per_layer['QKV_proj'] = 3 * 2 * B * L * d_model * d_model\n",
    "\n",
    "    # Output projection after heads (W_o : d_model x d_model)\n",
    "    per_layer['output_proj'] = 2 * B * L * d_model * d_model\n",
    "\n",
    "    # LSH hashing cost: approximate cost to compute hashes (random projection) ~ 2 * d_model per hash\n",
    "    per_layer['hash_proj'] = 2 * B * n_heads * n_hashes * L * d_model\n",
    "\n",
    "    # Sorting cost (dominant L log L term): we model as c_sort * B * n_heads * n_hashes * L * log2(L)\n",
    "    c_sort = 1.0   # coefficient (depends on sort implementation) — tune if needed\n",
    "    per_layer['bucket_sorting'] = c_sort * B * n_heads * n_hashes * L * math.log2(max(2, L))\n",
    "\n",
    "    # Attention inside buckets:\n",
    "    # number of buckets = ceil(L / bucket_size)\n",
    "    num_buckets = max(1, math.ceil(L / bucket_size))\n",
    "    # cost per bucket: for a bucket of size b, score computation ~ b^2 * head_dim * 2 (QK^T multiplies+adds)\n",
    "    # assume most buckets have size ~bucket_size, last may be smaller\n",
    "    b = bucket_size\n",
    "    per_layer['bucket_attention_scores'] = 2 * B * n_heads * num_buckets * (b * b) * head_dim\n",
    "    # softmax cost (approx 5 ops per element)\n",
    "    per_layer['bucket_softmax'] = 5 * B * n_heads * num_buckets * (b * b)\n",
    "    # attention @ V: b^2 * head_dim * 2\n",
    "    per_layer['bucket_weighted_values'] = 2 * B * n_heads * num_buckets * (b * b) * head_dim\n",
    "\n",
    "    # residual + layernorm approx\n",
    "    per_layer['residual_add'] = B * L * d_model\n",
    "    per_layer['layer_norm'] = 6 * B * L * d_model  # mean,var,scale,shift approximated\n",
    "\n",
    "    # Reversible feed-forward (applied to halves)\n",
    "    d_half = d_model // 2\n",
    "    # F/G each: linear1 (d_half -> d_ff), activation, linear2 (d_ff -> d_half)\n",
    "    per_layer['FF_linear1'] = 2 * B * L * d_half * d_ff\n",
    "    per_layer['FF_activation'] = 2 * B * L * d_ff\n",
    "    per_layer['FF_linear2'] = 2 * B * L * d_ff * d_half\n",
    "\n",
    "    per_layer_total = sum(per_layer.values())\n",
    "    flops['per_encoder_layer'] = per_layer\n",
    "    flops['per_encoder_layer_total'] = per_layer_total\n",
    "    flops['total_encoder'] = per_layer_total * e_layers\n",
    "\n",
    "    # final encoder norm\n",
    "    flops['encoder_norm'] = 6 * B * L * d_model\n",
    "\n",
    "    # output projection (anomaly detection / imputation)\n",
    "    flops['output'] = 2 * B * L * d_model * O\n",
    "\n",
    "    total = (flops['input_embedding'] + flops['temporal_embedding'] +\n",
    "             flops['total_encoder'] + flops['encoder_norm'] + flops['output'])\n",
    "\n",
    "    return flops, total\n",
    "\n",
    "def count_flops_reformer(model, batch_size=1, sequence_length=1, configs={}):\n",
    "    \"\"\"\n",
    "    Compte les FLOPs (Floating Point Operations) pour le modèle Reformer.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: Instance du modèle Reformer\n",
    "    - batch_size: Taille du batch\n",
    "    - sequence_length: Longueur de la séquence\n",
    "    - configs: Configuration du modèle (dict)\n",
    "    \n",
    "    Returns:\n",
    "    - dict: Dictionnaire détaillé des FLOPs par composant\n",
    "    - int: Total des FLOPs\n",
    "    \"\"\"\n",
    "    \n",
    "    # Paramètres du modèle\n",
    "    B = batch_size\n",
    "    L = sequence_length  # Sequence length\n",
    "    \n",
    "    # Récupérer les paramètres depuis la configuration\n",
    "    d_model = configs.get('d_model')  # Dimension du modèle\n",
    "    n_heads = configs.get('n_heads')  # Nombre de têtes d'attention\n",
    "    d_ff = configs.get('d_ff')  # Dimension feedforward\n",
    "    e_layers = configs.get('e_layers')  # Nombre de couches encoder\n",
    "    bucket_size = configs.get('bucket_size', 4)  # Taille des buckets pour LSH\n",
    "    n_hashes = configs.get('n_hashes', 4)  # Nombre de hash functions\n",
    "    \n",
    "    # Input/Output dimensions\n",
    "    I = configs.get('enc_in')  # Input features\n",
    "    O = configs.get('c_out')  # Output features\n",
    "    \n",
    "    # Dimension par tête d'attention\n",
    "    head_dim = d_model // n_heads\n",
    "    \n",
    "    flops_breakdown = {}\n",
    "    \n",
    "    # ==================== INPUT EMBEDDING ====================\n",
    "    \n",
    "    # Input embedding (DataEmbedding)\n",
    "    # Value embedding: Linear [I -> d_model]\n",
    "    flops_breakdown['input_embedding'] = B * L * I * d_model\n",
    "    \n",
    "    # Positional embedding (sinusoidal)\n",
    "    # Addition seulement, pas de FLOPs\n",
    "    \n",
    "    # Temporal embedding (si applicable)\n",
    "    # Approximation: embedding temporel basé sur les features temporelles\n",
    "    flops_breakdown['temporal_embedding'] = B * L * 4 * d_model  # 4 features temporelles typiques\n",
    "    \n",
    "    # ==================== ENCODER LAYERS ====================\n",
    "    \n",
    "    encoder_layer_flops = {}\n",
    "    \n",
    "    # ==================== LOCALITY SENSITIVE HASHING (LSH) ATTENTION ====================\n",
    "    \n",
    "    lsh_attention_flops = {}\n",
    "    \n",
    "    # Query, Key, Value projections\n",
    "    lsh_attention_flops['Q_projection'] = B * L * d_model * d_model\n",
    "    lsh_attention_flops['K_projection'] = B * L * d_model * d_model\n",
    "    lsh_attention_flops['V_projection'] = B * L * d_model * d_model\n",
    "    \n",
    "    # Reshape pour multi-head: [B, L, d_model] -> [B, n_heads, L, head_dim]\n",
    "    # Pas de FLOPs, juste un reshape\n",
    "    \n",
    "    # ==================== LSH ATTENTION CORRIGÉE ====================\n",
    "\n",
    "    # Hashing reste O(L log L)\n",
    "    lsh_attention_flops['bucket_sorting'] = B * n_heads * n_hashes * L * math.log2(L)\n",
    "\n",
    "    # Attention dans les buckets - COMPLEXITÉ FIXE\n",
    "    # Chaque bucket a une taille fixe (bucket_size = 4)\n",
    "    # Nombre total d'opérations d'attention : O(L * bucket_size) au lieu de O(L²)\n",
    "    effective_bucket_size = bucket_size  # Taille fixe, pas dépendante de L\n",
    "    num_attention_ops = L * effective_bucket_size  # O(L) au lieu de O(L²)\n",
    "\n",
    "    lsh_attention_flops['bucket_attention_scores'] = B * n_heads * num_attention_ops * head_dim\n",
    "    lsh_attention_flops['bucket_attention_softmax'] = B * n_heads * num_attention_ops * 3\n",
    "    lsh_attention_flops['bucket_attention_values'] = B * n_heads * num_attention_ops * head_dim\n",
    "    \n",
    "    # # ==================== ATTENTION DANS LES BUCKETS ====================\n",
    "    \n",
    "    # # Nombre moyen d'éléments par bucket\n",
    "    # avg_bucket_size = max(1, min(bucket_size, L // (2 ** (n_hashes // 2))))  # Approximation heuristique\n",
    "    # num_buckets = max(1, L // avg_bucket_size)\n",
    "    \n",
    "    # # Attention computation dans chaque bucket\n",
    "    # # Au lieu de O(L²), on a O(bucket_size²) par bucket\n",
    "    # lsh_attention_flops['bucket_attention_scores'] = B * n_heads * num_buckets * avg_bucket_size * head_dim * avg_bucket_size\n",
    "    \n",
    "    # # Scale by sqrt(head_dim)\n",
    "    # lsh_attention_flops['scale_attention'] = B * n_heads * num_buckets * avg_bucket_size * avg_bucket_size\n",
    "    \n",
    "    # # Softmax dans chaque bucket\n",
    "    # lsh_attention_flops['bucket_attention_softmax'] = B * n_heads * num_buckets * avg_bucket_size * avg_bucket_size * 3\n",
    "    \n",
    "    # # Attention weights @ V dans chaque bucket\n",
    "    # lsh_attention_flops['bucket_attention_values'] = B * n_heads * num_buckets * avg_bucket_size * avg_bucket_size * head_dim\n",
    "    \n",
    "    # # Reassembly des résultats des buckets\n",
    "    # lsh_attention_flops['bucket_reassembly'] = B * n_heads * L * head_dim\n",
    "    \n",
    "    # # Output projection\n",
    "    # lsh_attention_flops['output_projection'] = B * L * d_model * d_model\n",
    "    \n",
    "    # # Dropout (pas de FLOPs)\n",
    "    \n",
    "    # # Residual connection + Layer Norm\n",
    "    # lsh_attention_flops['residual_add'] = B * L * d_model\n",
    "    # lsh_attention_flops['layer_norm1'] = B * L * d_model * 3  # mean, var, normalize\n",
    "    \n",
    "    # ==================== REVERSIBLE RESIDUAL LAYERS ====================\n",
    "    \n",
    "    # Reformer utilise des couches réversibles qui divisent d_model en 2\n",
    "    # F(x1) + x2 et G(x2) + x1 où F et G sont des sous-réseaux\n",
    "    \n",
    "    reversible_flops = {}\n",
    "    \n",
    "    # Chunking: diviser d_model en 2 parties (pas de FLOPs)\n",
    "    d_model_half = d_model // 2\n",
    "    \n",
    "    # F function (Feed Forward sur la première moitié)\n",
    "    reversible_flops['F_linear1'] = B * L * d_model_half * d_ff\n",
    "    reversible_flops['F_activation'] = B * L * d_ff * 2  # GELU\n",
    "    reversible_flops['F_linear2'] = B * L * d_ff * d_model_half\n",
    "    \n",
    "    # Addition F(x1) + x2\n",
    "    reversible_flops['F_residual_add'] = B * L * d_model_half\n",
    "    \n",
    "    # G function (identique à F mais sur la deuxième moitié)\n",
    "    reversible_flops['G_linear1'] = B * L * d_model_half * d_ff\n",
    "    reversible_flops['G_activation'] = B * L * d_ff * 2  # GELU\n",
    "    reversible_flops['G_linear2'] = B * L * d_ff * d_model_half\n",
    "    \n",
    "    # Addition G(x2) + x1\n",
    "    reversible_flops['G_residual_add'] = B * L * d_model_half\n",
    "    \n",
    "    # Concatenation finale (pas de FLOPs)\n",
    "    \n",
    "    # Layer Norm finale\n",
    "    reversible_flops['layer_norm2'] = B * L * d_model * 3\n",
    "    \n",
    "    # ==================== ASSEMBLY ENCODER LAYER FLOPS ====================\n",
    "    \n",
    "    encoder_layer_total = (sum(lsh_attention_flops.values()) + \n",
    "                          sum(reversible_flops.values()))\n",
    "    \n",
    "    encoder_layer_flops['lsh_attention'] = lsh_attention_flops\n",
    "    encoder_layer_flops['reversible_layers'] = reversible_flops\n",
    "    encoder_layer_flops['total_per_layer'] = encoder_layer_total\n",
    "    \n",
    "    # ==================== FINAL ENCODER NORMALIZATION ====================\n",
    "    \n",
    "    encoder_norm_flops = B * L * d_model * 3\n",
    "    \n",
    "    # ==================== OUTPUT PROJECTION ====================\n",
    "    \n",
    "    output_flops = 0\n",
    "    \n",
    "    if configs.get('task_name') == 'classification':\n",
    "        # GELU activation: [B, L, d_model]\n",
    "        gelu_flops = B * L * d_model * 2\n",
    "        # Dropout (pas de FLOPs)\n",
    "        # Reshape: [B, L * d_model]\n",
    "        # Projection: [B, L * d_model] @ [L * d_model, num_class] -> [B, num_class]\n",
    "        num_class = configs.get('num_class', O)\n",
    "        projection_flops = B * (L * d_model) * num_class\n",
    "        \n",
    "        output_flops = gelu_flops + projection_flops\n",
    "        flops_breakdown['output_activation'] = gelu_flops\n",
    "        flops_breakdown['output_projection'] = projection_flops\n",
    "        \n",
    "    elif configs.get('task_name') in ['imputation', 'anomaly_detection']:\n",
    "        # Direct projection: [B, L, d_model] @ [d_model, O] -> [B, L, O]\n",
    "        output_flops = B * L * d_model * O\n",
    "        flops_breakdown['output_projection'] = output_flops\n",
    "        \n",
    "    elif configs.get('task_name') in ['long_term_forecast', 'short_term_forecast']:\n",
    "        # Pour forecasting, on ajoute un placeholder et projette\n",
    "        pred_len = configs.get('pred_len', L)\n",
    "        total_len = L + pred_len\n",
    "        \n",
    "        # Embedding du placeholder\n",
    "        placeholder_embedding_flops = B * pred_len * I * d_model\n",
    "        \n",
    "        # Encoder avec longueur étendue\n",
    "        extended_encoder_flops = encoder_layer_total * e_layers * (total_len / L)  # Proportionnel à la longueur\n",
    "        \n",
    "        # Projection finale\n",
    "        forecast_projection_flops = B * total_len * d_model * O\n",
    "        \n",
    "        # Normalization pour short_term_forecast\n",
    "        normalization_flops = 0\n",
    "        if configs.get('task_name') == 'short_term_forecast':\n",
    "            # Mean et std computation\n",
    "            normalization_flops += B * L * I * 3  # mean + var + sqrt\n",
    "            # Normalization et denormalization\n",
    "            normalization_flops += B * total_len * I * 4  # subtract + divide + multiply + add\n",
    "        \n",
    "        flops_breakdown['placeholder_embedding'] = placeholder_embedding_flops\n",
    "        flops_breakdown['extended_encoder'] = extended_encoder_flops\n",
    "        flops_breakdown['forecast_projection'] = forecast_projection_flops\n",
    "        flops_breakdown['forecast_normalization'] = normalization_flops\n",
    "        \n",
    "        output_flops = (placeholder_embedding_flops + extended_encoder_flops + \n",
    "                       forecast_projection_flops + normalization_flops)\n",
    "    \n",
    "    # ==================== TOTAL CALCULATION ====================\n",
    "    \n",
    "    flops_breakdown['input_embedding'] = flops_breakdown['input_embedding']\n",
    "    flops_breakdown['temporal_embedding'] = flops_breakdown['temporal_embedding']\n",
    "    flops_breakdown['encoder_layers'] = encoder_layer_flops\n",
    "    flops_breakdown['total_encoder_layers'] = encoder_layer_total * e_layers\n",
    "    flops_breakdown['encoder_norm'] = encoder_norm_flops\n",
    "    \n",
    "    total_flops = (flops_breakdown['input_embedding'] + \n",
    "                  flops_breakdown['temporal_embedding'] +\n",
    "                  flops_breakdown['total_encoder_layers'] + \n",
    "                  flops_breakdown['encoder_norm'] +\n",
    "                  output_flops)\n",
    "    \n",
    "    flops_breakdown['total'] = total_flops\n",
    "    \n",
    "    return flops_breakdown, total_flops\n",
    "\n",
    "def print_flops_breakdown_reformer(flops_breakdown, total_flops):\n",
    "    \"\"\"\n",
    "    Affiche un résumé détaillé des FLOPs pour le modèle Reformer.\n",
    "    \n",
    "    Parameters:\n",
    "    - flops_breakdown: Dictionnaire des FLOPs par composant\n",
    "    - total_flops: Total des FLOPs\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(f\"REFORMER - FLOPs BREAKDOWN\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"Input Embedding: {flops_breakdown['input_embedding']:,} FLOPs\")\n",
    "    print(f\"Temporal Embedding: {flops_breakdown['temporal_embedding']:,} FLOPs\")\n",
    "    print(f\"Encoder Norm: {flops_breakdown['encoder_norm']:,} FLOPs\")\n",
    "    \n",
    "    if 'output_projection' in flops_breakdown:\n",
    "        print(f\"Output Projection: {flops_breakdown['output_projection']:,} FLOPs\")\n",
    "    if 'output_activation' in flops_breakdown:\n",
    "        print(f\"Output Activation: {flops_breakdown['output_activation']:,} FLOPs\")\n",
    "    \n",
    "    # Forecasting specific components\n",
    "    if 'placeholder_embedding' in flops_breakdown:\n",
    "        print(f\"Placeholder Embedding: {flops_breakdown['placeholder_embedding']:,} FLOPs\")\n",
    "        print(f\"Extended Encoder: {flops_breakdown['extended_encoder']:,} FLOPs\")\n",
    "        print(f\"Forecast Projection: {flops_breakdown['forecast_projection']:,} FLOPs\")\n",
    "        if flops_breakdown['forecast_normalization'] > 0:\n",
    "            print(f\"Forecast Normalization: {flops_breakdown['forecast_normalization']:,} FLOPs\")\n",
    "    \n",
    "    print(f\"\\n=== ENCODER ===\")\n",
    "    print(f\"Per Encoder Layer FLOPs: {flops_breakdown['encoder_layers']['total_per_layer']:,}\")\n",
    "    print(f\"Total Encoder Layers FLOPs: {flops_breakdown['total_encoder_layers']:,}\")\n",
    "    \n",
    "    print(f\"\\n--- Encoder Layer Breakdown ---\")\n",
    "    lsh_attention_total = sum(flops_breakdown['encoder_layers']['lsh_attention'].values())\n",
    "    reversible_total = sum(flops_breakdown['encoder_layers']['reversible_layers'].values())\n",
    "    encoder_layer_total = flops_breakdown['encoder_layers']['total_per_layer']\n",
    "    \n",
    "    print(f\"LSH Attention: {lsh_attention_total:,} FLOPs ({lsh_attention_total/encoder_layer_total*100:.1f}%)\")\n",
    "    print(f\"Reversible Layers: {reversible_total:,} FLOPs ({reversible_total/encoder_layer_total*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\n--- Detailed LSH Attention Breakdown ---\")\n",
    "    for component, flops in flops_breakdown['encoder_layers']['lsh_attention'].items():\n",
    "        print(f\"  {component}: {flops:,} FLOPs\")\n",
    "    \n",
    "    print(f\"\\n--- Detailed Reversible Layers Breakdown ---\")\n",
    "    for component, flops in flops_breakdown['encoder_layers']['reversible_layers'].items():\n",
    "        print(f\"  {component}: {flops:,} FLOPs\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(f\"TOTAL FLOPs: {total_flops:,}\")\n",
    "    print(f\"TOTAL GFLOPs: {total_flops / 1e9:.3f}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "\n",
    "# Test du modèle Reformer\n",
    "from models.Reformer import Model as Reformer\n",
    "import math\n",
    "\n",
    "# Configuration pour le modèle Reformer\n",
    "conf_reformer = {\n",
    "    'task_name': 'anomaly_detection',\n",
    "    'seq_len': 96,\n",
    "    'pred_len': 96,\n",
    "    'enc_in': 10,        # Features d'entrée\n",
    "    'c_out': 10,         # Features de sortie\n",
    "    'num_class': 10,     # Nombre de classes pour classification\n",
    "    'd_model': 280,       # Dimension du modèle (doit être pair pour les couches réversibles)\n",
    "    'n_heads': 8,        # Nombre de têtes d'attention\n",
    "    'd_ff': 512,         # Dimension feedforward\n",
    "    'e_layers': 2,       # Nombre de couches encoder\n",
    "    'bucket_size': 4,    # Taille des buckets pour LSH\n",
    "    'n_hashes': 2,       # Nombre de hash functions\n",
    "    'dropout': 0.1,\n",
    "    'activation': 'gelu',\n",
    "    'embed': 'timeF',\n",
    "    'freq': 'h'\n",
    "}\n",
    "configs_reformer = type('Config', (), conf_reformer)()\n",
    "\n",
    "# Créer le modèle Reformer\n",
    "model_reformer = Reformer(configs_reformer, \n",
    "                         bucket_size=conf_reformer['bucket_size'],\n",
    "                         n_hashes=conf_reformer['n_hashes'])\n",
    "print(\"Model parameters:\", sum(p.numel() for p in model_reformer.parameters()))\n",
    "\n",
    "# Test des FLOPs\n",
    "batch_size = 32\n",
    "sequence_length = 96\n",
    "\n",
    "flops_breakdown, total_flops = count_flops_reformer(\n",
    "    model_reformer, batch_size=batch_size, sequence_length=sequence_length, configs=conf_reformer\n",
    ")\n",
    "\n",
    "# Afficher les résultats\n",
    "print_flops_breakdown_reformer(flops_breakdown, total_flops)\n",
    "\n",
    "# Test avec différentes longueurs de séquence\n",
    "print(f\"\\n=== TEST AVEC DIFFÉRENTES LONGUEURS DE SÉQUENCE ===\")\n",
    "flops_reformer = []\n",
    "\n",
    "for seq_len in SEQ_LENGTHS:\n",
    "    # Ajuster la configuration pour chaque longueur de séquence\n",
    "    conf_test = conf_reformer.copy()\n",
    "    conf_test['seq_len'] = seq_len\n",
    "    \n",
    "    flops_breakdown, total_flops = count_flops_reformer(\n",
    "        model_reformer, batch_size=10, sequence_length=seq_len, configs=conf_test\n",
    "    )\n",
    "    print(f\"Sequence Length: {seq_len:4d}, Total FLOPs: {total_flops:12,}, GFLOPs: {total_flops/1e9:8.3f}\")\n",
    "    flops_reformer.append(total_flops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "38ebb134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 1104224\n",
      "============================================================\n",
      "iTRANSFORMER - FLOPs BREAKDOWN\n",
      "============================================================\n",
      "Normalization: 246,720 FLOPs\n",
      "Input Embedding (Inverted): 7,864,320 FLOPs\n",
      "Temporal Embedding: 327,680 FLOPs\n",
      "Encoder Norm: 245,760 FLOPs\n",
      "Output Projection: 7,864,320 FLOPs\n",
      "\n",
      "=== ENCODER ===\n",
      "Per Encoder Layer FLOPs: 170,496,000\n",
      "Total Encoder Layers FLOPs: 340,992,000\n",
      "\n",
      "--- Encoder Layer Breakdown ---\n",
      "Multi-Head Attention (on Variables): 85,954,560 FLOPs (50.4%)\n",
      "Feed Forward Network: 84,541,440 FLOPs (49.6%)\n",
      "\n",
      "--- Detailed Attention Breakdown ---\n",
      "  Q_projection: 20,971,520 FLOPs\n",
      "  K_projection: 20,971,520 FLOPs\n",
      "  V_projection: 20,971,520 FLOPs\n",
      "  attention_scores: 819,200 FLOPs\n",
      "  scale_attention: 25,600 FLOPs\n",
      "  attention_softmax: 76,800 FLOPs\n",
      "  attention_values: 819,200 FLOPs\n",
      "  output_projection: 20,971,520 FLOPs\n",
      "  residual_add: 81,920 FLOPs\n",
      "  layer_norm1: 245,760 FLOPs\n",
      "\n",
      "--- Detailed Feed Forward Breakdown ---\n",
      "  linear1: 41,943,040 FLOPs\n",
      "  activation: 327,680 FLOPs\n",
      "  linear2: 41,943,040 FLOPs\n",
      "  residual_add: 81,920 FLOPs\n",
      "  layer_norm2: 245,760 FLOPs\n",
      "\n",
      "--- iTransformer Specifics ---\n",
      "• Attention operates on VARIABLES (not time steps)\n",
      "• Input is inverted: [B, L, N] -> [B, N, L] -> [B, N, d_model]\n",
      "• Each variable attends to all other variables\n",
      "• Complexity: O(N²) instead of O(L²)\n",
      "============================================================\n",
      "TOTAL FLOPs: 357,540,800\n",
      "TOTAL GFLOPs: 0.358\n",
      "============================================================\n",
      "\n",
      "=== TEST AVEC DIFFÉRENTES LONGUEURS DE SÉQUENCE ===\n",
      "Sequence Length:    1, Total FLOPs:  106,791,500, GFLOPs:    0.107\n",
      "Sequence Length:    2, Total FLOPs:  106,843,500, GFLOPs:    0.107\n",
      "Sequence Length:    4, Total FLOPs:  106,947,500, GFLOPs:    0.107\n",
      "Sequence Length:    8, Total FLOPs:  107,155,500, GFLOPs:    0.107\n",
      "Sequence Length:   16, Total FLOPs:  107,571,500, GFLOPs:    0.108\n",
      "Sequence Length:   32, Total FLOPs:  108,403,500, GFLOPs:    0.108\n",
      "Sequence Length:   64, Total FLOPs:  110,067,500, GFLOPs:    0.110\n",
      "Sequence Length:  128, Total FLOPs:  113,395,500, GFLOPs:    0.113\n",
      "Sequence Length:  256, Total FLOPs:  120,051,500, GFLOPs:    0.120\n",
      "Sequence Length:  512, Total FLOPs:  133,363,500, GFLOPs:    0.133\n",
      "Sequence Length: 1024, Total FLOPs:  159,987,500, GFLOPs:    0.160\n",
      "Sequence Length: 2048, Total FLOPs:  213,235,500, GFLOPs:    0.213\n",
      "Sequence Length: 4096, Total FLOPs:  319,731,500, GFLOPs:    0.320\n",
      "Sequence Length: 8192, Total FLOPs:  532,723,500, GFLOPs:    0.533\n",
      "Sequence Length: 16384, Total FLOPs:  958,707,500, GFLOPs:    0.959\n",
      "Sequence Length: 32768, Total FLOPs: 1,810,675,500, GFLOPs:    1.811\n",
      "Sequence Length: 65536, Total FLOPs: 3,514,611,500, GFLOPs:    3.515\n",
      "Sequence Length: 131072, Total FLOPs: 6,922,483,500, GFLOPs:    6.922\n",
      "Sequence Length: 262144, Total FLOPs: 13,738,227,500, GFLOPs:   13.738\n",
      "Sequence Length: 524288, Total FLOPs: 27,369,715,500, GFLOPs:   27.370\n",
      "Sequence Length: 1048576, Total FLOPs: 54,632,691,500, GFLOPs:   54.633\n",
      "Sequence Length: 2097152, Total FLOPs: 109,158,643,500, GFLOPs:  109.159\n",
      "Sequence Length: 4194304, Total FLOPs: 218,210,547,500, GFLOPs:  218.211\n",
      "Sequence Length: 8388608, Total FLOPs: 436,314,355,500, GFLOPs:  436.314\n",
      "Sequence Length: 16777216, Total FLOPs: 872,521,971,500, GFLOPs:  872.522\n"
     ]
    }
   ],
   "source": [
    "def count_flops_itransformer(model, batch_size=1, sequence_length=1, configs={}):\n",
    "    \"\"\"\n",
    "    Compte les FLOPs (Floating Point Operations) pour le modèle iTransformer.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: Instance du modèle iTransformer\n",
    "    - batch_size: Taille du batch\n",
    "    - sequence_length: Longueur de la séquence\n",
    "    - configs: Configuration du modèle (dict)\n",
    "    \n",
    "    Returns:\n",
    "    - dict: Dictionnaire détaillé des FLOPs par composant\n",
    "    - int: Total des FLOPs\n",
    "    \"\"\"\n",
    "    \n",
    "    # Paramètres du modèle\n",
    "    B = batch_size\n",
    "    L = sequence_length  # Sequence length\n",
    "    \n",
    "    # Récupérer les paramètres depuis la configuration\n",
    "    d_model = configs.get('d_model')  # Dimension du modèle\n",
    "    n_heads = configs.get('n_heads')  # Nombre de têtes d'attention\n",
    "    d_ff = configs.get('d_ff')  # Dimension feedforward\n",
    "    e_layers = configs.get('e_layers')  # Nombre de couches encoder\n",
    "    \n",
    "    # Input/Output dimensions\n",
    "    N = configs.get('enc_in')  # Number of variables (features)\n",
    "    \n",
    "    # Output dimension selon la tâche\n",
    "    if configs.get('task_name') == 'classification':\n",
    "        O = configs.get('num_class')\n",
    "    elif configs.get('task_name') in ['long_term_forecast', 'short_term_forecast']:\n",
    "        O = configs.get('pred_len')\n",
    "    else:  # imputation, anomaly_detection\n",
    "        O = L  # seq_len\n",
    "    \n",
    "    # Dimension par tête d'attention\n",
    "    head_dim = d_model // n_heads\n",
    "    \n",
    "    flops_breakdown = {}\n",
    "    \n",
    "    # ==================== NORMALIZATION (pour forecasting/imputation/anomaly) ====================\n",
    "    \n",
    "    normalization_flops = 0\n",
    "    if configs.get('task_name') in ['long_term_forecast', 'short_term_forecast', 'imputation', 'anomaly_detection']:\n",
    "        # Mean computation: [B, L, N] -> [B, 1, N]\n",
    "        normalization_flops += B * L * N  # sum\n",
    "        normalization_flops += B * N      # divide par L\n",
    "        \n",
    "        # Variance computation: [B, L, N]\n",
    "        normalization_flops += B * L * N  # subtract mean\n",
    "        normalization_flops += B * L * N  # square\n",
    "        normalization_flops += B * L * N  # sum\n",
    "        normalization_flops += B * N      # divide par L\n",
    "        normalization_flops += B * N      # sqrt\n",
    "        \n",
    "        # Normalization: [B, L, N]\n",
    "        normalization_flops += B * L * N  # subtract mean\n",
    "        normalization_flops += B * L * N  # divide by std\n",
    "        \n",
    "        # Denormalization (en sortie)\n",
    "        if configs.get('task_name') in ['long_term_forecast', 'short_term_forecast']:\n",
    "            pred_len = configs.get('pred_len', L)\n",
    "            normalization_flops += B * pred_len * N * 2  # multiply + add\n",
    "        else:\n",
    "            normalization_flops += B * L * N * 2  # multiply + add\n",
    "    \n",
    "    # ==================== INVERTED EMBEDDING ====================\n",
    "    \n",
    "    # iTransformer utilise DataEmbedding_inverted\n",
    "    # Input: [B, L, N] -> Transposed: [B, N, L]\n",
    "    # Chaque variable devient une séquence de longueur L\n",
    "    \n",
    "    # Value embedding: Linear projection [L -> d_model] pour chaque variable\n",
    "    flops_breakdown['input_embedding'] = B * N * L * d_model\n",
    "    \n",
    "    # Temporal embedding (si applicable)\n",
    "    # Pour chaque variable, on applique l'embedding temporel\n",
    "    flops_breakdown['temporal_embedding'] = B * N * 4 * d_model  # 4 features temporelles typiques\n",
    "    \n",
    "    # ==================== ENCODER LAYERS ====================\n",
    "    \n",
    "    encoder_layer_flops = {}\n",
    "    \n",
    "    # ==================== MULTI-HEAD ATTENTION ====================\n",
    "    \n",
    "    attention_flops = {}\n",
    "    \n",
    "    # Dans iTransformer, l'attention s'applique sur les variables (dimension N)\n",
    "    # Input shape après embedding: [B, N, d_model]\n",
    "    \n",
    "    # Query, Key, Value projections\n",
    "    # Input: [B, N, d_model] -> Output: [B, N, d_model] (pour chacune des 3 projections)\n",
    "    attention_flops['Q_projection'] = B * N * d_model * d_model\n",
    "    attention_flops['K_projection'] = B * N * d_model * d_model\n",
    "    attention_flops['V_projection'] = B * N * d_model * d_model\n",
    "    \n",
    "    # Reshape pour multi-head: [B, N, d_model] -> [B, n_heads, N, head_dim]\n",
    "    # Pas de FLOPs, juste un reshape\n",
    "    \n",
    "    # Attention scores: Q @ K^T\n",
    "    # Q: [B, n_heads, N, head_dim], K^T: [B, n_heads, head_dim, N] -> [B, n_heads, N, N]\n",
    "    attention_flops['attention_scores'] = B * n_heads * N * head_dim * N\n",
    "    \n",
    "    # Scale by sqrt(head_dim)\n",
    "    attention_flops['scale_attention'] = B * n_heads * N * N\n",
    "    \n",
    "    # Softmax sur la dimension des clés (N)\n",
    "    # Approximation: exp + sum + divide = 3 opérations par élément\n",
    "    attention_flops['attention_softmax'] = B * n_heads * N * N * 3\n",
    "    \n",
    "    # Dropout (pas de FLOPs)\n",
    "    \n",
    "    # Attention weights @ V: [B, n_heads, N, N] @ [B, n_heads, N, head_dim] -> [B, n_heads, N, head_dim]\n",
    "    attention_flops['attention_values'] = B * n_heads * N * N * head_dim\n",
    "    \n",
    "    # Concatenate heads: [B, n_heads, N, head_dim] -> [B, N, d_model]\n",
    "    # Pas de FLOPs, juste un reshape\n",
    "    \n",
    "    # Output projection: [B, N, d_model] @ [d_model, d_model] -> [B, N, d_model]\n",
    "    attention_flops['output_projection'] = B * N * d_model * d_model\n",
    "    \n",
    "    # Dropout (pas de FLOPs)\n",
    "    \n",
    "    # Residual connection + LayerNorm1\n",
    "    attention_flops['residual_add'] = B * N * d_model\n",
    "    attention_flops['layer_norm1'] = B * N * d_model * 3  # mean, var, normalize\n",
    "    \n",
    "    # ==================== FEED FORWARD NETWORK ====================\n",
    "    \n",
    "    feedforward_flops = {}\n",
    "    \n",
    "    # Linear 1: [B, N, d_model] -> [B, N, d_ff]\n",
    "    feedforward_flops['linear1'] = B * N * d_model * d_ff\n",
    "    \n",
    "    # Activation (GELU)\n",
    "    # Approximation: 2 opérations par élément pour GELU\n",
    "    feedforward_flops['activation'] = B * N * d_ff * 2\n",
    "    \n",
    "    # Dropout (pas de FLOPs)\n",
    "    \n",
    "    # Linear 2: [B, N, d_ff] -> [B, N, d_model]\n",
    "    feedforward_flops['linear2'] = B * N * d_ff * d_model\n",
    "    \n",
    "    # Dropout (pas de FLOPs)\n",
    "    \n",
    "    # Residual connection + LayerNorm2\n",
    "    feedforward_flops['residual_add'] = B * N * d_model\n",
    "    feedforward_flops['layer_norm2'] = B * N * d_model * 3\n",
    "    \n",
    "    # ==================== ASSEMBLY ENCODER LAYER FLOPS ====================\n",
    "    \n",
    "    encoder_layer_total = sum(attention_flops.values()) + sum(feedforward_flops.values())\n",
    "    \n",
    "    encoder_layer_flops['attention'] = attention_flops\n",
    "    encoder_layer_flops['feedforward'] = feedforward_flops\n",
    "    encoder_layer_flops['total_per_layer'] = encoder_layer_total\n",
    "    \n",
    "    # ==================== FINAL ENCODER NORMALIZATION ====================\n",
    "    \n",
    "    encoder_norm_flops = B * N * d_model * 3\n",
    "    \n",
    "    # ==================== OUTPUT PROJECTION ====================\n",
    "    \n",
    "    output_flops = 0\n",
    "    \n",
    "    if configs.get('task_name') == 'classification':\n",
    "        # GELU activation: [B, N, d_model]\n",
    "        gelu_flops = B * N * d_model * 2\n",
    "        # Dropout (pas de FLOPs)\n",
    "        # Reshape: [B, N * d_model]\n",
    "        # Projection: [B, N * d_model] @ [N * d_model, num_class] -> [B, num_class]\n",
    "        projection_flops = B * (N * d_model) * O\n",
    "        \n",
    "        output_flops = gelu_flops + projection_flops\n",
    "        flops_breakdown['output_activation'] = gelu_flops\n",
    "        flops_breakdown['output_projection'] = projection_flops\n",
    "        \n",
    "    elif configs.get('task_name') in ['long_term_forecast', 'short_term_forecast']:\n",
    "        # Projection: [B, N, d_model] @ [d_model, pred_len] -> [B, N, pred_len]\n",
    "        # Puis permute: [B, N, pred_len] -> [B, pred_len, N]\n",
    "        output_flops = B * N * d_model * O\n",
    "        flops_breakdown['output_projection'] = output_flops\n",
    "        \n",
    "    elif configs.get('task_name') in ['imputation', 'anomaly_detection']:\n",
    "        # Projection: [B, N, d_model] @ [d_model, seq_len] -> [B, N, seq_len]\n",
    "        # Puis permute: [B, N, seq_len] -> [B, seq_len, N]\n",
    "        output_flops = B * N * d_model * O\n",
    "        flops_breakdown['output_projection'] = output_flops\n",
    "    \n",
    "    # ==================== TOTAL CALCULATION ====================\n",
    "    \n",
    "    flops_breakdown['normalization'] = normalization_flops\n",
    "    flops_breakdown['input_embedding'] = flops_breakdown['input_embedding']\n",
    "    flops_breakdown['temporal_embedding'] = flops_breakdown['temporal_embedding']\n",
    "    flops_breakdown['encoder_layers'] = encoder_layer_flops\n",
    "    flops_breakdown['total_encoder_layers'] = encoder_layer_total * e_layers\n",
    "    flops_breakdown['encoder_norm'] = encoder_norm_flops\n",
    "    \n",
    "    total_flops = (flops_breakdown['normalization'] +\n",
    "                  flops_breakdown['input_embedding'] + \n",
    "                  flops_breakdown['temporal_embedding'] +\n",
    "                  flops_breakdown['total_encoder_layers'] + \n",
    "                  flops_breakdown['encoder_norm'] +\n",
    "                  output_flops)\n",
    "    \n",
    "    flops_breakdown['total'] = total_flops\n",
    "    \n",
    "    return flops_breakdown, total_flops\n",
    "\n",
    "def print_flops_breakdown_itransformer(flops_breakdown, total_flops):\n",
    "    \"\"\"\n",
    "    Affiche un résumé détaillé des FLOPs pour le modèle iTransformer.\n",
    "    \n",
    "    Parameters:\n",
    "    - flops_breakdown: Dictionnaire des FLOPs par composant\n",
    "    - total_flops: Total des FLOPs\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(f\"iTRANSFORMER - FLOPs BREAKDOWN\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if flops_breakdown['normalization'] > 0:\n",
    "        print(f\"Normalization: {flops_breakdown['normalization']:,} FLOPs\")\n",
    "    print(f\"Input Embedding (Inverted): {flops_breakdown['input_embedding']:,} FLOPs\")\n",
    "    print(f\"Temporal Embedding: {flops_breakdown['temporal_embedding']:,} FLOPs\")\n",
    "    print(f\"Encoder Norm: {flops_breakdown['encoder_norm']:,} FLOPs\")\n",
    "    \n",
    "    if 'output_projection' in flops_breakdown:\n",
    "        print(f\"Output Projection: {flops_breakdown['output_projection']:,} FLOPs\")\n",
    "    if 'output_activation' in flops_breakdown:\n",
    "        print(f\"Output Activation: {flops_breakdown['output_activation']:,} FLOPs\")\n",
    "    \n",
    "    print(f\"\\n=== ENCODER ===\")\n",
    "    print(f\"Per Encoder Layer FLOPs: {flops_breakdown['encoder_layers']['total_per_layer']:,}\")\n",
    "    print(f\"Total Encoder Layers FLOPs: {flops_breakdown['total_encoder_layers']:,}\")\n",
    "    \n",
    "    print(f\"\\n--- Encoder Layer Breakdown ---\")\n",
    "    encoder_attention_total = sum(flops_breakdown['encoder_layers']['attention'].values())\n",
    "    encoder_ff_total = sum(flops_breakdown['encoder_layers']['feedforward'].values())\n",
    "    encoder_layer_total = flops_breakdown['encoder_layers']['total_per_layer']\n",
    "    \n",
    "    print(f\"Multi-Head Attention (on Variables): {encoder_attention_total:,} FLOPs ({encoder_attention_total/encoder_layer_total*100:.1f}%)\")\n",
    "    print(f\"Feed Forward Network: {encoder_ff_total:,} FLOPs ({encoder_ff_total/encoder_layer_total*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\n--- Detailed Attention Breakdown ---\")\n",
    "    for component, flops in flops_breakdown['encoder_layers']['attention'].items():\n",
    "        print(f\"  {component}: {flops:,} FLOPs\")\n",
    "    \n",
    "    print(f\"\\n--- Detailed Feed Forward Breakdown ---\")\n",
    "    for component, flops in flops_breakdown['encoder_layers']['feedforward'].items():\n",
    "        print(f\"  {component}: {flops:,} FLOPs\")\n",
    "    \n",
    "    print(\"\\n--- iTransformer Specifics ---\")\n",
    "    print(\"• Attention operates on VARIABLES (not time steps)\")\n",
    "    print(\"• Input is inverted: [B, L, N] -> [B, N, L] -> [B, N, d_model]\")\n",
    "    print(\"• Each variable attends to all other variables\")\n",
    "    print(\"• Complexity: O(N²) instead of O(L²)\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(f\"TOTAL FLOPs: {total_flops:,}\")\n",
    "    print(f\"TOTAL GFLOPs: {total_flops / 1e9:.3f}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "# Test du modèle iTransformer\n",
    "from models.iTransformer import Model as iTransformer\n",
    "\n",
    "# Configuration pour le modèle iTransformer\n",
    "conf_itransformer = {\n",
    "    'task_name': 'anomaly_detection',\n",
    "    'seq_len': 96,\n",
    "    'pred_len': 96,\n",
    "    'enc_in': 10,        # Features d'entrée (nombre de variables)\n",
    "    'c_out': 10,         # Features de sortie\n",
    "    'num_class': 10,     # Nombre de classes pour classification\n",
    "    'd_model': 256,       # Dimension du modèle\n",
    "    'n_heads': 8,        # Nombre de têtes d'attention\n",
    "    'd_ff': 512,         # Dimension feedforward\n",
    "    'e_layers': 2,       # Nombre de couches encoder\n",
    "    'factor': 3,         # Facteur pour attention\n",
    "    'dropout': 0.1,\n",
    "    'activation': 'gelu',\n",
    "    'embed': 'timeF',\n",
    "    'freq': 'h'\n",
    "}\n",
    "configs_itransformer = type('Config', (), conf_itransformer)()\n",
    "\n",
    "# Créer le modèle iTransformer\n",
    "model_itransformer = iTransformer(configs_itransformer)\n",
    "print(\"Model parameters:\", sum(p.numel() for p in model_itransformer.parameters()))\n",
    "\n",
    "# Test des FLOPs\n",
    "batch_size = 32\n",
    "sequence_length = 96\n",
    "\n",
    "flops_breakdown, total_flops = count_flops_itransformer(\n",
    "    model_itransformer, batch_size=batch_size, sequence_length=sequence_length, configs=conf_itransformer\n",
    ")\n",
    "\n",
    "# Afficher les résultats\n",
    "print_flops_breakdown_itransformer(flops_breakdown, total_flops)\n",
    "\n",
    "# Test avec différentes longueurs de séquence\n",
    "print(f\"\\n=== TEST AVEC DIFFÉRENTES LONGUEURS DE SÉQUENCE ===\")\n",
    "flops_itransformer = []\n",
    "\n",
    "for seq_len in SEQ_LENGTHS:\n",
    "    # Ajuster la configuration pour chaque longueur de séquence\n",
    "    conf_test = conf_itransformer.copy()\n",
    "    conf_test['seq_len'] = seq_len\n",
    "    \n",
    "    flops_breakdown, total_flops = count_flops_itransformer(\n",
    "        model_itransformer, batch_size=10, sequence_length=seq_len, configs=conf_test\n",
    "    )\n",
    "    print(f\"Sequence Length: {seq_len:4d}, Total FLOPs: {total_flops:12,}, GFLOPs: {total_flops/1e9:8.3f}\")\n",
    "    flops_itransformer.append(total_flops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89fceab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 1091168\n",
      "=== MODÈLE PATCHTST CRÉÉ ===\n",
      "Task: anomaly_detection\n",
      "Sequence length: 96\n",
      "Prediction length: 96\n",
      "Encoder layers: 2\n",
      "Features (variables): 10\n",
      "Patch length: 16\n",
      "Stride: 8\n",
      "Number of patches: 12\n",
      "============================================================\n",
      "PATCHTST - FLOPs BREAKDOWN\n",
      "============================================================\n",
      "Patch Configuration:\n",
      "  Variables: 10\n",
      "  Patch Length: 16\n",
      "  Stride: 8\n",
      "  Number of Patches: 12\n",
      "  Head Features: 3072\n",
      "\n",
      "FLOPs Breakdown:\n",
      "Normalization: 246,720 FLOPs\n",
      "Patch Embedding: 15,790,080 FLOPs\n",
      "Encoder Norm: 2,949,120 FLOPs\n",
      "Reshape/Permute: 0 FLOPs\n",
      "Head/Output: 94,371,840 FLOPs\n",
      "\n",
      "=== ENCODER ===\n",
      "Per Encoder Layer FLOPs: 1,544,847,360\n",
      "Total Encoder Layers FLOPs: 3,089,694,720\n",
      "\n",
      "--- Encoder Layer Breakdown ---\n",
      "Multi-Head Attention (on Patches): 1,035,632,640 FLOPs (67.0%)\n",
      "Feed Forward Network: 509,214,720 FLOPs (33.0%)\n",
      "\n",
      "--- Detailed Patch Embedding Breakdown ---\n",
      "  unfold_patches: 61,440 FLOPs\n",
      "  value_embedding: 15,728,640 FLOPs\n",
      "\n",
      "--- Detailed Attention Breakdown ---\n",
      "  Q_projection: 251,658,240 FLOPs\n",
      "  K_projection: 251,658,240 FLOPs\n",
      "  V_projection: 251,658,240 FLOPs\n",
      "  attention_scores: 11,796,480 FLOPs\n",
      "  scale_attention: 368,640 FLOPs\n",
      "  attention_softmax: 1,105,920 FLOPs\n",
      "  attention_values: 11,796,480 FLOPs\n",
      "  output_projection: 251,658,240 FLOPs\n",
      "  residual_add: 983,040 FLOPs\n",
      "  layer_norm1: 2,949,120 FLOPs\n",
      "\n",
      "--- Detailed Head Breakdown ---\n",
      "  flatten: 0 FLOPs\n",
      "  linear_projection: 94,371,840 FLOPs\n",
      "\n",
      "--- PatchTST Specifics ---\n",
      "• Input is segmented into patches\n",
      "• Each variable processed independently\n",
      "• Attention operates on PATCHES (not time steps)\n",
      "• Complexity: O(P²) where P = 12 patches\n",
      "• Channel Independence: each variable has separate attention\n",
      "============================================================\n",
      "TOTAL FLOPs: 3,203,052,480\n",
      "TOTAL GFLOPs: 3.203\n",
      "============================================================\n",
      "\n",
      "=== TEST AVEC DIFFÉRENTES LONGUEURS DE SÉQUENCE ===\n",
      "Sequence Length:    1, Total FLOPs:   79,778,700, GFLOPs:    0.080\n",
      "Sequence Length:    2, Total FLOPs:   79,805,100, GFLOPs:    0.080\n",
      "Sequence Length:    4, Total FLOPs:   79,857,900, GFLOPs:    0.080\n",
      "Sequence Length:    8, Total FLOPs:   79,963,500, GFLOPs:    0.080\n",
      "Sequence Length:   16, Total FLOPs:  160,553,900, GFLOPs:    0.161\n",
      "Sequence Length:   32, Total FLOPs:  323,616,300, GFLOPs:    0.324\n",
      "Sequence Length:   64, Total FLOPs:  657,267,500, GFLOPs:    0.657\n",
      "Sequence Length:  128, Total FLOPs: 1,354,675,500, GFLOPs:    1.355\n",
      "Sequence Length:  256, Total FLOPs: 2,869,913,900, GFLOPs:    2.870\n",
      "Sequence Length:  512, Total FLOPs: 6,382,080,300, GFLOPs:    6.382\n",
      "Sequence Length: 1024, Total FLOPs: 15,333,171,500, GFLOPs:   15.333\n",
      "Sequence Length: 2048, Total FLOPs: 40,942,387,500, GFLOPs:   40.942\n",
      "Sequence Length: 4096, Total FLOPs: 122,988,953,900, GFLOPs:  122.989\n",
      "Sequence Length: 8192, Total FLOPs: 410,394,624,300, GFLOPs:  410.395\n",
      "Sequence Length: 16384, Total FLOPs: 1,478,456,115,500, GFLOPs: 1478.456\n",
      "Sequence Length: 32768, Total FLOPs: 5,587,579,699,500, GFLOPs: 5587.580\n",
      "Sequence Length: 65536, Total FLOPs: 21,697,829,273,900, GFLOPs: 21697.829\n",
      "Sequence Length: 131072, Total FLOPs: 85,486,338,048,300, GFLOPs: 85486.338\n",
      "Sequence Length: 262144, Total FLOPs: 339,335,394,099,500, GFLOPs: 339335.394\n",
      "Sequence Length: 524288, Total FLOPs: 1,352,121,660,211,500, GFLOPs: 1352121.660\n",
      "Sequence Length: 1048576, Total FLOPs: 5,398,046,808,473,900, GFLOPs: 5398046.808\n",
      "Sequence Length: 2097152, Total FLOPs: 21,571,307,569,152,300, GFLOPs: 21571307.569\n",
      "Sequence Length: 4194304, Total FLOPs: 86,243,470,947,123,500, GFLOPs: 86243470.947\n",
      "Sequence Length: 8388608, Total FLOPs: 344,890,365,129,523,500, GFLOPs: 344890365.130\n",
      "Sequence Length: 16777216, Total FLOPs: 1,379,394,423,200,153,900, GFLOPs: 1379394423.200\n"
     ]
    }
   ],
   "source": [
    "def count_flops_patchtst(model, batch_size=1, sequence_length=1, configs={}):\n",
    "    \"\"\"\n",
    "    Compte les FLOPs (Floating Point Operations) pour le modèle PatchTST.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: Instance du modèle PatchTST\n",
    "    - batch_size: Taille du batch\n",
    "    - sequence_length: Longueur de la séquence\n",
    "    - configs: Configuration du modèle (dict)\n",
    "    \n",
    "    Returns:\n",
    "    - dict: Dictionnaire détaillé des FLOPs par composant\n",
    "    - int: Total des FLOPs\n",
    "    \"\"\"\n",
    "    \n",
    "    # Paramètres du modèle\n",
    "    B = batch_size\n",
    "    L = sequence_length  # Sequence length\n",
    "    \n",
    "    # Récupérer les paramètres depuis la configuration\n",
    "    d_model = configs.get('d_model')  # Dimension du modèle\n",
    "    n_heads = configs.get('n_heads')  # Nombre de têtes d'attention\n",
    "    d_ff = configs.get('d_ff')  # Dimension feedforward\n",
    "    e_layers = configs.get('e_layers')  # Nombre de couches encoder\n",
    "    patch_len = configs.get('patch_len', 16)  # Longueur des patches\n",
    "    stride = configs.get('stride', 8)  # Stride pour les patches\n",
    "    \n",
    "    # Input/Output dimensions\n",
    "    N = configs.get('enc_in')  # Number of variables (features)\n",
    "    \n",
    "    # Calcul du nombre de patches\n",
    "    padding = stride\n",
    "    patch_num = int((L - patch_len) / stride + 1)\n",
    "    if padding:\n",
    "        patch_num += 1\n",
    "    \n",
    "    # Output dimension selon la tâche\n",
    "    if configs.get('task_name') == 'classification':\n",
    "        O = configs.get('num_class')\n",
    "    elif configs.get('task_name') in ['long_term_forecast', 'short_term_forecast']:\n",
    "        O = configs.get('pred_len')\n",
    "    else:  # imputation, anomaly_detection\n",
    "        O = L  # seq_len\n",
    "    \n",
    "    # Dimension par tête d'attention\n",
    "    head_dim = d_model // n_heads\n",
    "    \n",
    "    flops_breakdown = {}\n",
    "    \n",
    "    # ==================== NORMALIZATION ====================\n",
    "    \n",
    "    normalization_flops = 0\n",
    "    if configs.get('task_name') in ['long_term_forecast', 'short_term_forecast', 'anomaly_detection', 'classification']:\n",
    "        # Mean computation: [B, L, N] -> [B, 1, N]\n",
    "        normalization_flops += B * L * N  # sum\n",
    "        normalization_flops += B * N      # divide par L\n",
    "        \n",
    "        # Variance computation: [B, L, N]\n",
    "        normalization_flops += B * L * N  # subtract mean\n",
    "        normalization_flops += B * L * N  # square\n",
    "        normalization_flops += B * L * N  # sum\n",
    "        normalization_flops += B * N      # divide par L\n",
    "        normalization_flops += B * N      # sqrt\n",
    "        \n",
    "        # Normalization: [B, L, N]\n",
    "        normalization_flops += B * L * N  # subtract mean\n",
    "        normalization_flops += B * L * N  # divide by std\n",
    "        \n",
    "        # Denormalization (en sortie)\n",
    "        if configs.get('task_name') in ['long_term_forecast', 'short_term_forecast']:\n",
    "            pred_len = configs.get('pred_len', L)\n",
    "            normalization_flops += B * pred_len * N * 2  # multiply + add\n",
    "        else:\n",
    "            normalization_flops += B * L * N * 2  # multiply + add\n",
    "    \n",
    "    elif configs.get('task_name') == 'imputation':\n",
    "        # Imputation has special normalization with mask\n",
    "        # Approximation similaire mais avec masking\n",
    "        normalization_flops += B * L * N * 10  # Approximation pour toutes les opérations de masking\n",
    "    \n",
    "    # ==================== PATCH EMBEDDING ====================\n",
    "    \n",
    "    patch_embedding_flops = {}\n",
    "    \n",
    "    # Input shape après permutation: [B, N, L] -> [B*N, L]\n",
    "    # Unfold operation pour créer les patches\n",
    "    # Unfold: [B*N, L] -> [B*N, patch_num, patch_len]\n",
    "    # Approximation: coût de l'extraction des patches\n",
    "    patch_embedding_flops['unfold_patches'] = B * N * patch_num * patch_len\n",
    "    \n",
    "    # Linear projection: [B*N, patch_num, patch_len] @ [patch_len, d_model] -> [B*N, patch_num, d_model]\n",
    "    patch_embedding_flops['value_embedding'] = B * N * patch_num * patch_len * d_model\n",
    "    \n",
    "    # Positional embedding (learnable)\n",
    "    # Addition [B*N, patch_num, d_model] + [patch_num, d_model]\n",
    "    # Pas de FLOPs pour l'addition, juste accès mémoire\n",
    "    \n",
    "    # Dropout (pas de FLOPs)\n",
    "    \n",
    "    # ==================== ENCODER LAYERS ====================\n",
    "    \n",
    "    encoder_layer_flops = {}\n",
    "    \n",
    "    # ==================== MULTI-HEAD ATTENTION ====================\n",
    "    \n",
    "    attention_flops = {}\n",
    "    \n",
    "    # Input shape: [B*N, patch_num, d_model]\n",
    "    # Query, Key, Value projections\n",
    "    attention_flops['Q_projection'] = B * N * patch_num * d_model * d_model\n",
    "    attention_flops['K_projection'] = B * N * patch_num * d_model * d_model\n",
    "    attention_flops['V_projection'] = B * N * patch_num * d_model * d_model\n",
    "    \n",
    "    # Reshape pour multi-head: [B*N, patch_num, d_model] -> [B*N, n_heads, patch_num, head_dim]\n",
    "    # Pas de FLOPs, juste un reshape\n",
    "    \n",
    "    # Attention scores: Q @ K^T\n",
    "    # Q: [B*N, n_heads, patch_num, head_dim], K^T: [B*N, n_heads, head_dim, patch_num] -> [B*N, n_heads, patch_num, patch_num]\n",
    "    attention_flops['attention_scores'] = B * N * n_heads * patch_num * head_dim * patch_num\n",
    "    \n",
    "    # Scale by sqrt(head_dim)\n",
    "    attention_flops['scale_attention'] = B * N * n_heads * patch_num * patch_num\n",
    "    \n",
    "    # Softmax sur la dimension des clés (patch_num)\n",
    "    # Approximation: exp + sum + divide = 3 opérations par élément\n",
    "    attention_flops['attention_softmax'] = B * N * n_heads * patch_num * patch_num * 3\n",
    "    \n",
    "    # Dropout (pas de FLOPs)\n",
    "    \n",
    "    # Attention weights @ V: [B*N, n_heads, patch_num, patch_num] @ [B*N, n_heads, patch_num, head_dim] -> [B*N, n_heads, patch_num, head_dim]\n",
    "    attention_flops['attention_values'] = B * N * n_heads * patch_num * patch_num * head_dim\n",
    "    \n",
    "    # Concatenate heads: [B*N, n_heads, patch_num, head_dim] -> [B*N, patch_num, d_model]\n",
    "    # Pas de FLOPs, juste un reshape\n",
    "    \n",
    "    # Output projection: [B*N, patch_num, d_model] @ [d_model, d_model] -> [B*N, patch_num, d_model]\n",
    "    attention_flops['output_projection'] = B * N * patch_num * d_model * d_model\n",
    "    \n",
    "    # Dropout (pas de FLOPs)\n",
    "    \n",
    "    # Residual connection + LayerNorm1\n",
    "    attention_flops['residual_add'] = B * N * patch_num * d_model\n",
    "    attention_flops['layer_norm1'] = B * N * patch_num * d_model * 3  # mean, var, normalize\n",
    "    \n",
    "    # ==================== FEED FORWARD NETWORK ====================\n",
    "    \n",
    "    feedforward_flops = {}\n",
    "    \n",
    "    # Linear 1: [B*N, patch_num, d_model] -> [B*N, patch_num, d_ff]\n",
    "    feedforward_flops['linear1'] = B * N * patch_num * d_model * d_ff\n",
    "    \n",
    "    # Activation (GELU)\n",
    "    # Approximation: 2 opérations par élément pour GELU\n",
    "    feedforward_flops['activation'] = B * N * patch_num * d_ff * 2\n",
    "    \n",
    "    # Dropout (pas de FLOPs)\n",
    "    \n",
    "    # Linear 2: [B*N, patch_num, d_ff] -> [B*N, patch_num, d_model]\n",
    "    feedforward_flops['linear2'] = B * N * patch_num * d_ff * d_model\n",
    "    \n",
    "    # Dropout (pas de FLOPs)\n",
    "    \n",
    "    # Residual connection + LayerNorm2\n",
    "    feedforward_flops['residual_add'] = B * N * patch_num * d_model\n",
    "    feedforward_flops['layer_norm2'] = B * N * patch_num * d_model * 3\n",
    "    \n",
    "    # ==================== ASSEMBLY ENCODER LAYER FLOPS ====================\n",
    "    \n",
    "    encoder_layer_total = sum(attention_flops.values()) + sum(feedforward_flops.values())\n",
    "    \n",
    "    encoder_layer_flops['attention'] = attention_flops\n",
    "    encoder_layer_flops['feedforward'] = feedforward_flops\n",
    "    encoder_layer_flops['total_per_layer'] = encoder_layer_total\n",
    "    \n",
    "    # ==================== FINAL ENCODER NORMALIZATION ====================\n",
    "    \n",
    "    # BatchNorm1d avec transpose\n",
    "    # Transpose: [B*N, patch_num, d_model] -> [B*N, d_model, patch_num] (pas de FLOPs)\n",
    "    # BatchNorm1d: [B*N, d_model, patch_num] (3 ops par élément: mean, var, normalize)\n",
    "    encoder_norm_flops = B * N * d_model * patch_num * 3\n",
    "    # Transpose retour: [B*N, d_model, patch_num] -> [B*N, patch_num, d_model] (pas de FLOPs)\n",
    "    \n",
    "    # ==================== RESHAPE ET PERMUTE ====================\n",
    "    \n",
    "    reshape_flops = {}\n",
    "    \n",
    "    # Reshape: [B*N, patch_num, d_model] -> [B, N, patch_num, d_model] (pas de FLOPs)\n",
    "    # Permute: [B, N, patch_num, d_model] -> [B, N, d_model, patch_num] (pas de FLOPs)\n",
    "    reshape_flops['reshapes_permutes'] = 0  # Pas de FLOPs computationnels\n",
    "    \n",
    "    # ==================== HEAD/OUTPUT PROJECTION ====================\n",
    "    \n",
    "    head_flops = {}\n",
    "    \n",
    "    # head_nf calculation\n",
    "    head_nf = d_model * patch_num\n",
    "    \n",
    "    if configs.get('task_name') == 'classification':\n",
    "        # Flatten: [B, N, d_model, patch_num] -> [B, N * d_model * patch_num] (pas de FLOPs)\n",
    "        head_flops['flatten'] = 0\n",
    "        \n",
    "        # Dropout (pas de FLOPs)\n",
    "        \n",
    "        # Projection: [B, N * d_model * patch_num] @ [N * d_model * patch_num, num_class] -> [B, num_class]\n",
    "        head_flops['classification_projection'] = B * (N * head_nf) * O\n",
    "        \n",
    "    else:\n",
    "        # FlattenHead pour autres tâches\n",
    "        # Flatten: [B, N, d_model, patch_num] -> [B, N, d_model * patch_num] (pas de FLOPs)\n",
    "        head_flops['flatten'] = 0\n",
    "        \n",
    "        # Linear: [B, N, head_nf] @ [head_nf, target_window] -> [B, N, target_window]\n",
    "        head_flops['linear_projection'] = B * N * head_nf * O\n",
    "        \n",
    "        # Dropout (pas de FLOPs)\n",
    "        \n",
    "        # Permute final: [B, N, target_window] -> [B, target_window, N] (pas de FLOPs)\n",
    "    \n",
    "    # ==================== TOTAL CALCULATION ====================\n",
    "    \n",
    "    flops_breakdown['normalization'] = normalization_flops\n",
    "    flops_breakdown['patch_embedding'] = sum(patch_embedding_flops.values())\n",
    "    flops_breakdown['encoder_layers'] = encoder_layer_flops\n",
    "    flops_breakdown['total_encoder_layers'] = encoder_layer_total * e_layers\n",
    "    flops_breakdown['encoder_norm'] = encoder_norm_flops\n",
    "    flops_breakdown['reshape_permute'] = sum(reshape_flops.values())\n",
    "    flops_breakdown['head'] = sum(head_flops.values())\n",
    "    \n",
    "    # Détails pour debug\n",
    "    flops_breakdown['patch_embedding_detail'] = patch_embedding_flops\n",
    "    flops_breakdown['head_detail'] = head_flops\n",
    "    \n",
    "    total_flops = (flops_breakdown['normalization'] +\n",
    "                  flops_breakdown['patch_embedding'] + \n",
    "                  flops_breakdown['total_encoder_layers'] + \n",
    "                  flops_breakdown['encoder_norm'] +\n",
    "                  flops_breakdown['reshape_permute'] +\n",
    "                  flops_breakdown['head'])\n",
    "    \n",
    "    flops_breakdown['total'] = total_flops\n",
    "    \n",
    "    # Informations supplémentaires pour debug\n",
    "    flops_breakdown['debug_info'] = {\n",
    "        'patch_num': patch_num,\n",
    "        'head_nf': head_nf,\n",
    "        'variables': N,\n",
    "        'patch_len': patch_len,\n",
    "        'stride': stride\n",
    "    }\n",
    "    \n",
    "    return flops_breakdown, total_flops\n",
    "\n",
    "def print_flops_breakdown_patchtst(flops_breakdown, total_flops):\n",
    "    \"\"\"\n",
    "    Affiche un résumé détaillé des FLOPs pour le modèle PatchTST.\n",
    "    \n",
    "    Parameters:\n",
    "    - flops_breakdown: Dictionnaire des FLOPs par composant\n",
    "    - total_flops: Total des FLOPs\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(f\"PATCHTST - FLOPs BREAKDOWN\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    debug_info = flops_breakdown['debug_info']\n",
    "    print(f\"Patch Configuration:\")\n",
    "    print(f\"  Variables: {debug_info['variables']}\")\n",
    "    print(f\"  Patch Length: {debug_info['patch_len']}\")\n",
    "    print(f\"  Stride: {debug_info['stride']}\")\n",
    "    print(f\"  Number of Patches: {debug_info['patch_num']}\")\n",
    "    print(f\"  Head Features: {debug_info['head_nf']}\")\n",
    "    \n",
    "    print(f\"\\nFLOPs Breakdown:\")\n",
    "    print(f\"Normalization: {flops_breakdown['normalization']:,} FLOPs\")\n",
    "    print(f\"Patch Embedding: {flops_breakdown['patch_embedding']:,} FLOPs\")\n",
    "    print(f\"Encoder Norm: {flops_breakdown['encoder_norm']:,} FLOPs\")\n",
    "    print(f\"Reshape/Permute: {flops_breakdown['reshape_permute']:,} FLOPs\")\n",
    "    print(f\"Head/Output: {flops_breakdown['head']:,} FLOPs\")\n",
    "    \n",
    "    print(f\"\\n=== ENCODER ===\")\n",
    "    print(f\"Per Encoder Layer FLOPs: {flops_breakdown['encoder_layers']['total_per_layer']:,}\")\n",
    "    print(f\"Total Encoder Layers FLOPs: {flops_breakdown['total_encoder_layers']:,}\")\n",
    "    \n",
    "    print(f\"\\n--- Encoder Layer Breakdown ---\")\n",
    "    encoder_attention_total = sum(flops_breakdown['encoder_layers']['attention'].values())\n",
    "    encoder_ff_total = sum(flops_breakdown['encoder_layers']['feedforward'].values())\n",
    "    encoder_layer_total = flops_breakdown['encoder_layers']['total_per_layer']\n",
    "    \n",
    "    print(f\"Multi-Head Attention (on Patches): {encoder_attention_total:,} FLOPs ({encoder_attention_total/encoder_layer_total*100:.1f}%)\")\n",
    "    print(f\"Feed Forward Network: {encoder_ff_total:,} FLOPs ({encoder_ff_total/encoder_layer_total*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\n--- Detailed Patch Embedding Breakdown ---\")\n",
    "    for component, flops in flops_breakdown['patch_embedding_detail'].items():\n",
    "        print(f\"  {component}: {flops:,} FLOPs\")\n",
    "    \n",
    "    print(f\"\\n--- Detailed Attention Breakdown ---\")\n",
    "    for component, flops in flops_breakdown['encoder_layers']['attention'].items():\n",
    "        print(f\"  {component}: {flops:,} FLOPs\")\n",
    "    \n",
    "    print(f\"\\n--- Detailed Head Breakdown ---\")\n",
    "    for component, flops in flops_breakdown['head_detail'].items():\n",
    "        print(f\"  {component}: {flops:,} FLOPs\")\n",
    "    \n",
    "    print(\"\\n--- PatchTST Specifics ---\")\n",
    "    print(\"• Input is segmented into patches\")\n",
    "    print(\"• Each variable processed independently\")\n",
    "    print(\"• Attention operates on PATCHES (not time steps)\")\n",
    "    print(f\"• Complexity: O(P²) where P = {debug_info['patch_num']} patches\")\n",
    "    print(\"• Channel Independence: each variable has separate attention\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(f\"TOTAL FLOPs: {total_flops:,}\")\n",
    "    print(f\"TOTAL GFLOPs: {total_flops / 1e9:.3f}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "# Test du modèle PatchTST\n",
    "from models.PatchTST import Model as PatchTST\n",
    "\n",
    "# Configuration pour le modèle PatchTST\n",
    "conf_patchtst = {\n",
    "    'task_name': 'anomaly_detection',\n",
    "    'seq_len': 96,\n",
    "    'pred_len': 96,\n",
    "    'enc_in': 10,        # Features d'entrée (nombre de variables)\n",
    "    'c_out': 10,         # Features de sortie\n",
    "    'num_class': 10,     # Nombre de classes pour classification\n",
    "    'd_model': 256,       # Dimension du modèle\n",
    "    'n_heads': 8,        # Nombre de têtes d'attention\n",
    "    'd_ff': 256,         # Dimension feedforward\n",
    "    'e_layers': 2,       # Nombre de couches encoder\n",
    "    'factor': 3,         # Facteur pour attention\n",
    "    'patch_len': 16,     # Longueur des patches\n",
    "    'stride': 8,         # Stride pour les patches\n",
    "    'dropout': 0.1,\n",
    "    'activation': 'gelu',\n",
    "    'embed': 'timeF',\n",
    "    'freq': 'h'\n",
    "}\n",
    "configs_patchtst = type('Config', (), conf_patchtst)()\n",
    "\n",
    "# Créer le modèle PatchTST\n",
    "model_patchtst = PatchTST(configs_patchtst, \n",
    "                         patch_len=conf_patchtst['patch_len'],\n",
    "                         stride=conf_patchtst['stride'])\n",
    "print(\"Model parameters:\", sum(p.numel() for p in model_patchtst.parameters()))\n",
    "\n",
    "print(\"=== MODÈLE PATCHTST CRÉÉ ===\")\n",
    "print(f\"Task: {model_patchtst.task_name}\")\n",
    "print(f\"Sequence length: {model_patchtst.seq_len}\")\n",
    "if hasattr(model_patchtst, 'pred_len'):\n",
    "    print(f\"Prediction length: {model_patchtst.pred_len}\")\n",
    "print(f\"Encoder layers: {len(model_patchtst.encoder.attn_layers)}\")\n",
    "print(f\"Features (variables): {conf_patchtst['enc_in']}\")\n",
    "print(f\"Patch length: {conf_patchtst['patch_len']}\")\n",
    "print(f\"Stride: {conf_patchtst['stride']}\")\n",
    "\n",
    "# Calculer le nombre de patches pour info\n",
    "patch_num = int((conf_patchtst['seq_len'] - conf_patchtst['patch_len']) / conf_patchtst['stride'] + 1) + 1  # +1 pour padding\n",
    "print(f\"Number of patches: {patch_num}\")\n",
    "\n",
    "# Test des FLOPs\n",
    "batch_size = 32\n",
    "sequence_length = 96\n",
    "\n",
    "flops_breakdown, total_flops = count_flops_patchtst(\n",
    "    model_patchtst, batch_size=batch_size, sequence_length=sequence_length, configs=conf_patchtst\n",
    ")\n",
    "\n",
    "# Afficher les résultats\n",
    "print_flops_breakdown_patchtst(flops_breakdown, total_flops)\n",
    "\n",
    "# Test avec différentes longueurs de séquence\n",
    "print(f\"\\n=== TEST AVEC DIFFÉRENTES LONGUEURS DE SÉQUENCE ===\")\n",
    "flops_patchtst = []\n",
    "\n",
    "for seq_len in SEQ_LENGTHS:\n",
    "    # Ajuster la configuration pour chaque longueur de séquence\n",
    "    conf_test = conf_patchtst.copy()\n",
    "    conf_test['seq_len'] = seq_len\n",
    "    \n",
    "    flops_breakdown, total_flops = count_flops_patchtst(\n",
    "        model_patchtst, batch_size=10, sequence_length=seq_len, configs=conf_test\n",
    "    )\n",
    "    print(f\"Sequence Length: {seq_len:4d}, Total FLOPs: {total_flops:12,}, GFLOPs: {total_flops/1e9:8.3f}\")\n",
    "    flops_patchtst.append(total_flops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a5bd8c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 1077738\n",
      "=== MODÈLE TIMESNET CRÉÉ ===\n",
      "Task: anomaly_detection\n",
      "Sequence length: 96\n",
      "Prediction length: 96\n",
      "TimesBlock layers: 2\n",
      "Features: 10\n",
      "d_model: 32\n",
      "d_ff: 100\n",
      "Top-k periods: 5\n",
      "============================================================\n",
      "TIMESNET - FLOPs BREAKDOWN\n",
      "============================================================\n",
      "Normalization: 215,040 FLOPs\n",
      "Input Embedding: 983,040 FLOPs\n",
      "Temporal Embedding: 393,216 FLOPs\n",
      "Output Projection: 983,040 FLOPs\n",
      "\n",
      "=== TIMESBLOCK LAYERS ===\n",
      "Per TimesBlock Layer FLOPs: 772,303,329.8792971\n",
      "Total TimesBlock Layers FLOPs: 1,544,606,659.7585943\n",
      "\n",
      "--- TimesBlock Layer Breakdown ---\n",
      "FFT Period Detection: 439,585.8792970912 FLOPs (0.1%)\n",
      "Period Processing (Inception): 771,384,032 FLOPs (99.9%)\n",
      "Adaptive Aggregation: 184,800 FLOPs (0.0%)\n",
      "Layer Normalization: 294,912 FLOPs (0.0%)\n",
      "\n",
      "--- Detailed FFT Breakdown ---\n",
      "  rfft: 404,580.0960443078 FLOPs\n",
      "  amplitude: 15,680 FLOPs\n",
      "  mean_topk: 19,320.783252783385 FLOPs\n",
      "  period_calc: 5 FLOPs\n",
      "\n",
      "--- Detailed Aggregation Breakdown ---\n",
      "  period_softmax: 480 FLOPs\n",
      "  weighted_sum: 153,600 FLOPs\n",
      "  residual_add: 30,720 FLOPs\n",
      "\n",
      "--- TimesNet Specifics ---\n",
      "• Uses FFT for period detection in time series\n",
      "• Transforms 1D time series to 2D representation\n",
      "• Applies 2D Inception blocks for feature extraction\n",
      "• Adaptively aggregates multiple period representations\n",
      "• Complexity dominated by FFT: O(L log L) and Inception: O(H*W*C²)\n",
      "============================================================\n",
      "TOTAL FLOPs: 1,547,180,995.7585943\n",
      "TOTAL GFLOPs: 1.547\n",
      "============================================================\n",
      "\n",
      "=== TEST AVEC DIFFÉRENTES LONGUEURS DE SÉQUENCE ===\n",
      "Sequence Length:    1, Total FLOPs: 215,348,656.4385619, GFLOPs:    0.215\n",
      "Sequence Length:    2, Total FLOPs: 217,558,702.8771238, GFLOPs:    0.218\n",
      "Sequence Length:    4, Total FLOPs: 222,418,809.3156857, GFLOPs:    0.222\n",
      "Sequence Length:    8, Total FLOPs: 233,019,942.1928095, GFLOPs:    0.233\n",
      "Sequence Length:   16, Total FLOPs: 254,665,067.94705707, GFLOPs:    0.255\n",
      "Sequence Length:   32, Total FLOPs: 295,763,419.4555523, GFLOPs:    0.296\n",
      "Sequence Length:   64, Total FLOPs: 385,007,482.47254264, GFLOPs:    0.385\n",
      "Sequence Length:  128, Total FLOPs: 538,020,928.5065234, GFLOPs:    0.538\n",
      "Sequence Length:  256, Total FLOPs: 851,133,580.5744848, GFLOPs:    0.851\n",
      "Sequence Length:  512, Total FLOPs: 1,493,289,044.7104077, GFLOPs:    1.493\n",
      "Sequence Length: 1024, Total FLOPs: 2,914,099,372.9822536, GFLOPs:    2.914\n",
      "Sequence Length: 2048, Total FLOPs: 5,834,828,769.525946, GFLOPs:    5.835\n",
      "Sequence Length: 4096, Total FLOPs: 11,675,787,782.613329, GFLOPs:   11.676\n",
      "Sequence Length: 8192, Total FLOPs: 23,358,025,228.788094, GFLOPs:   23.358\n",
      "Sequence Length: 16384, Total FLOPs: 46,724,457,941.13763, GFLOPs:   46.724\n",
      "Sequence Length: 32768, Total FLOPs: 93,467,833,905.8367, GFLOPs:   93.468\n",
      "Sequence Length: 65536, Total FLOPs: 186,966,374,055.23483, GFLOPs:  186.966\n",
      "Sequence Length: 131072, Total FLOPs: 373,988,349,774.03107, GFLOPs:  373.988\n",
      "Sequence Length: 262144, Total FLOPs: 748,083,411,031.6237, GFLOPs:  748.083\n",
      "Sequence Length: 524288, Total FLOPs: 1,496,382,348,086.8086, GFLOPs: 1496.382\n",
      "Sequence Length: 1048576, Total FLOPs: 2,993,188,618,417.1787, GFLOPs: 2993.189\n",
      "Sequence Length: 2097152, Total FLOPs: 5,987,219,270,497.919, GFLOPs: 5987.219\n",
      "Sequence Length: 4194304, Total FLOPs: 11,976,118,116,479.4, GFLOPs: 11976.118\n",
      "Sequence Length: 8388608, Total FLOPs: 23,955,597,486,982.36, GFLOPs: 23955.597\n",
      "Sequence Length: 16777216, Total FLOPs: 47,917,910,352,208.28, GFLOPs: 47917.910\n"
     ]
    }
   ],
   "source": [
    "def count_flops_timesnet(model, batch_size=1, sequence_length=1, configs={}):\n",
    "    \"\"\"\n",
    "    Compte les FLOPs (Floating Point Operations) pour le modèle TimesNet.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: Instance du modèle TimesNet\n",
    "    - batch_size: Taille du batch\n",
    "    - sequence_length: Longueur de la séquence\n",
    "    - configs: Configuration du modèle (dict)\n",
    "    \n",
    "    Returns:\n",
    "    - dict: Dictionnaire détaillé des FLOPs par composant\n",
    "    - int: Total des FLOPs\n",
    "    \"\"\"\n",
    "    \n",
    "    # Paramètres du modèle\n",
    "    B = batch_size\n",
    "    L = sequence_length  # Sequence length\n",
    "    \n",
    "    # Récupérer les paramètres depuis la configuration\n",
    "    d_model = configs.get('d_model')  # Dimension du modèle\n",
    "    d_ff = configs.get('d_ff')  # Dimension feedforward (utilisée dans Inception blocks)\n",
    "    e_layers = configs.get('e_layers')  # Nombre de couches TimesBlock\n",
    "    top_k = configs.get('top_k', 5)  # Nombre de périodes top-k à analyser\n",
    "    num_kernels = configs.get('num_kernels', 6)  # Nombre de kernels dans Inception\n",
    "    \n",
    "    # Input/Output dimensions\n",
    "    I = configs.get('enc_in')  # Input features\n",
    "    O = configs.get('c_out')  # Output features\n",
    "    \n",
    "    # Dimensions pour forecasting\n",
    "    pred_len = configs.get('pred_len', 0)\n",
    "    \n",
    "    flops_breakdown = {}\n",
    "    \n",
    "    # ==================== NORMALIZATION (pour forecasting/imputation/anomaly) ====================\n",
    "    \n",
    "    normalization_flops = 0\n",
    "    if configs.get('task_name') in ['long_term_forecast', 'short_term_forecast']:\n",
    "        # Mean computation: [B, L, I]\n",
    "        normalization_flops += B * L * I  # sum\n",
    "        normalization_flops += B * I      # divide par L\n",
    "        \n",
    "        # Variance computation: [B, L, I]\n",
    "        normalization_flops += B * L * I  # subtract mean\n",
    "        normalization_flops += B * L * I  # square + sqrt\n",
    "        normalization_flops += B * L * I  # sum\n",
    "        normalization_flops += B * I      # divide par L + sqrt\n",
    "        \n",
    "        # Normalization: [B, L, I]\n",
    "        normalization_flops += B * L * I  # subtract mean\n",
    "        normalization_flops += B * L * I  # divide by std\n",
    "        \n",
    "        # Denormalization (en sortie)\n",
    "        normalization_flops += B * (L + pred_len) * I * 2  # multiply + add\n",
    "        \n",
    "    elif configs.get('task_name') == 'imputation':\n",
    "        # Imputation avec masking (approximation)\n",
    "        normalization_flops += B * L * I * 10  # Diverses opérations avec masques\n",
    "        \n",
    "    elif configs.get('task_name') == 'anomaly_detection':\n",
    "        # Normalization similaire mais sans denormalization\n",
    "        normalization_flops += B * L * I * 7  # mean, var, sqrt, normalize\n",
    "    \n",
    "    # ==================== INPUT EMBEDDING ====================\n",
    "    \n",
    "    # DataEmbedding: value embedding + temporal embedding\n",
    "    # Value embedding: Linear [I -> d_model]\n",
    "    flops_breakdown['input_embedding'] = B * L * I * d_model\n",
    "    \n",
    "    # Temporal embedding (approximation)\n",
    "    flops_breakdown['temporal_embedding'] = B * L * 4 * d_model  # 4 features temporelles typiques\n",
    "    \n",
    "    # ==================== PREDICT LINEAR (pour forecasting) ====================\n",
    "    \n",
    "    predict_linear_flops = 0\n",
    "    if configs.get('task_name') in ['long_term_forecast', 'short_term_forecast']:\n",
    "        # predict_linear: permute + linear + permute\n",
    "        # [B, L, d_model] -> [B, d_model, L] -> [B, d_model, L+pred_len] -> [B, L+pred_len, d_model]\n",
    "        predict_linear_flops = B * d_model * L * (L + pred_len)\n",
    "    \n",
    "    # ==================== TIMESBLOCK LAYERS ====================\n",
    "    \n",
    "    timesblock_layer_flops = {}\n",
    "    \n",
    "    # ==================== FFT FOR PERIOD DETECTION ====================\n",
    "    \n",
    "    fft_flops = {}\n",
    "    \n",
    "    # FFT computation: [B, L, I] -> complexe\n",
    "    # FFT: O(L log L) per feature per batch\n",
    "    fft_flops['rfft'] = B * I * L * math.log2(L) * 2  # complexe FFT\n",
    "    \n",
    "    # Amplitude computation: abs(xf)\n",
    "    fft_flops['amplitude'] = B * I * (L // 2 + 1)  # rfft donne (L//2 + 1) fréquences\n",
    "    \n",
    "    # Mean over features and find top-k\n",
    "    fft_flops['mean_topk'] = B * (L // 2 + 1) * I + B * (L // 2 + 1) * math.log2(top_k)\n",
    "    \n",
    "    # Period calculation (division)\n",
    "    fft_flops['period_calc'] = top_k\n",
    "    \n",
    "    # ==================== TIMESBLOCK PROCESSING ====================\n",
    "    \n",
    "    timesblock_processing_flops = {}\n",
    "    \n",
    "    # Pour chaque période dans top_k\n",
    "    for k in range(top_k):\n",
    "        period_flops = {}\n",
    "        \n",
    "        # Estimation de la période moyenne (heuristique)\n",
    "        avg_period = max(L // (2 + k), 1)  # Période décroissante\n",
    "\n",
    "        # Padding computation (si nécessaire)\n",
    "        # Approximation: dans la plupart des cas il y a du padding\n",
    "        length = ((L + pred_len) // avg_period + 1) * avg_period\n",
    "        padding_length = length - (L + pred_len)\n",
    "        period_flops['padding'] = B * padding_length * I if padding_length > 0 else 0\n",
    "        \n",
    "        # Reshape vers 2D: [B, length//period, period, I] -> [B, I, length//period, period]\n",
    "        # Pas de FLOPs, juste reshape et permute\n",
    "        \n",
    "        # Dimensions après reshape\n",
    "        H = length // avg_period  # hauteur\n",
    "        W = avg_period            # largeur\n",
    "        \n",
    "        # ==================== INCEPTION BLOCK V1 (2 blocs en séquence) ====================\n",
    "        \n",
    "        inception_flops = {}\n",
    "        \n",
    "        # Premier Inception Block: [B, I, H, W] -> [B, d_ff, H, W]\n",
    "        # Approximation des conv2d dans Inception (plusieurs kernel sizes)\n",
    "        \n",
    "        # 1x1 conv branch\n",
    "        inception_flops['conv1x1_1'] = B * H * W * d_model * d_ff\n",
    "        \n",
    "        # 1x1 -> 3x3 conv branch  \n",
    "        inception_flops['conv1x1_to_3x3'] = B * H * W * d_model * (d_ff // 4)  # reduction\n",
    "        inception_flops['conv3x3'] = B * H * W * (d_ff // 4) * (d_ff // 4) * 9  # 3x3 kernel\n",
    "        \n",
    "        # 1x1 -> 5x5 conv branch\n",
    "        inception_flops['conv1x1_to_5x5'] = B * H * W * d_model * (d_ff // 8)  # reduction\n",
    "        inception_flops['conv5x5'] = B * H * W * (d_ff // 8) * (d_ff // 8) * 25  # 5x5 kernel\n",
    "        \n",
    "        # MaxPool -> 1x1 conv branch\n",
    "        inception_flops['maxpool'] = B * H * W * d_model * 9  # 3x3 maxpool approximation\n",
    "        inception_flops['conv1x1_after_pool'] = B * H * W * d_model * (d_ff // 4)\n",
    "        \n",
    "        # Concatenation (pas de FLOPs)\n",
    "        \n",
    "        # GELU activation\n",
    "        inception_flops['gelu'] = B * H * W * d_ff * 2\n",
    "        \n",
    "        # Deuxième Inception Block: [B, d_ff, H, W] -> [B, d_model, H, W]\n",
    "        # Structure similaire mais d_ff -> d_model\n",
    "        inception_flops['conv1x1_2'] = B * H * W * d_ff * d_model\n",
    "        inception_flops['conv1x1_to_3x3_2'] = B * H * W * d_ff * (d_model // 4)\n",
    "        inception_flops['conv3x3_2'] = B * H * W * (d_model // 4) * (d_model // 4) * 9\n",
    "        inception_flops['conv1x1_to_5x5_2'] = B * H * W * d_ff * (d_model // 8)\n",
    "        inception_flops['conv5x5_2'] = B * H * W * (d_model // 8) * (d_model // 8) * 25\n",
    "        inception_flops['maxpool_2'] = B * H * W * d_ff * 9\n",
    "        inception_flops['conv1x1_after_pool_2'] = B * H * W * d_ff * (d_model // 4)\n",
    "        \n",
    "        # Reshape back: [B, d_model, H, W] -> [B, length, I]\n",
    "        # Pas de FLOPs, juste permute et reshape\n",
    "        \n",
    "        # Crop to original length\n",
    "        period_flops['inception'] = sum(inception_flops.values())\n",
    "        \n",
    "        timesblock_processing_flops[f'period_{k}'] = period_flops\n",
    "    \n",
    "    # ==================== ADAPTIVE AGGREGATION ====================\n",
    "    \n",
    "    aggregation_flops = {}\n",
    "    \n",
    "    # Softmax sur les poids de période: [B, top_k] -> [B, top_k]\n",
    "    aggregation_flops['period_softmax'] = B * top_k * 3  # exp + sum + div\n",
    "    \n",
    "    # Reshape weights: [B, top_k] -> [B, L, I, top_k]\n",
    "    # Pas de FLOPs, juste broadcast\n",
    "    \n",
    "    # Weighted sum: [B, L, I, top_k] -> [B, L, I]\n",
    "    aggregation_flops['weighted_sum'] = B * L * I * top_k\n",
    "    \n",
    "    # Residual connection: res + x\n",
    "    aggregation_flops['residual_add'] = B * L * I\n",
    "    \n",
    "    # ==================== LAYER NORMALIZATION ====================\n",
    "    \n",
    "    layer_norm_flops = {}\n",
    "    \n",
    "    # LayerNorm après chaque TimesBlock: [B, L, d_model]\n",
    "    layer_norm_flops['layer_norm'] = B * L * d_model * 3  # mean, var, normalize\n",
    "    \n",
    "    # ==================== ASSEMBLY TIMESBLOCK LAYER FLOPS ====================\n",
    "    \n",
    "    # Total des FLOPs pour un TimesBlock\n",
    "    timesblock_layer_total = (sum(fft_flops.values()) + \n",
    "                             sum([sum(period_flops.values()) for period_flops in timesblock_processing_flops.values()]) +\n",
    "                             sum(aggregation_flops.values()) + \n",
    "                             sum(layer_norm_flops.values()))\n",
    "    \n",
    "    timesblock_layer_flops['fft_period_detection'] = fft_flops\n",
    "    timesblock_layer_flops['period_processing'] = timesblock_processing_flops\n",
    "    timesblock_layer_flops['adaptive_aggregation'] = aggregation_flops\n",
    "    timesblock_layer_flops['layer_norm'] = layer_norm_flops\n",
    "    timesblock_layer_flops['total_per_layer'] = timesblock_layer_total\n",
    "    \n",
    "    # ==================== OUTPUT PROJECTION ====================\n",
    "    \n",
    "    output_flops = 0\n",
    "    \n",
    "    if configs.get('task_name') == 'classification':\n",
    "        # GELU activation: [B, L, d_model]\n",
    "        gelu_flops = B * L * d_model * 2\n",
    "        # Dropout (pas de FLOPs)\n",
    "        # Reshape: [B, L * d_model]\n",
    "        # Projection: [B, L * d_model] @ [L * d_model, num_class] -> [B, num_class]\n",
    "        num_class = configs.get('num_class', O)\n",
    "        projection_flops = B * (L * d_model) * num_class\n",
    "        \n",
    "        output_flops = gelu_flops + projection_flops\n",
    "        flops_breakdown['output_activation'] = gelu_flops\n",
    "        flops_breakdown['output_projection'] = projection_flops\n",
    "        \n",
    "    elif configs.get('task_name') in ['long_term_forecast', 'short_term_forecast']:\n",
    "        # Projection: [B, L+pred_len, d_model] @ [d_model, O] -> [B, L+pred_len, O]\n",
    "        output_flops = B * (L + pred_len) * d_model * O\n",
    "        flops_breakdown['output_projection'] = output_flops\n",
    "        \n",
    "    elif configs.get('task_name') in ['imputation', 'anomaly_detection']:\n",
    "        # Direct projection: [B, L, d_model] @ [d_model, O] -> [B, L, O]\n",
    "        output_flops = B * L * d_model * O\n",
    "        flops_breakdown['output_projection'] = output_flops\n",
    "    \n",
    "    # ==================== TOTAL CALCULATION ====================\n",
    "    \n",
    "    flops_breakdown['normalization'] = normalization_flops\n",
    "    flops_breakdown['input_embedding'] = flops_breakdown['input_embedding']\n",
    "    flops_breakdown['temporal_embedding'] = flops_breakdown['temporal_embedding']\n",
    "    flops_breakdown['predict_linear'] = predict_linear_flops\n",
    "    flops_breakdown['timesblock_layers'] = timesblock_layer_flops\n",
    "    flops_breakdown['total_timesblock_layers'] = timesblock_layer_total * e_layers\n",
    "    \n",
    "    total_flops = (flops_breakdown['normalization'] +\n",
    "                  flops_breakdown['input_embedding'] + \n",
    "                  flops_breakdown['temporal_embedding'] +\n",
    "                  flops_breakdown['predict_linear'] +\n",
    "                  flops_breakdown['total_timesblock_layers'] + \n",
    "                  output_flops)\n",
    "    \n",
    "    flops_breakdown['total'] = total_flops\n",
    "    \n",
    "    return flops_breakdown, total_flops\n",
    "\n",
    "def print_flops_breakdown_timesnet(flops_breakdown, total_flops):\n",
    "    \"\"\"\n",
    "    Affiche un résumé détaillé des FLOPs pour le modèle TimesNet.\n",
    "    \n",
    "    Parameters:\n",
    "    - flops_breakdown: Dictionnaire des FLOPs par composant\n",
    "    - total_flops: Total des FLOPs\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(f\"TIMESNET - FLOPs BREAKDOWN\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if flops_breakdown['normalization'] > 0:\n",
    "        print(f\"Normalization: {flops_breakdown['normalization']:,} FLOPs\")\n",
    "    print(f\"Input Embedding: {flops_breakdown['input_embedding']:,} FLOPs\")\n",
    "    print(f\"Temporal Embedding: {flops_breakdown['temporal_embedding']:,} FLOPs\")\n",
    "    if flops_breakdown['predict_linear'] > 0:\n",
    "        print(f\"Predict Linear: {flops_breakdown['predict_linear']:,} FLOPs\")\n",
    "    \n",
    "    if 'output_projection' in flops_breakdown:\n",
    "        print(f\"Output Projection: {flops_breakdown['output_projection']:,} FLOPs\")\n",
    "    if 'output_activation' in flops_breakdown:\n",
    "        print(f\"Output Activation: {flops_breakdown['output_activation']:,} FLOPs\")\n",
    "    \n",
    "    print(f\"\\n=== TIMESBLOCK LAYERS ===\")\n",
    "    print(f\"Per TimesBlock Layer FLOPs: {flops_breakdown['timesblock_layers']['total_per_layer']:,}\")\n",
    "    print(f\"Total TimesBlock Layers FLOPs: {flops_breakdown['total_timesblock_layers']:,}\")\n",
    "    \n",
    "    print(f\"\\n--- TimesBlock Layer Breakdown ---\")\n",
    "    fft_total = sum(flops_breakdown['timesblock_layers']['fft_period_detection'].values())\n",
    "    processing_total = sum([sum(period_flops.values()) for period_flops in flops_breakdown['timesblock_layers']['period_processing'].values()])\n",
    "    aggregation_total = sum(flops_breakdown['timesblock_layers']['adaptive_aggregation'].values())\n",
    "    norm_total = sum(flops_breakdown['timesblock_layers']['layer_norm'].values())\n",
    "    \n",
    "    layer_total = flops_breakdown['timesblock_layers']['total_per_layer']\n",
    "    print(f\"FFT Period Detection: {fft_total:,} FLOPs ({fft_total/layer_total*100:.1f}%)\")\n",
    "    print(f\"Period Processing (Inception): {processing_total:,} FLOPs ({processing_total/layer_total*100:.1f}%)\")\n",
    "    print(f\"Adaptive Aggregation: {aggregation_total:,} FLOPs ({aggregation_total/layer_total*100:.1f}%)\")\n",
    "    print(f\"Layer Normalization: {norm_total:,} FLOPs ({norm_total/layer_total*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\n--- Detailed FFT Breakdown ---\")\n",
    "    for component, flops in flops_breakdown['timesblock_layers']['fft_period_detection'].items():\n",
    "        print(f\"  {component}: {flops:,} FLOPs\")\n",
    "    \n",
    "    print(f\"\\n--- Detailed Aggregation Breakdown ---\")\n",
    "    for component, flops in flops_breakdown['timesblock_layers']['adaptive_aggregation'].items():\n",
    "        print(f\"  {component}: {flops:,} FLOPs\")\n",
    "    \n",
    "    print(\"\\n--- TimesNet Specifics ---\")\n",
    "    print(\"• Uses FFT for period detection in time series\")\n",
    "    print(\"• Transforms 1D time series to 2D representation\")\n",
    "    print(\"• Applies 2D Inception blocks for feature extraction\")\n",
    "    print(\"• Adaptively aggregates multiple period representations\")\n",
    "    print(\"• Complexity dominated by FFT: O(L log L) and Inception: O(H*W*C²)\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(f\"TOTAL FLOPs: {total_flops:,}\")\n",
    "    print(f\"TOTAL GFLOPs: {total_flops / 1e9:.3f}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "# Test du modèle TimesNet\n",
    "from models.TimesNet import Model as TimesNet\n",
    "import math\n",
    "\n",
    "# Configuration pour le modèle TimesNet\n",
    "conf_timesnet = {\n",
    "    'task_name': 'anomaly_detection',\n",
    "    'seq_len': 96,\n",
    "    'pred_len': 96,\n",
    "    'label_len': 10,\n",
    "    'enc_in': 10,        # Features d'entrée\n",
    "    'c_out': 10,         # Features de sortie\n",
    "    'num_class': 10,     # Nombre de classes pour classification\n",
    "    'd_model': 32,       # Dimension du modèle\n",
    "    'd_ff': 100,          # Dimension feedforward (dans Inception blocks)\n",
    "    'e_layers': 2,       # Nombre de couches TimesBlock\n",
    "    'top_k': 5,          # Nombre de périodes top-k\n",
    "    'num_kernels': 4,    # Nombre de kernels dans Inception\n",
    "    'dropout': 0.1,\n",
    "    'embed': 'timeF',\n",
    "    'freq': 'h'\n",
    "}\n",
    "configs_timesnet = type('Config', (), conf_timesnet)()\n",
    "\n",
    "# Créer le modèle TimesNet\n",
    "model_timesnet = TimesNet(configs_timesnet)\n",
    "print(\"Model parameters:\", sum(p.numel() for p in model_timesnet.parameters()))\n",
    "\n",
    "print(\"=== MODÈLE TIMESNET CRÉÉ ===\")\n",
    "print(f\"Task: {model_timesnet.task_name}\")\n",
    "print(f\"Sequence length: {model_timesnet.seq_len}\")\n",
    "if hasattr(model_timesnet, 'pred_len'):\n",
    "    print(f\"Prediction length: {model_timesnet.pred_len}\")\n",
    "print(f\"TimesBlock layers: {len(model_timesnet.model)}\")\n",
    "print(f\"Features: {conf_timesnet['enc_in']}\")\n",
    "print(f\"d_model: {conf_timesnet['d_model']}\")\n",
    "print(f\"d_ff: {conf_timesnet['d_ff']}\")\n",
    "print(f\"Top-k periods: {conf_timesnet['top_k']}\")\n",
    "\n",
    "# Test des FLOPs\n",
    "batch_size = 32\n",
    "sequence_length = 96\n",
    "\n",
    "flops_breakdown, total_flops = count_flops_timesnet(\n",
    "    model_timesnet, batch_size=batch_size, sequence_length=sequence_length, configs=conf_timesnet\n",
    ")\n",
    "\n",
    "# Afficher les résultats\n",
    "print_flops_breakdown_timesnet(flops_breakdown, total_flops)\n",
    "\n",
    "# Test avec différentes longueurs de séquence\n",
    "print(f\"\\n=== TEST AVEC DIFFÉRENTES LONGUEURS DE SÉQUENCE ===\")\n",
    "flops_timesnet = []\n",
    "\n",
    "for seq_len in SEQ_LENGTHS:\n",
    "    # Ajuster la configuration pour chaque longueur de séquence\n",
    "    conf_test = conf_timesnet.copy()\n",
    "    conf_test['seq_len'] = seq_len\n",
    "    \n",
    "    flops_breakdown, total_flops = count_flops_timesnet(\n",
    "        model_timesnet, batch_size=10, sequence_length=seq_len, configs=conf_test\n",
    "    )\n",
    "    print(f\"Sequence Length: {seq_len:4d}, Total FLOPs: {total_flops:12,}, GFLOPs: {total_flops/1e9:8.3f}\")\n",
    "    flops_timesnet.append(total_flops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3642c674",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "sns.set(style=\"whitegrid\", font_scale=1.2, palette=\"muted\")\n",
    "\n",
    "sns.lineplot(x=SEQ_LENGTHS, y=flops_mamba, label='Mamba', linewidth=2)\n",
    "sns.lineplot(x=SEQ_LENGTHS, y=flops_transformer_vanilla, label='Transformer', linewidth=2)\n",
    "sns.lineplot(x=SEQ_LENGTHS, y=flops_reformer, label='Reformer', linewidth=2)\n",
    "sns.lineplot(x=SEQ_LENGTHS, y=flops_est, label='EST', linestyle='--', linewidth=3)\n",
    "sns.lineplot(x=SEQ_LENGTHS, y=flops_itransformer, label='iTransformer', linewidth=2)\n",
    "sns.lineplot(x=SEQ_LENGTHS, y=flops_patchtst, label='PatchTST', linewidth=2)\n",
    "sns.lineplot(x=SEQ_LENGTHS, y=flops_timesnet, label='TimesNet', linewidth=2)\n",
    "\n",
    "plt.xlabel('Sequence Length', fontsize=14, labelpad=10, weight='bold')\n",
    "plt.ylabel('FLOPs', fontsize=14, labelpad=10, weight='bold')\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.xticks([1, 1e1, 1e2, 1e3, 1e4, 1e5, 1e6, 1e7], ['1', '10', '100', '1K', '10K', '100K', '1M', '10M'])\n",
    "plt.title('FLOPs vs Sequence Length', fontsize=16, weight='bold', pad=25)\n",
    "plt.suptitle('~1M Parameters model size', fontsize=12, y=0.82, x=0.55)\n",
    "plt.legend(fontsize=12, title_fontsize=13, loc='upper left', frameon=True)\n",
    "plt.tight_layout()\n",
    "plt.savefig('flops.pdf', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4781d4df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.lineplot(x=SEQ_LENGTHS, y=flops_reformer, label='Mamba', linewidth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094b145d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5675fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b089eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tsl_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
